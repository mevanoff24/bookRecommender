{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommendation System\n",
    "\n",
    "Recommendation Systems have been around with many years now. Nowadays, recommender systems are one of the most successful and widespread application of machine learning technologies in business and have a substaincial impact on customer experience, building creadibiity with customers and have made businesses a lot of money along the way. Recommender systems are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. There are also recommender systems for experts, collaborators, jokes, restaurants, garments, financial services, life insurance, romantic partners (online dating), and Twitter pages [[1]](https://en.wikipedia.org/wiki/Recommender_system). Here in this notebook we focus on one of the cornerstone of recommendation systems, a Book Recommendation System. \n",
    "\n",
    "\n",
    "As a quick overview, recommender systems can be classified into 4 types: Simple recommenders, Content-based recommenders, Collaborative Filtering and the Hybrid recommender system. \n",
    "\n",
    "**Simple recommenders**\n",
    "- These are the most basic systems. These are referred to as 'non-personalized' recommenders and offer generalized recommendations to every user. This is something like returning the ten most popular books regardless of the users likes and tastes and the content of the book. \n",
    "\n",
    "**Content-based recommenders** \n",
    "- These are based on attributes of items and users. These systems don't take into consideration behavior or interations of different users or visitors. The only these systems use is other attributes of products which makes them very limitting. As the system is not able to recommend you something that is completely different, but content-based recommenders just recommend you something that is 'similar' that you have been interactiving with in the past. For example, if you enjoyed the first and second Harry Potter book, you might be recommend the third Harry Potter book. \n",
    "\n",
    "**Collaborative Filtering**\n",
    "- These systems are much more powerful than the first two. Collaborative filtering systems can help you explore diverse content based on other 'similar' users. Collaborative methods work with the interaction matrix that can also be called rating matrix in the rare case when users provide explicit rating of items. These systems try to predict the rating that a user would give an item-based on past ratings and preferences of other users. One of the benefits of these models, is that it doesn't require item metadata like its content-based counterparts.\n",
    "\n",
    "**Hybrid recommender**\n",
    "- These systems ultimitely combine collaborative filtering and content-based filtering effectively. Hybrid approaches can be implemented in different ways: by making content-based and collaborative-based predictions separately and then 'ensembling' them or by adding content-based capabilities to a collaborative-based approach (or vice versa). For example, you might combine a KNN model based on book features (Content-based recommenders) with a Matrix Factorization model (Collaborative Filtering). These often get the best of both worlds. Another benefit of this approach is that you can handle the [cold start problem](https://www.yuspify.com/blog/cold-start-problem-recommender-systems/) a bit more naturally. A good real life example of this Hybrid recommender is Netflix The website makes recommendations by comparing the watching and searching habits of similar users as well as by offering movies that share characteristics with films that a user has rated highly.[[2]](https://dl.acm.org/citation.cfm?doid=2869770.2843948)\n",
    "\n",
    "\n",
    "Can use behavior data when necessarily, but can fall back on content attributes when necessary. \n",
    "\n",
    "\n",
    "Could reserve certain topN results for certin recommender systmes, use one recommender system primarily with various fall backs to use in case the primary recommender cannot fill all the topN slots. Or even produce rating predictions from many recommender systems in parrell and add or average their scores together before ranking them. \n",
    "Overview of Sections. \n",
    "\n",
    "Could have a hierachy of recommenders that only fill in individual topN slots if a better recommender was unable to produce results. Or maybe a system that actually actually learns the correct weights between different recommenders based on the results from online experiements. Or could even create a system to personalize those weights based on user attributes or contextual information. \n",
    "\n",
    "We have chosen equal weights, but in a real system, we would want to run experiements to find the optimal weights to use and maybe even personalize those weights based on the individual user or work contextual information to them. \n",
    "\n",
    "### [Non-personalized-recommenders](#Non-personalized-recommenders)\n",
    "\n",
    "### [Collaborative Filtering](#Content-Based-Filtering)\n",
    "\n",
    "### [Collaborative Filtering](#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "Content Based\n",
    "- based on attributes of items / users. Doesn't take into consideration behavior or interations of different users or visitors. The only thing you can use is other attributes of products. Very limitting. Not able to recommend you something that is completely different, just recommends you something that is 'similar' that you have been interativing with in the past. \n",
    "\n",
    "\n",
    "Collaborative Filtering\n",
    "- more powerful since it can help you explore diverse content based on other similar users. Collaborative methods work with the interaction matrix that can also be called rating matrix in the rare case when users provide explicit rating of items. \n",
    "\n",
    "\n",
    "User based k-Nearest Neighbors\n",
    "- use cosine similarity, Jaccard similarity, etc. (correlation similarity) of rows (users) or columns (items) and recommends items that k — nearest neighbors enjoyed. \n",
    "- Lazy algorithm. Tough to scale \n",
    "\n",
    "\n",
    "Matrix Factorization\n",
    "- Matrix factorization attempts to reduce dimensionality of the interaction matrix (rating matrix) and approximate it by two or more small matrices (q and p) with k latent components. The Matrix Factorization techniques are usually more effective, because they allow users to discover the latent (hidden)features underlying the interactions between users and items (books). Takes into account ratings for individual users. For example, if one user always gives higher ratings or low ratings, this algorithm can take this into account and make a relative contribution to the system.\n",
    "- Optimizations\n",
    " - stochastic gradient descent. Most popular training algorithm is a stochastic gradient descent minimizing loss by gradient updates of both columns and rows of p a q matrices. The p and q matrixes are updated independently \n",
    " - ALS. Alternating Least Squares method that iteratively optimizes matrix p and matrix q by general least squares step. Iterily, fix P and optimize q, fix q and optimize p, etc... Need like 10 iterations. \n",
    " \n",
    " \n",
    "Association rules\n",
    "- Association rules can also be used for recommendation. Items that are frequently consumed together are connected with an edge in the graph. You can see clusters of best sellers (densely connected items that almost everybody interacted with) and small separated clusters of niche content. Mining rules is not very scalable. The APRIORI algorithm explores the state space of possible frequent itemsets and eliminates branches of the search space, that are not frequent. Frequent itemsets are used to generate rules and these rules generate recommendations.\n",
    "\n",
    "Neural Network\n",
    "- Can use auto-encoder to reconstruct the information (bottleneck), typically smaller size, same number of inputs and outputs. The benefit of this over basic Matrix Factorization is that we can have multiple layer for interations and can model non-lineararities. This can be used as the rating matrix then another algorithm can be run on top (KNN). \n",
    "- Nice because you can use both Content Based and Collaborative Filtering in one hybrid algorithm. As you can train auto-encoder to encode attributes, then you perform matrix factorization on interations and you train it together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "The actual 'ranking' of the model is not the main goal (only in acedimia). Ideally there is no 'correct' or 'ideal' metric in recommender systems. A business is ideally trying to optimizer it revenue or proxy for it, such as clicks, revenue, impressions, etc... Therefore in the real world we should A/B test the top results to determine what suits the business more. \n",
    "\n",
    "\n",
    "#### Online Testing \n",
    "The only way to determine is my deploying your algorithm and collecting data based on how users behave. Explore - exploit delemmma. Put recommendations in front of different users and measure if they buy, watch, or have interest in the recommendations you have presented. \n",
    "- Surragate problem\n",
    "\n",
    "At the end of the day, the only evaluation metric that matters is the results from your online A/B testing. \n",
    "\n",
    "Need to consider 'surprise' and 'diversity'. \n",
    "\n",
    "\n",
    "#### Offline Testing \n",
    "Root Mean Squared Error\n",
    "- The typical metric for recommender systems. How far off is the prediction. Punishes larger errors more. \n",
    "- Netfix (couldn't use it in the real world)\n",
    "\n",
    "\n",
    "Precision / Recall on top-N\n",
    "- Since you are showing a list of books that are recommended, how many of them are irrelevant\n",
    "- precision -- Precision at k is the proportion of recommended books in the top-k set that are relevant. Its interpretation is as follows. Suppose that my precision at 10 in a top-10 recommendation problem is 80%. This means that 80% of the recommendation I make are relevant to the user.\n",
    "\n",
    "Precision@k = (# of recommended items @k that are relevant) / (# of recommended items @k).\n",
    "\n",
    "- recall -- Recall at k is the proportion of relevant items found in the top-k recommendations. Suppose that we computed recall at 10 and found it is 40% in our top-10 recommendation system. This means that 40% of the total number of the relevant items appear in the top-k results.\n",
    "\n",
    "Recall@k = (# of recommended items @k that are relevant) / (total # of relevant items)\n",
    "\n",
    "\n",
    "Discounted Comulitve Gain\n",
    "- DCG takes also the position into consideration assuming that relevance of items logarithmically decreases.\n",
    "\n",
    "\n",
    "\n",
    "Multi-Objective optimization\n",
    "- can penalize best sellers, can increase the diversity of recommendations, and the precision is increased \n",
    "\n",
    "\n",
    "\n",
    "TopN Hit rate\n",
    "- Sum up number of hits in the topN\n",
    "- RMSE and Hit Rates aren't always related \n",
    "\n",
    "ARHR (average reciprocal hit rate) \n",
    "- more user focused metric, since more important to get the top value right than the bottom value. Sum up the reciprocal rank of each hit. For example 3 - 1/3 /  2 - 1/2 /  1 - 1 \n",
    "\n",
    "\n",
    "Need to consider \n",
    "- coverage \n",
    "- diversity - use average similarity scores between topN to measure diversity diversity = (1 - similarity score)\n",
    "- novelty - mean populatarity rank of recommended items \n",
    "- churn\n",
    "- responsiveness - how quickly does new user behavior influnce your recommendations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold Start Problem \n",
    "\n",
    "\n",
    "Since we won't have interations everytime, thus sometimes we have items or user completely without the interactions or even just one or two interations. Therefore you are not able to use the collaborative filtering approach. So you typically use content based methods. \n",
    "\n",
    "If you have an article you can measure the 'similarity' of different articles with NLP techniques, BOW, Tf-Idf, LSA, word vectors, Average Tf-idf word vectors, etc..\n",
    "\n",
    "Therefore items clustered based on their interaction similarity and attribute similarity are often aligned.\n",
    "\n",
    "\n",
    "You can use neural network to predict interaction similarity from attributes similarity and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Stuff\n",
    "\n",
    "- Stoplists \n",
    " - need to remove certain things all the time, race books, sex toys, etc... Need to build up this list in the real world\n",
    " \n",
    "- Filter bubbles\n",
    " - if a person buys a book about right wing politics, the system will probably recommend more and more books about right wing politics, if they respond to these recocmmendations, this person get more and more emersed in right wing ideology. It really has caused people to be exclusively exposed to things that reinforce their existing beliefs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# basics \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# my classes \n",
    "from BookData import BookDataSet\n",
    "from EvaluationData import CreateDataSets\n",
    "from EvaluatedAlgorithm import EvaluatedAlgorithm\n",
    "from Evaluator import Evaluator\n",
    "from utils import * \n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data saved in feather format in `ETL` notebook \n",
    "ratings = load_feather(DATA_PATH/'ratings_explicit_clean.feather')\n",
    "ratings.drop(['index'], axis=1, inplace=True)\n",
    "users = load_feather(DATA_PATH/'users_clean.feather')\n",
    "books = load_feather(DATA_PATH/'books_clean.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID        ISBN  Book_Rating\n",
       "0   276726  0155061224            5\n",
       "1   276729  052165615X            3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN           Book_Title           Book_Author  Year_Of_Publication  \\\n",
       "0  0195153448  Classical Mythology    Mark P. O. Morford               2002.0   \n",
       "1  0002005018         Clara Callan  Richard Bruce Wright               2001.0   \n",
       "\n",
       "                 Publisher language  \n",
       "0  Oxford University Press       en  \n",
       "1    HarperFlamingo Canada       it  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID                   Location   Age\n",
       "0        1         nyc, new york, usa   NaN\n",
       "1        2  stockton, california, usa  18.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[display(df.head(2)) for df in [ratings, books, users]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% of individuals have rated 1.0 number of books or fewer.\n",
      "The mean number of books rated is 5.57.\n",
      "The total number of user-book interactions in the dataset is 433671.\n",
      "The number of unique books: 271360\n",
      "The number of users: 278858\n",
      "The number of unique books that have at least 1 rating: 149836\n",
      "The most rated book in the dataset was rated 707 times.\n",
      "The book name of the most rated book is \"The Lovely Bones: A Novel\"\n"
     ]
    }
   ],
   "source": [
    "print('50% of individuals have rated {} number of books or fewer.'.format(ratings.groupby(['User_ID'])['Book_Rating'].count().median()))\n",
    "print('The mean number of books rated is {}.'.format(round(ratings.groupby(['User_ID'])['Book_Rating'].count().mean(), 2)))\n",
    "print('The total number of user-book interactions in the dataset is {}.'.format(ratings.groupby(['User_ID', 'ISBN']).count().sum().values[0]))\n",
    "print('The number of unique books: {}'.format(books.ISBN.nunique()))\n",
    "print('The number of users: {}'.format(users.User_ID.nunique()))\n",
    "print('The number of unique books that have at least 1 rating: {}'.format(books.ISBN[books.ISBN.isin(ratings.ISBN)].nunique()))\n",
    "print('The most rated book in the dataset was rated {} times.'.format(ratings.ISBN.value_counts().max()))\n",
    "print('The book name of the most rated book is \"{}\"'.format(books.loc[books.ISBN == ratings.ISBN.value_counts().index[0], 'Book_Title'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-personalized recommenders\n",
    "\n",
    "In non-personalized recommender systems, we don't take into account what users have liked and disliked before in order to suggest new things to the user. For example, if you go to amazon.com, you should expect to see things that you would like to buy regardless if you have been to amazon.com before. \n",
    "We can use the behavior of the 'population' as a whole in order to infer what users might like. Since other people like this, you will probably like it to! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here in this `NonPersonalizedRecommendation` system we build a `simple_recommendation` that does recommend books based on the simple non-personalized engine described above. But we take this a bit further in our `confidence_recommendation` system, since there are some flaws with solely using the top average rating books.\n",
    "\n",
    "\n",
    "Average Rating issues\n",
    "- Book with one 5 star would be put higher than a book with 1000 ratings with 4.95 stars. Therefore we need to build a confidence interval (we are typical strategy for ranking is to be pessimistic and use the lower bound). Suppose 2 books have 4 stars on average. Book 1 has 3 rating and Book 2 has 100, if you use the upper bound, Book 1 would be ranked higher than Book 2, which is obviously not desired. The law of large number states that the more reviews the ‘narrower’ the confidence interval (distribution), so the ‘lower bound’ will be higher! Central limit theorem . Higher number of rating > smaller CI > higher lower bound. Some issues with low rated books. In this case popularity is what really increases this score. \n",
    "- 95% of the normal distribution with CDF function. \n",
    "- Explore / exploit dilemma \n",
    "- Since we only have a rating limit of 10, we don't include smoothing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non personalized recommenders\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "class NonPersonalizedRecommendation:\n",
    "    '''Non-Personalized Recommendations'''\n",
    "    \n",
    "    def __init__(self, rating_limit=100, top_k=30, langs=[]):\n",
    "        '''\n",
    "        input: (\n",
    "            top_k: number of recommendations to return: int\n",
    "            rating_limit: lower bound for ratings: int\n",
    "            top_k: number of recommendations to return: int\n",
    "            langs: list of subsetted languages: list\n",
    "        )\n",
    "        '''\n",
    "        #  we don't include smoothing, need a value above 10\n",
    "        if rating_limit < 10: \n",
    "            raise ValueError('choose value about 10: Your value: {}'.format(rating_limit))\n",
    "        self.rating_limit = rating_limit\n",
    "        self.top_k = top_k \n",
    "        self.langs = langs\n",
    "        self.init_subset = 100\n",
    "    \n",
    "    def _compute_lower_bound(self, isbn):\n",
    "        '''\n",
    "        input: (isbn: Book ISBN: str)\n",
    "        computes a 95% confidence interval based on book ratings\n",
    "        output: (lower bound confidence interval score: float)\n",
    "        '''\n",
    "        return sms.DescrStatsW(ratings.Book_Rating[ratings.ISBN == isbn].values).tconfint_mean()[0]\n",
    "\n",
    "    def _compute_average_ratings(self, ratings, rating_limit):\n",
    "        '''\n",
    "        input: (\n",
    "            ratings: pandas dataframe\n",
    "            rating_limit: lower bound for ratings: int\n",
    "        )\n",
    "        groups books based on ISBN, computes stats and returns subseted ratings df\n",
    "        output: (pandas dataframe)\n",
    "        '''\n",
    "        average_ratings = ratings.groupby('ISBN')['Book_Rating'].agg(['mean', 'count', 'std'])\n",
    "        average_ratings = average_ratings[average_ratings['count'] > self.rating_limit]\n",
    "        return average_ratings\n",
    "\n",
    "    def _get_top_books(self, average_ratings, books, col):\n",
    "        '''\n",
    "        input: (\n",
    "            average_ratings: pandas dataframe\n",
    "            books: pandas dataframe\n",
    "            col: column to sort by: str\n",
    "        )\n",
    "        sort dataframe by 'col' and joins book information to dataframe\n",
    "        output: (pandas dataframe)\n",
    "        '''\n",
    "        top_books = average_ratings.sort_values([col], ascending=False)[:self.init_subset]    \n",
    "        top_books = top_books.merge(books, left_index=True, right_on='ISBN', how='left')\n",
    "        return top_books.dropna()\n",
    "\n",
    "    def simple_recommendation(self, ratings, books):\n",
    "        '''\n",
    "        input: (\n",
    "           ratings: pandas dataframe\n",
    "           books: pandas dataframe\n",
    "        )\n",
    "        calls `compute_average_ratings` and `get_top_books`\n",
    "        returns top_k books based on mean value\n",
    "        output: (pandas dataframe)\n",
    "        '''\n",
    "        average_ratings = self._compute_average_ratings(ratings, self.rating_limit)\n",
    "        top_books = self._get_top_books(average_ratings, books, 'mean')\n",
    "#         top_books = top_books[['Book_Title', 'Book_Author', 'mean', 'count']]\n",
    "        return top_books.sort_values(['mean'], ascending=False)[:self.top_k]\n",
    "\n",
    "    def confidence_recommendation(self, ratings, books):\n",
    "        '''\n",
    "        input: (\n",
    "           ratings: pandas dataframe\n",
    "           books: pandas dataframe\n",
    "        )\n",
    "        calls `compute_average_ratings` and `get_top_books`\n",
    "        subsets data by language\n",
    "        returns top_k books based on lower bound confidence interval score\n",
    "        output: (pandas dataframe)\n",
    "        '''\n",
    "        average_ratings = self._compute_average_ratings(ratings, self.rating_limit)\n",
    "        average_ratings['score'] = average_ratings.index.map(lambda x: self._compute_lower_bound(x))\n",
    "\n",
    "        top_books = average_ratings.sort_values(['score'], ascending=False)[:self.init_subset]\n",
    "        if (len(self.langs) > 0): books = books[(books.language.isin(self.langs))]\n",
    "        top_books = self._get_top_books(average_ratings, books, 'score')\n",
    "        top_books = top_books[['Book_Title', 'Book_Author', 'mean', 'count', 'score']]\n",
    "        return top_books.sort_values(['score'], ascending=False)[:self.top_k]\n",
    "    \n",
    "    def diversity_score(self, model):\n",
    "        interactions_top = ratings[ratings.ISBN.isin(list(model.ISBN))].pivot(index='User_ID', columns='ISBN')['Book_Rating']\n",
    "        mean_corr = 0\n",
    "        for book in list(model.ISBN):\n",
    "            mean_corr += interactions_top.corrwith(interactions_top[book]).mean()    \n",
    "        mean_corr /= len(list(model.ISBN))\n",
    "        print(1 - mean_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NonPersonalizedRecommendation(rating_limit=10, top_k=10, langs=['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>std</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1888054557</td>\n",
       "      <td>Postmarked Yesteryear: 30 Rare Holiday Postcards</td>\n",
       "      <td>Pamela E. Apkarian-Russell</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Collectors Press</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64267</th>\n",
       "      <td>9.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>0.277350</td>\n",
       "      <td>0836213319</td>\n",
       "      <td>Dilbert: A Book of Postcards</td>\n",
       "      <td>Scott Adams</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Andrews McMeel Pub</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79370</th>\n",
       "      <td>9.869565</td>\n",
       "      <td>23</td>\n",
       "      <td>0.344350</td>\n",
       "      <td>0439425220</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets Postca...</td>\n",
       "      <td>J. K. Rowling</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10038</th>\n",
       "      <td>9.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>0.425815</td>\n",
       "      <td>0394800389</td>\n",
       "      <td>Fox in Socks (I Can Read It All by Myself Begi...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>Random House Children's Books</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>9.750000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.638666</td>\n",
       "      <td>0060256656</td>\n",
       "      <td>The Giving Tree</td>\n",
       "      <td>Shel Silverstein</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>9.727273</td>\n",
       "      <td>11</td>\n",
       "      <td>0.646670</td>\n",
       "      <td>0312099045</td>\n",
       "      <td>Route 66 Postcards: Greetings from the Mother ...</td>\n",
       "      <td>Michael Wallis</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>St. Martin's Press</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12762</th>\n",
       "      <td>9.720000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.613732</td>\n",
       "      <td>0618002235</td>\n",
       "      <td>The Two Towers (The Lord of the Rings, Part 2)</td>\n",
       "      <td>J. R. R. Tolkien</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Houghton Mifflin Company</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12761</th>\n",
       "      <td>9.625000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.718795</td>\n",
       "      <td>0618002243</td>\n",
       "      <td>The Return of the King (The Lord of The Rings,...</td>\n",
       "      <td>J. R. R. Tolkien</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Houghton Mifflin Company</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4066</th>\n",
       "      <td>9.600000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.680557</td>\n",
       "      <td>0836218221</td>\n",
       "      <td>The Authoritative Calvin and Hobbes (Calvin an...</td>\n",
       "      <td>Bill Watterson</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>Andrews McMeel Publishing</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15187</th>\n",
       "      <td>9.583333</td>\n",
       "      <td>24</td>\n",
       "      <td>0.717282</td>\n",
       "      <td>0836220889</td>\n",
       "      <td>Calvin and Hobbes</td>\n",
       "      <td>Bill Watterson</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>Andrews McMeel Publishing</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean  count       std        ISBN  \\\n",
       "5871   10.000000     11  0.000000  1888054557   \n",
       "64267   9.923077     13  0.277350  0836213319   \n",
       "79370   9.869565     23  0.344350  0439425220   \n",
       "10038   9.785714     14  0.425815  0394800389   \n",
       "13300   9.750000     20  0.638666  0060256656   \n",
       "5872    9.727273     11  0.646670  0312099045   \n",
       "12762   9.720000     25  0.613732  0618002235   \n",
       "12761   9.625000     16  0.718795  0618002243   \n",
       "4066    9.600000     20  0.680557  0836218221   \n",
       "15187   9.583333     24  0.717282  0836220889   \n",
       "\n",
       "                                              Book_Title  \\\n",
       "5871    Postmarked Yesteryear: 30 Rare Holiday Postcards   \n",
       "64267                       Dilbert: A Book of Postcards   \n",
       "79370  Harry Potter and the Chamber of Secrets Postca...   \n",
       "10038  Fox in Socks (I Can Read It All by Myself Begi...   \n",
       "13300                                    The Giving Tree   \n",
       "5872   Route 66 Postcards: Greetings from the Mother ...   \n",
       "12762     The Two Towers (The Lord of the Rings, Part 2)   \n",
       "12761  The Return of the King (The Lord of The Rings,...   \n",
       "4066   The Authoritative Calvin and Hobbes (Calvin an...   \n",
       "15187                                  Calvin and Hobbes   \n",
       "\n",
       "                      Book_Author  Year_Of_Publication  \\\n",
       "5871   Pamela E. Apkarian-Russell               2001.0   \n",
       "64267                 Scott Adams               1996.0   \n",
       "79370               J. K. Rowling               2002.0   \n",
       "10038                   Dr. Seuss               1965.0   \n",
       "13300            Shel Silverstein               1964.0   \n",
       "5872               Michael Wallis               1993.0   \n",
       "12762            J. R. R. Tolkien               1999.0   \n",
       "12761            J. R. R. Tolkien               1999.0   \n",
       "4066               Bill Watterson               1990.0   \n",
       "15187              Bill Watterson               1987.0   \n",
       "\n",
       "                           Publisher language  \n",
       "5871                Collectors Press       en  \n",
       "64267             Andrews McMeel Pub       en  \n",
       "79370                     Scholastic       en  \n",
       "10038  Random House Children's Books       en  \n",
       "13300       HarperCollins Publishers       en  \n",
       "5872              St. Martin's Press       en  \n",
       "12762       Houghton Mifflin Company       en  \n",
       "12761       Houghton Mifflin Company       en  \n",
       "4066       Andrews McMeel Publishing       en  \n",
       "15187      Andrews McMeel Publishing       en  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_N_mean_model = model.simple_recommendation(ratings, books)\n",
    "top_N_mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>Postmarked Yesteryear: 30 Rare Holiday Postcards</td>\n",
       "      <td>Pamela E. Apkarian-Russell</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64267</th>\n",
       "      <td>Dilbert: A Book of Postcards</td>\n",
       "      <td>Scott Adams</td>\n",
       "      <td>9.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>9.755476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79370</th>\n",
       "      <td>Harry Potter and the Chamber of Secrets Postca...</td>\n",
       "      <td>J. K. Rowling</td>\n",
       "      <td>9.869565</td>\n",
       "      <td>23</td>\n",
       "      <td>9.720657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10038</th>\n",
       "      <td>Fox in Socks (I Can Read It All by Myself Begi...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>9.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>9.539856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12762</th>\n",
       "      <td>The Two Towers (The Lord of the Rings, Part 2)</td>\n",
       "      <td>J. R. R. Tolkien</td>\n",
       "      <td>9.720000</td>\n",
       "      <td>25</td>\n",
       "      <td>9.466664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>The Giving Tree</td>\n",
       "      <td>Shel Silverstein</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>20</td>\n",
       "      <td>9.451095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30743</th>\n",
       "      <td>My Sister's Keeper : A Novel (Picoult, Jodi)</td>\n",
       "      <td>Jodi Picoult</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>22</td>\n",
       "      <td>9.319490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>Route 66 Postcards: Greetings from the Mother ...</td>\n",
       "      <td>Michael Wallis</td>\n",
       "      <td>9.727273</td>\n",
       "      <td>11</td>\n",
       "      <td>9.292834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4066</th>\n",
       "      <td>The Authoritative Calvin and Hobbes (Calvin an...</td>\n",
       "      <td>Bill Watterson</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>20</td>\n",
       "      <td>9.281489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15187</th>\n",
       "      <td>Calvin and Hobbes</td>\n",
       "      <td>Bill Watterson</td>\n",
       "      <td>9.583333</td>\n",
       "      <td>24</td>\n",
       "      <td>9.280452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Book_Title  \\\n",
       "5871    Postmarked Yesteryear: 30 Rare Holiday Postcards   \n",
       "64267                       Dilbert: A Book of Postcards   \n",
       "79370  Harry Potter and the Chamber of Secrets Postca...   \n",
       "10038  Fox in Socks (I Can Read It All by Myself Begi...   \n",
       "12762     The Two Towers (The Lord of the Rings, Part 2)   \n",
       "13300                                    The Giving Tree   \n",
       "30743       My Sister's Keeper : A Novel (Picoult, Jodi)   \n",
       "5872   Route 66 Postcards: Greetings from the Mother ...   \n",
       "4066   The Authoritative Calvin and Hobbes (Calvin an...   \n",
       "15187                                  Calvin and Hobbes   \n",
       "\n",
       "                      Book_Author       mean  count      score  \n",
       "5871   Pamela E. Apkarian-Russell  10.000000     11  10.000000  \n",
       "64267                 Scott Adams   9.923077     13   9.755476  \n",
       "79370               J. K. Rowling   9.869565     23   9.720657  \n",
       "10038                   Dr. Seuss   9.785714     14   9.539856  \n",
       "12762            J. R. R. Tolkien   9.720000     25   9.466664  \n",
       "13300            Shel Silverstein   9.750000     20   9.451095  \n",
       "30743                Jodi Picoult   9.545455     22   9.319490  \n",
       "5872               Michael Wallis   9.727273     11   9.292834  \n",
       "4066               Bill Watterson   9.600000     20   9.281489  \n",
       "15187              Bill Watterson   9.583333     24   9.280452  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.confidence_recommendation(ratings, books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Filtering \n",
    "\n",
    "Content Based Filtering builds a system that recommends books that are similar to a particular book. More specifically, we compute pairwise similarity scores for all books based on their Book_Title, Book_Author, and Publisher and recommend books based on that similarity score.\n",
    "\n",
    "This should help discover similar books based on the same author, publisher, and book title. This model is computed naively, as all the Book_Title, Book_Author, and Publisher are weighted equally based on the raw counts of unigrams. \n",
    "\n",
    "Another addition in this model is to use the year the book was released and recommend books weigh books within the same decade higher using an exponential decay based on the year comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_fields(x, combine=False):\n",
    "    x = x.lower()\n",
    "    x = re.sub('[^a-z0-9 ]', '', x)\n",
    "    if combine:\n",
    "        return ''.join(x.replace(' ', '') for x in x).strip()\n",
    "    return x.strip()\n",
    "\n",
    "# sort and return top\n",
    "def sort_sims(sim_scores, topN=10):\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    return sim_scores[:topN+1]\n",
    "        \n",
    "\n",
    "\n",
    "class ContentBased(object):\n",
    "    '''Content Based Filtering'''\n",
    "    def __init__(self, ratings, books):\n",
    "        self.ratings = ratings\n",
    "        books = self.prepare_data(books, ratings)\n",
    "        self.cosine_sim, self.books, self.indices = self.compute_similarity(books)\n",
    "        \n",
    "    def prepare_data(self, books, ratings, rating_threshold=2):\n",
    "        books = books[books.ISBN.isin(ratings.ISBN)]\n",
    "        books = books.drop_duplicates(subset=['Book_Title'])\n",
    "        most_popular_ISBN = list(ratings.ISBN.value_counts()[ratings.ISBN.value_counts() >= rating_threshold].index)\n",
    "        books = books[books.ISBN.isin(most_popular_ISBN)]\n",
    "        books['Publisher'] = books.Publisher.map(lambda x: clean_fields(x, combine=True))\n",
    "        books['Book_Author'] = books.Book_Author.map(lambda x: clean_fields(x, combine=True))\n",
    "        books['Book_Title_Clean'] = books.Book_Title.map(lambda x: clean_fields(x, combine=False))\n",
    "        books['soup'] = books['Book_Title_Clean'] + ' ' + books['Book_Author'] + ' ' + books['Publisher']\n",
    "        return books \n",
    "        \n",
    "    def compute_similarity(self, books):\n",
    "        vect = CountVectorizer(stop_words='english', max_features=1000)\n",
    "        count_matrix = vect.fit_transform(books['soup'])\n",
    "        cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "        books = books.reset_index()\n",
    "        indices = pd.Series(books.index, index=books['Book_Title']).drop_duplicates()\n",
    "        return cosine_sim, books, indices\n",
    "    \n",
    "    def compute_year_similarity(self, book1, book2, value=10):\n",
    "        diff = abs(book1 - book2)\n",
    "        sim = np.exp(-diff / value)\n",
    "        return sim\n",
    "        \n",
    "    def get_recommendations(self, title):\n",
    "        # Get the index of the book that matches the title\n",
    "        idx = self.indices[title]\n",
    "        year = self.books.Year_Of_Publication[self.books['Book_Title'] == title]\n",
    "        if isinstance(idx, pd.Series):\n",
    "            idx = idx.iloc[0]\n",
    "            year = year.iloc[0]\n",
    "        # Get the pairwsie similarity scores of all books with that book\n",
    "        sim_scores = list(enumerate(self.cosine_sim[idx]))\n",
    "        sim_scores = sort_sims(sim_scores, topN=50)\n",
    "\n",
    "        # Get the book indices\n",
    "        book_indices = [i[0] for i in sim_scores]\n",
    "        year_scores = self.books.Year_Of_Publication.iloc[book_indices].map(\n",
    "                                                    lambda x: self.compute_year_similarity(x, year)).values\n",
    "        # multiply tfidf scores with year scores \n",
    "        final_scores = list(zip(book_indices, np.array([i[1] for i in sim_scores]) * year_scores))\n",
    "        sim_scores = sort_sims(sim_scores, topN=10)\n",
    "\n",
    "        # remove current title\n",
    "        book_indices = []\n",
    "        for i, _ in sim_scores:\n",
    "            if i != idx:\n",
    "                book_indices.append(i)\n",
    "\n",
    "        # Return the top 10 most similar books\n",
    "        return self.books['Book_Title'].iloc[book_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_rec = ContentBased(ratings, books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16594            Silmarillion\n",
       "12                   Airframe\n",
       "13                   Timeline\n",
       "46                 Seabiscuit\n",
       "140                     Congo\n",
       "141        Protect and Defend\n",
       "142       The Tall Pine Polka\n",
       "338              Househusband\n",
       "498      My Name Is Asher Lev\n",
       "701           Degree of Guilt\n",
       "Name: Book_Title, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_rec.get_recommendations(\"The Hobbit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering -- User based k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is very computationally expensive, we will only use a rating count of 200, users with 200 ratings and books with more than 1 rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Ratings Shape (24036, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ISBN</th>\n",
       "      <th>0001056107</th>\n",
       "      <th>0002570122</th>\n",
       "      <th>0006157629</th>\n",
       "      <th>000624565X</th>\n",
       "      <th>0006379702</th>\n",
       "      <th>0006472427</th>\n",
       "      <th>000648302X</th>\n",
       "      <th>0006496423</th>\n",
       "      <th>000649840X</th>\n",
       "      <th>0006498744</th>\n",
       "      <th>...</th>\n",
       "      <th>8485224752</th>\n",
       "      <th>8486433525</th>\n",
       "      <th>8489669635</th>\n",
       "      <th>849550121X</th>\n",
       "      <th>8496246620</th>\n",
       "      <th>9500700891</th>\n",
       "      <th>9500723549</th>\n",
       "      <th>9536000444</th>\n",
       "      <th>9706612084</th>\n",
       "      <th>O67174142X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8621 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ISBN     0001056107  0002570122  0006157629  000624565X  0006379702  \\\n",
       "User_ID                                                               \n",
       "2276            0.0         0.0         0.0         0.0         0.0   \n",
       "3757            0.0         0.0         0.0         0.0         0.0   \n",
       "4385            0.0         0.0         0.0         0.0         0.0   \n",
       "6242            0.0         0.0         0.0         0.0         0.0   \n",
       "6251            0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "ISBN     0006472427  000648302X  0006496423  000649840X  0006498744  \\\n",
       "User_ID                                                               \n",
       "2276            0.0         0.0         0.0         0.0         0.0   \n",
       "3757            0.0         0.0         0.0         0.0         0.0   \n",
       "4385            0.0         0.0         0.0         0.0         0.0   \n",
       "6242            0.0         0.0         0.0         0.0         0.0   \n",
       "6251            0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "ISBN        ...      8485224752  8486433525  8489669635  849550121X  \\\n",
       "User_ID     ...                                                       \n",
       "2276        ...             0.0         0.0         0.0         0.0   \n",
       "3757        ...             5.0         0.0         0.0         0.0   \n",
       "4385        ...             0.0         0.0         0.0         0.0   \n",
       "6242        ...             0.0         0.0         0.0         0.0   \n",
       "6251        ...             0.0         0.0         0.0         0.0   \n",
       "\n",
       "ISBN     8496246620  9500700891  9500723549  9536000444  9706612084  \\\n",
       "User_ID                                                               \n",
       "2276            0.0         0.0         0.0         0.0         0.0   \n",
       "3757            0.0         0.0         0.0         0.0         0.0   \n",
       "4385            0.0         0.0         0.0         0.0         0.0   \n",
       "6242            0.0         0.0         0.0         0.0         0.0   \n",
       "6251            0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "ISBN     O67174142X  \n",
       "User_ID              \n",
       "2276            0.0  \n",
       "3757            0.0  \n",
       "4385            0.0  \n",
       "6242            0.0  \n",
       "6251            0.0  \n",
       "\n",
       "[5 rows x 8621 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ratings = popular_ratings(ratings, user_threshold=200, rating_threshold=200, book_threshold=1)\n",
    "print('Sample Ratings Shape', sample_ratings.shape)\n",
    "book_ratings = sample_ratings.pivot_table(index='User_ID', columns='ISBN', values='Book_Rating').fillna(0)\n",
    "book_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from sortedcontainers import SortedList\n",
    "\n",
    "from BookData import BookDataSet\n",
    "from EvaluationData import CreateDataSets\n",
    "from EvaluatedAlgorithm import EvaluatedAlgorithm\n",
    "\n",
    "\n",
    "class SimpleCollaborativeFiltering:\n",
    "    \n",
    "    def __init__(self, ratings, books, users, k=10, max_rating=10.0):\n",
    "        self.data = BookDataSet(ratings, books, users)\n",
    "        self.k = k\n",
    "        self.max_rating = max_rating\n",
    "        \n",
    "    def get_neighbors(self, user_id, user_based_=True, metric='cosine', verbose=False):\n",
    "        # create model\n",
    "        self.train = self.data.build_full_trainset()\n",
    "\n",
    "        sim_options = {'name': metric,\n",
    "                       'user_based': user_based_\n",
    "        }\n",
    "\n",
    "        model = KNNBasic(sim_options=sim_options, verbose=verbose)\n",
    "        model.fit(self.train)\n",
    "        interations = model.compute_similarities()\n",
    "\n",
    "        self.surprise_user_id = self.train.to_inner_uid(user_id)\n",
    "        similarity_rows = interations[self.surprise_user_id]\n",
    "        return interations, similarity_rows\n",
    "\n",
    "    \n",
    "    def show_top_books(self, candidates, watched_list):\n",
    "        # print out top 10 books and score \n",
    "        N = 0\n",
    "        print('Top {} Book Recommendations:'.format(self.k))\n",
    "        print('\\n')\n",
    "        for item_id, ratings in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
    "            if item_id not in watched_list:\n",
    "                book_id = self.train.to_raw_iid(item_id)\n",
    "                book_name = self.data.get_book_name(book_id)\n",
    "                book_author = self.data.get_book_author(book_id)\n",
    "                book_year = self.data.get_book_year(book_id)\n",
    "                try:\n",
    "                    book_year = int(book_year) \n",
    "                except:\n",
    "                    book_year = 'Not Available'\n",
    "                print('{} - {} - ({}) - Score: {}'.format(book_name, book_author, book_year, round(ratings, 2)))\n",
    "                N += 1\n",
    "                if (N > self.k): break\n",
    "        \n",
    "    def user_based(self, user_id, threshold=False):\n",
    "        '''\n",
    "        input: (\n",
    "            user_id\n",
    "        )\n",
    "        user based collaborative filtering\n",
    "        '''\n",
    "        _, similarity_rows = self.get_neighbors(user_id)\n",
    "        \n",
    "        similar_users = SortedList(key=lambda x: -x[1])\n",
    "        for i, score in enumerate(similarity_rows):\n",
    "            if i != user_id:\n",
    "                similar_users.add((i, score))\n",
    "        \n",
    "        if threshold:\n",
    "            similar_users = [rating for rating in similar_users if rating[1] >= 0.95]\n",
    "\n",
    "        candidates = defaultdict(float)\n",
    "        for similar_user in similar_users[:self.k]:\n",
    "            surprise_sim_user_idx, score = similar_user\n",
    "            sim_user_rating = self.train.ur[surprise_sim_user_idx]\n",
    "            ### NEED TO GET MEANS FROM sim_user_rating\n",
    "            for info in sim_user_rating:\n",
    "                book_id, rating = info\n",
    "                # use += and increase weight for books that appear more than once \n",
    "                candidates[book_id] += (rating / self.max_rating) * score\n",
    "\n",
    "        # list of books that the user has seen\n",
    "        watched_list = [book_id for book_id, rating in self.train.ur[self.surprise_user_id]]\n",
    "\n",
    "        # Get top-rated items from similar users\n",
    "        self.show_top_books(candidates, watched_list)\n",
    "        \n",
    "    def item_based(self, user_id, threshold=False):\n",
    "        interations, _ = self.get_neighbors(user_id, user_based_=False)\n",
    "\n",
    "        # Get the top K items we rated\n",
    "        test_user_ratings = self.train.ur[self.surprise_user_id]\n",
    "\n",
    "        if threshold:\n",
    "            kNeighbors = [rating for rating in test_user_ratings if rating[1] >= 7.0]\n",
    "        else:\n",
    "            kNeighbors = heapq.nlargest(self.k, test_user_ratings, key=lambda t: t[1])\n",
    "        \n",
    "        # Get similar items to stuff we liked (weighted by rating)\n",
    "        candidates = defaultdict(float)\n",
    "        for item_id, rating in kNeighbors:\n",
    "            similarity_row = interations[item_id]\n",
    "            for inner_id, score in enumerate(similarity_row):\n",
    "                candidates[inner_id] += score * (rating / 10.0)\n",
    "\n",
    "        # list of books that the user has seen\n",
    "        watched_list = [book_id for book_id, rating in self.train.ur[self.surprise_user_id]]\n",
    "        \n",
    "        # Get top-rated items from similar items\n",
    "        self.show_top_books(candidates, watched_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 6242\n",
    "cf = SimpleCollaborativeFiltering(sample_ratings, books, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Book Recommendations:\n",
      "\n",
      "\n",
      "All Around the Town - Mary Higgins Clark - (1993) - Score: 2.6\n",
      "Gerald's Game - Stephen King - (2001) - Score: 2.6\n",
      "The Scarlet Letter - NATHANIEL HAWTHORNE - (1965) - Score: 2.5\n",
      "Dolores Claiborne - Stephen King - (2004) - Score: 2.5\n",
      "The Dark Half - Stephen King - (1994) - Score: 2.4\n",
      "Loves Music, Loves to Dance - Mary Higgins Clark - (1992) - Score: 2.4\n",
      "One Door Away from Heaven - Dean R. Koontz - (2002) - Score: 2.1\n",
      "Harry Potter and the Prisoner of Azkaban (Book 3) - J. K. Rowling - (1999) - Score: 2.0\n",
      "Harry Potter and the Goblet of Fire (Book 4) - J. K. Rowling - (2000) - Score: 2.0\n",
      "Stuart Little - E.B. White - (Not Available) - Score: 2.0\n",
      "It Came From The Far Side - Gary Larson - (1986) - Score: 2.0\n"
     ]
    }
   ],
   "source": [
    "cf.user_based(user_id=user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Book Recommendations:\n",
      "\n",
      "\n",
      "Como Agua Para Chocolate/Like Water for Chocolate - Laura Esquivel - (2001) - Score: 8.0\n",
      "Naked Lunch - William S. Burroughs - (1992) - Score: 8.0\n",
      "Slaughterhouse Five or the Children's Crusade: A Duty Dance With Death - Kurt Vonnegut - (1991) - Score: 8.0\n",
      "One Hundred Years of Solitude - Gabriel Garcia Marquez - (1998) - Score: 7.99\n",
      "The Phantom Tollbooth - Norton Juster - (1993) - Score: 7.99\n",
      "Shiloh (Yearling Newbery) - Phyllis Reynolds Naylor - (1992) - Score: 7.99\n",
      "Lakota Woman - Dog Mary Crow - (1991) - Score: 7.98\n",
      "Girl in Hyacinth Blue - Susan Vreeland - (2000) - Score: 7.97\n",
      "The World According to Garp - John Irving - (1994) - Score: 7.92\n",
      "Anne Frank: The Diary of a Young Girl - ANNE FRANK - (1993) - Score: 7.91\n",
      "Blue Diary - Alice Hoffman - (2002) - Score: 7.89\n"
     ]
    }
   ],
   "source": [
    "cf.item_based(user_id=user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  SVD ...\n",
      "Evaluating accuracy...\n",
      "Training Time: 0:00:12.742640\n",
      "Prediction Time: 0:00:00.704133\n",
      "Analysis complete.\n",
      "Evaluating  SVD++ ...\n",
      "Evaluating accuracy...\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD, SVDpp\n",
    "\n",
    "data = BookDataSet(ratings, books, users)\n",
    "rankings = data.get_popularity_ranks()\n",
    "evaluator = Evaluator(data, rankings)\n",
    "\n",
    "# SVD\n",
    "SVD = SVD()\n",
    "evaluator.add_model(SVD, 'SVD')\n",
    "\n",
    "# SVD++\n",
    "SVDPlusPlus = SVDpp()\n",
    "evaluator.add_model(SVDPlusPlus, 'SVD++')\n",
    "\n",
    "\n",
    "\n",
    "# get RMSE and MAE scores \n",
    "evaluator.evaluate(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score attained: 1.643737678188077\n",
      "{'n_epochs': 20, 'lr_all': 0.005, 'n_factors': 50}\n",
      "Evaluating  SVD - Tuned ...\n",
      "Evaluating accuracy...\n",
      "Analysis complete.\n",
      "Evaluating  SVD - Untuned ...\n",
      "Evaluating accuracy...\n",
      "Analysis complete.\n",
      "\n",
      "\n",
      "Algorithm  RMSE       MAE       \n",
      "SVD - Tuned 1.6348     1.2615    \n",
      "SVD - Untuned 1.6374     1.2654    \n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD, SVDpp\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "data = BookDataSet(ratings, books, users)\n",
    "rankings = data.get_popularity_ranks()\n",
    "evaluator = Evaluator(data, rankings)\n",
    "\n",
    "param_grid = {'n_epochs': [20, 30], 'lr_all': [0.005, 0.010], 'n_factors': [50, 100]}\n",
    "\n",
    "grid = GridSearchCV(SVD, param_grid=param_grid, cv=3)\n",
    "\n",
    "grid.fit(data)\n",
    "\n",
    "print('Best RMSE score attained: {}'.format(grid.best_score['rmse']))\n",
    "\n",
    "# # combination of parameters that gave the best RMSE score\n",
    "print(grid.best_params['rmse'])\n",
    "\n",
    "# Construct an Evaluator to, you know, evaluate them\n",
    "evaluator = Evaluator(data, rankings)\n",
    "\n",
    "params = grid.best_params['rmse']\n",
    "SVDtuned = SVD(n_epochs=params['n_epochs'], lr_all=params['lr_all'], n_factors=params['n_factors'])\n",
    "evaluator.add_model(SVDtuned, \"SVD - Tuned\")\n",
    "\n",
    "SVDUntuned = SVD()\n",
    "evaluator.add_model(SVDUntuned, \"SVD - Untuned\")\n",
    "\n",
    "# get RMSE and MAE scores \n",
    "evaluator.evaluate(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using recommender  SVD - Tuned\n",
      "\n",
      "Building recommendation model...\n",
      "Computing recommendations...\n",
      "\n",
      "We recommend:\n",
      "The Two Towers (The Lord of the Rings, Part 2) 9.91\n",
      " 9.88\n",
      "Ender's Game (Ender Wiggins Saga (Paperback)) 9.82\n",
      "Fast Food Nation: The Dark Side of the All-American Meal 9.8\n",
      "Weirdos From Another Planet! 9.79\n",
      "84 Charing Cross Road 9.78\n",
      "The Red Tent (Bestselling Backlist) 9.78\n",
      "Calvin and Hobbes 9.74\n",
      "Dilbert: A Book of Postcards 9.73\n",
      "Maus 1. Mein Vater kotzt Geschichte aus. Die Geschichte eines Ã?Â?berlebenden. 9.71\n",
      "\n",
      "Using recommender  SVD - Untuned\n",
      "\n",
      "Building recommendation model...\n",
      "Computing recommendations...\n",
      "\n",
      "We recommend:\n",
      "My Sister's Keeper : A Novel (Picoult, Jodi) 10\n",
      "Harry Potter and the Chamber of Secrets Postcard Book 10\n",
      "Harry Potter and the Prisoner of Azkaban (Book 3) 9.99\n",
      " 9.93\n",
      "The Lovely Bones: A Novel 9.89\n",
      "The Little Prince 9.85\n",
      "Calvin and Hobbes 9.84\n",
      "Harry Potter and the Goblet of Fire (Book 4) 9.81\n",
      "Outlander 9.8\n",
      "52 Deck Series: 52 Ways to Celebrate Friendship 9.75\n"
     ]
    }
   ],
   "source": [
    "evaluator.recommend_top_books(data, test_user_id=276847)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, SVDpp\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "data = BookDataSet(ratings, books, users)\n",
    "rankings = data.get_popularity_ranks()\n",
    "evaluator = Evaluator(data, rankings)\n",
    "\n",
    "param_grid = {'n_epochs': [20, 30], 'lr_all': [0.005, 0.010], 'n_factors': [50, 100]}\n",
    "\n",
    "grid = GridSearchCV(SVDpp, param_grid=param_grid, cv=3)\n",
    "\n",
    "grid.fit(data)\n",
    "\n",
    "print('Best RMSE score attained: {}'.format(grid.best_score['rmse']))\n",
    "\n",
    "# # combination of parameters that gave the best RMSE score\n",
    "print(grid.best_params['rmse'])\n",
    "\n",
    "# Construct an Evaluator to, you know, evaluate them\n",
    "evaluator = Evaluator(data, rankings)\n",
    "\n",
    "params = grid.best_params['rmse']\n",
    "SVDpp_tuned = SVDpp(n_epochs=params['n_epochs'], lr_all=params['lr_all'], n_factors=params['n_factors'])\n",
    "evaluator.add_model(SVDpp_tuned, \"SVD++ - Tuned\")\n",
    "\n",
    "SVDpp_untuned = SVD()\n",
    "evaluator.add_model(SVDpp_untuned, \"SVD++ - Untuned\")\n",
    "\n",
    "# get RMSE and MAE scores \n",
    "evaluator.evaluate(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
