{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "import pprint\n",
    "from operator import itemgetter\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix, csr_matrix, save_npz, load_npz\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_feather(filepath, **kwargs):\n",
    "    '''\n",
    "    input: (path to feather file)\n",
    "    read feather file to pandas dataframe\n",
    "    output: (pandas dataframe)\n",
    "    '''\n",
    "    return pd.read_feather(filepath, **kwargs)\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"Loads three datasets of ratings, users, books\"\"\"\n",
    "    ratings = load_feather(DATA_PATH/'ratings_explicit_clean.feather')\n",
    "    ratings.drop(['index'], axis=1, inplace=True)\n",
    "    users = load_feather(DATA_PATH/'users_clean.feather')\n",
    "    books = load_feather(DATA_PATH/'books_clean.feather')\n",
    "    return ratings, users, books\n",
    "\n",
    "\n",
    "ratings = load_feather(DATA_PATH/'ratings_explicit_clean.feather')\n",
    "ratings.drop(['index'], axis=1, inplace=True)\n",
    "users = load_feather(DATA_PATH/'users_clean.feather')\n",
    "books = load_feather(DATA_PATH/'books_clean.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_uniq = ratings.User_ID.unique()\n",
    "user2idx = {o:i for i,o in enumerate(u_uniq)}\n",
    "ratings['New_User_ID'] = ratings.User_ID.apply(lambda x: user2idx[x])\n",
    "\n",
    "m_uniq = ratings.ISBN.unique()\n",
    "book2idx = {o:i for i,o in enumerate(m_uniq)}\n",
    "ratings['New_Book_ID'] = ratings.ISBN.apply(lambda x: book2idx[x])\n",
    "\n",
    "n_users = int(ratings.New_User_ID.nunique())\n",
    "n_books = int(ratings.New_Book_ID.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std of Year_Of_Publication for User\n",
    "def get_year_of_pub(ratings):\n",
    "    '''\n",
    "    create features based on Year_Of_Publication and users\n",
    "    fill missing values with mean Year_Of_Publication\n",
    "    '''\n",
    "    ratings['Year_Of_Publication'] = ratings.Year_Of_Publication.fillna(round(ratings.Year_Of_Publication.mean()))\n",
    "    user_group = ratings.groupby('User_ID', as_index=False)['Year_Of_Publication'].agg(\n",
    "                            {'User_Mean_Year_Of_Publication': 'mean', 'User_Std_Year_Of_Publication': 'std'})\n",
    "    user_group['User_Mean_Year_Of_Publication'] = round(user_group['User_Mean_Year_Of_Publication'], 2)\n",
    "    user_group['User_Std_Year_Of_Publication'] = round(user_group['User_Std_Year_Of_Publication'].fillna(0), 2)\n",
    "    ratings = ratings.merge(user_group, on='User_ID', how='left')\n",
    "    features = ['Year_Of_Publication', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication']\n",
    "    return ratings, features\n",
    "\n",
    "def get_age(ratings, log=False):\n",
    "    '''\n",
    "    log transform age if `log` and impute missing values with age median \n",
    "    '''\n",
    "    if log: ratings['Age'] = np.log(ratings['Age'])\n",
    "    ratings['Age'] = ratings.Age.fillna(round(ratings.Age.median()))\n",
    "    features = ['Age']\n",
    "    return ratings, features\n",
    "\n",
    "\n",
    "def clean_text(df, col, threshold=None, missing_value=None, mappings=None):\n",
    "    '''\n",
    "    removes punct and lowers and joins text to single string\n",
    "    '''\n",
    "    df[col] = df[col].str.lower().replace(r'[^A-Za-z]', '', regex=True)\n",
    "    if mappings:\n",
    "        df[col] = df[col].map(lambda x: mappings[x] if x in mappings else x)\n",
    "    if threshold:\n",
    "        counts = df[col].value_counts()\n",
    "        df.loc[df[col].isin(list(counts[counts < threshold].index)), col] = np.nan\n",
    "    if missing_value: df[col] = df[col].fillna(missing_value)\n",
    "    return df, col\n",
    "\n",
    "def numericalize_col(ratings, col):\n",
    "    '''\n",
    "    convert string to category codes \n",
    "    '''\n",
    "    ratings[col] = ratings[col].astype('category').cat.codes\n",
    "    return ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import ignite\n",
    "from ignite.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanSquaredError\n",
    "from ignite.engine import Events, create_supervised_evaluator, Engine\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "from ignite.handlers import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "class MixedInputDataSet(Dataset):\n",
    "    '''Mixed Input Torch DataDataSet'''\n",
    "    def __init__(self, cats, conts, y):\n",
    "        '''\n",
    "        Args:\n",
    "            cats (pandas df): pandas dataframe with categorical features \n",
    "            conts (pandas df): pandas dataframe with numerical features \n",
    "            y (list or pandas series): book rating list or series \n",
    "        '''\n",
    "        self.cats = np.asarray(cats, dtype=np.int64)\n",
    "        self.conts = np.asarray(conts, dtype=np.float32)\n",
    "        self.N_cats = self.cats.shape[1]\n",
    "        self.N_conts = self.conts.shape[1]\n",
    "        self.X = np.hstack((self.cats, self.conts))\n",
    "        self.N = len(y)\n",
    "        y = np.zeros((n,1)) if y is None else y[:,None]\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "            \n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "def init_embeddings(x):\n",
    "    x = x.weight.data\n",
    "    value = 2 / (x.size(1) + 1)\n",
    "    x.uniform_(-value, value)\n",
    "    \n",
    "class MiniNetPlusMeta(nn.Module):\n",
    "    def __init__(self, emb_szs, layer_sizes=[256, 100], n_factors=100, drops=[0.5, 0.5], emb_drop=0.1, \n",
    "                         y_range=(1, 10), n_cont=0, n_cat=2):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(emb_szs['n_users'], n_factors)\n",
    "        self.book_embedding = nn.Embedding(emb_szs['n_books'], n_factors)\n",
    "        self.author_embedding = nn.Embedding(emb_szs['Book_Author'], n_factors)\n",
    "        self.publisher_embedding = nn.Embedding(emb_szs['Publisher'], n_factors)\n",
    "        self.city_embedding = nn.Embedding(emb_szs['city'], n_factors)\n",
    "        self.state_embedding = nn.Embedding(emb_szs['state'], n_factors)\n",
    "        self.country_embedding = nn.Embedding(emb_szs['country'], n_factors)\n",
    "        \n",
    "        self.embeddings = [self.user_embedding, self.book_embedding, self.author_embedding, self.publisher_embedding, \n",
    "                           self.city_embedding, self.state_embedding, self.country_embedding]\n",
    "        \n",
    "        self.lin_size = (n_factors * n_cat) + n_cont\n",
    "        self.linear1 = nn.Linear(self.lin_size, layer_sizes[0])\n",
    "        self.linear2 = nn.Linear(layer_sizes[0], layer_sizes[1])\n",
    "        self.linear3 = nn.Linear(layer_sizes[1], 1)\n",
    "        self.linears = [self.linear1, self.linear2, self.linear3]\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.bn_lin = nn.BatchNorm1d(layer_sizes[0])\n",
    "        self.bn_lin2 = nn.BatchNorm1d(layer_sizes[1])\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drop1 = nn.Dropout(drops[0])\n",
    "        self.drop2 = nn.Dropout(drops[1])\n",
    "        \n",
    "        for layer in self.embeddings:\n",
    "            init_embeddings(layer)\n",
    "            \n",
    "        for layer in self.linears:\n",
    "            nn.init.kaiming_normal(layer.weight.data)\n",
    "        \n",
    "        self.n_cont = n_cont\n",
    "        self.n_cat = n_cat\n",
    "        self.y_range = y_range\n",
    "    \n",
    "    def forward(self, X):\n",
    "        cats, conts = X[:, :self.n_cat], X[:, self.n_cat:]\n",
    "        cats, conts = cats.long(), conts.float()\n",
    "        cat_list = [self.embeddings[x](cats[:,x]) for x in range(self.n_cat)]\n",
    "        X = torch.cat(cat_list, dim=1)\n",
    "        X = self.emb_drop(X)\n",
    "        if self.n_cont != 0:\n",
    "            X2 = self.bn(conts)\n",
    "            X = torch.cat([X, X2], dim=1) if self.n_cont != 0 else X2\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = self.bn_lin(X)\n",
    "        X = self.drop1(X)\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.bn_lin2(X)\n",
    "        X = self.drop2(X)\n",
    "        X = self.linear3(X)\n",
    "        X = F.sigmoid(X)\n",
    "        X = X * (self.y_range[1] - self.y_range[0])\n",
    "        X = X + self.y_range[0] \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n",
    "def init_embeddings(x):\n",
    "    x = x.weight.data\n",
    "    value = 2 / (x.size(1) + 1)\n",
    "    x.uniform_(-value, value)\n",
    "    \n",
    "    \n",
    "# class BnLayer(nn.Module):\n",
    "#     def __init__(self, ni, nf, stride=2, kernel_size=3):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Conv2d(ni, nf, kernel_size=kernel_size, stride=stride,\n",
    "#                               bias=False, padding=1)\n",
    "#         self.a = nn.Parameter(torch.zeros(nf,1,1))\n",
    "#         self.m = nn.Parameter(torch.ones(nf,1,1))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv(x))\n",
    "#         x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)\n",
    "#         if self.training:\n",
    "#             self.means = x_chan.mean(1)[:,None,None]\n",
    "#             self.stds  = x_chan.std (1)[:,None,None]\n",
    "#         return (x-self.means) / self.stds *self.m + self.a\n",
    "\n",
    "# class ResnetLayer(nn.Module):\n",
    "#     def forward(self, x): \n",
    "#         return x + super().forward(x)\n",
    "    \n",
    "class MiniNetPlusMetaResid(nn.Module):\n",
    "    def __init__(self, emb_szs, layer_sizes=[256, 100], n_factors=100, user_book_n_factors=100, \n",
    "                       drops=[0.5, 0.5], emb_drop=0.1, y_range=(1, 10), n_cont=0, n_cat=2):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(emb_szs['n_users'], n_factors)\n",
    "        self.book_embedding = nn.Embedding(emb_szs['n_books'], n_factors)\n",
    "        self.author_embedding = nn.Embedding(emb_szs['Book_Author'], n_factors)\n",
    "        self.publisher_embedding = nn.Embedding(emb_szs['Publisher'], n_factors)\n",
    "        self.city_embedding = nn.Embedding(emb_szs['city'], n_factors)\n",
    "        self.state_embedding = nn.Embedding(emb_szs['state'], n_factors)\n",
    "        self.country_embedding = nn.Embedding(emb_szs['country'], n_factors)\n",
    "        \n",
    "        self.user_embedding_side = nn.Embedding(emb_szs['n_users'], user_book_n_factors)\n",
    "        self.book_embedding_side = nn.Embedding(emb_szs['n_books'], user_book_n_factors)\n",
    "        \n",
    "        self.user_book_embeddings = [self.user_embedding_side, self.book_embedding_side]\n",
    "        self.embeddings = [self.user_embedding, self.book_embedding, self.author_embedding, self.publisher_embedding, \n",
    "                           self.city_embedding, self.state_embedding, self.country_embedding]\n",
    "        \n",
    "        self.lin_size = (n_factors * n_cat) + n_cont\n",
    "        self.linear1 = nn.Linear(self.lin_size, layer_sizes[0])\n",
    "        self.linear2 = nn.Linear(layer_sizes[0], layer_sizes[1])\n",
    "        self.linear3 = nn.Linear(layer_sizes[1] + user_book_n_factors*2, 1) \n",
    "        self.linears = [self.linear1, self.linear2, self.linear3]\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.bn_lin = nn.BatchNorm1d(layer_sizes[0])\n",
    "        self.bn_lin2 = nn.BatchNorm1d(layer_sizes[1])\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drop1 = nn.Dropout(drops[0])\n",
    "        self.drop2 = nn.Dropout(drops[1])\n",
    "        \n",
    "        for layer in self.embeddings:\n",
    "            init_embeddings(layer)\n",
    "            \n",
    "        for layer in self.user_book_embeddings:\n",
    "            init_embeddings(layer)\n",
    "            \n",
    "        for layer in self.linears:\n",
    "            nn.init.kaiming_normal(layer.weight.data)\n",
    "        \n",
    "        self.n_cont = n_cont\n",
    "        self.n_cat = n_cat\n",
    "        self.y_range = y_range\n",
    "        \n",
    "   \n",
    "    def forward(self, X):\n",
    "        \n",
    "        \n",
    "        cats, conts = X[:, :self.n_cat], X[:, self.n_cat:]\n",
    "        cats, conts = cats.long(), conts.float()\n",
    "        # ------\n",
    "        user_book_cat_list = [self.user_book_embeddings[x](cats[:,x]) for x in range(2)]\n",
    "        # ------\n",
    "        cat_list = [self.embeddings[x](cats[:,x]) for x in range(self.n_cat)]\n",
    "        X = torch.cat(cat_list, dim=1)\n",
    "        X = self.emb_drop(X)\n",
    "        if self.n_cont != 0:\n",
    "            X2 = self.bn(conts)\n",
    "            X = torch.cat([X, X2], dim=1) if self.n_cont != 0 else X2\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = self.bn_lin(X)\n",
    "        X = self.drop1(X)\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.bn_lin2(X)\n",
    "        X = self.drop2(X)\n",
    "        # ------\n",
    "        ub_X = torch.cat(user_book_cat_list, dim=1)\n",
    "        ub_X = self.emb_drop(ub_X)\n",
    "        X = torch.cat([X, ub_X], dim=1)\n",
    "        # ------\n",
    "        X = self.linear3(X)\n",
    "        X = F.sigmoid(X)\n",
    "        X = X * (self.y_range[1] - self.y_range[0])\n",
    "        X = X + self.y_range[0] \n",
    "        return X\n",
    "\n",
    "# --------------------\n",
    "\n",
    "import ignite\n",
    "from ignite.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "import torch\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "\n",
    "def apply_to_tensor(input_, func):\n",
    "    \"\"\"\n",
    "    Apply a function on a tensor or mapping, or sequence of tensors.\n",
    "    \"\"\"\n",
    "    return apply_to_type(input_, torch.Tensor, func)\n",
    "\n",
    "\n",
    "def apply_to_type(input_, input_type, func):\n",
    "    \"\"\"\n",
    "    Apply a function on a object of `input_type` or mapping, or sequence of objects of `input_type`.\n",
    "    \"\"\"\n",
    "    if isinstance(input_, input_type):\n",
    "        return func(input_)\n",
    "    elif isinstance(input_, string_classes):\n",
    "        return input_\n",
    "    elif isinstance(input_, collections.Mapping):\n",
    "        return {k: apply_to_type(sample, input_type, func) for k, sample in input_.items()}\n",
    "    elif isinstance(input_, collections.Sequence):\n",
    "        return [apply_to_type(sample, input_type, func) for sample in input_]\n",
    "    else:\n",
    "        raise TypeError((\"input must contain {}, dicts or lists; found {}\"\n",
    "                         .format(input_type, type(input_))))\n",
    "\n",
    "\n",
    "def convert_tensor(input_, device=None, non_blocking=False):\n",
    "    \"\"\"\n",
    "    Move tensors to relevant device.\n",
    "    \"\"\"\n",
    "    def _func(tensor):\n",
    "        return tensor.to(device=device, non_blocking=non_blocking) if device else tensor\n",
    "\n",
    "    return apply_to_tensor(input_, _func)\n",
    "\n",
    "def _prepare_batch(batch, device=None, non_blocking=False):\n",
    "    \"\"\"\n",
    "    Prepare batch for training: pass to a device with options\n",
    "    \"\"\"\n",
    "    x, y = batch\n",
    "    return (convert_tensor(x, device=device, non_blocking=non_blocking),\n",
    "            convert_tensor(y, device=device, non_blocking=non_blocking))\n",
    "\n",
    "\n",
    "all_lr = []\n",
    "\n",
    "def create_supervised_trainer(model, optimizer, loss_fn, scheduler=None,\n",
    "                              device=None, non_blocking=False,\n",
    "                              prepare_batch=_prepare_batch):\n",
    "    if device:\n",
    "        model.to(device)\n",
    "\n",
    "    def _update(engine, batch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            all_lr.append(param_group['lr'])\n",
    "        # scheduler\n",
    "        if scheduler: scheduler.step()\n",
    "        return loss.item()\n",
    "\n",
    "    return Engine(_update)\n",
    "\n",
    "# --------------------\n",
    "\n",
    "def run_model(emb_szs, params, train_loader, val_loader=None, history={}, iteration=1):\n",
    "#     model = MiniNetPlusMeta(emb_szs=emb_szs, layer_sizes=params['layer_sizes'], n_factors=params['n_factors'], \n",
    "#                             drops=params['drops'], emb_drop=params['emb_drop'], \n",
    "#                             y_range=(min_rating, max_rating), n_cont=n_cont, n_cat=n_cat).to(DEVICE)\n",
    "    \n",
    "    model = MiniNetPlusMetaResid(emb_szs=emb_szs, layer_sizes=params['layer_sizes'], n_factors=params['n_factors'], \n",
    "                            user_book_n_factors=params['user_book_n_factors'], drops=params['drops'], \n",
    "                            emb_drop=params['emb_drop'], y_range=(min_rating, max_rating), \n",
    "                            n_cont=n_cont, n_cat=n_cat).to(DEVICE)\n",
    "    \n",
    "    \n",
    "    n_batches = train_loader.dataset.N // params['batch_size']\n",
    "    cycle_len = 1\n",
    "    \n",
    "    # loss \n",
    "    crit = nn.MSELoss()\n",
    "    # optimzer \n",
    "    if params['optimizer']:\n",
    "        optimizer = params['optimizer'](get_trainable(model.parameters()), \n",
    "                                 lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(get_trainable(model.parameters()), \n",
    "                                 lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    # scheduler \n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "    # create trainer \n",
    "    trainer = create_supervised_trainer(model, optimizer, crit, scheduler=scheduler, device=params['DEVICE'])\n",
    "    # create evaluator \n",
    "    evaluator = create_supervised_evaluator(model, metrics={'RMSE': RootMeanSquaredError(),\n",
    "                                                            'MSE': MeanSquaredError(),\n",
    "                                                            'MAE': MeanAbsoluteError()}, device=params['DEVICE'])\n",
    "\n",
    "    history[iteration] = create_metrics()\n",
    "    history['params'] = params\n",
    "    history['params']['emb_szs'] = emb_c\n",
    "    history['params']['features'] = cat_features + cont_features\n",
    "\n",
    "    desc = \"ITERATION - loss: {:.2f}\"\n",
    "    pbar = tqdm_notebook(initial=0, leave=False, total=len(train_loader), desc=desc.format(0))\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED)\n",
    "    def log_training_loss(engine):\n",
    "        iter = (engine.state.iteration - 1) % len(train_loader) + 1\n",
    "        if iter % PRINT_PERIOD == 0:\n",
    "            pbar.desc = desc.format(engine.state.output)\n",
    "            pbar.update(PRINT_PERIOD)\n",
    "            history[iteration]['train']['loss'].append(engine.state.output)\n",
    "\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        pbar.update(len(train_loader))\n",
    "        pbar.refresh()\n",
    "        evaluator.run(train_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_rmse = metrics['RMSE']\n",
    "        avg_mae = metrics['MAE']\n",
    "        history[iteration]['train']['rmse'].append(avg_rmse)\n",
    "        history[iteration]['train']['mae'].append(avg_mae)\n",
    "        tqdm.write(\n",
    "            \"Training Results - Epoch: {}  Avg RMSE: {:.2f} Avg MAE: {:.2f}\"\n",
    "            .format(engine.state.epoch, avg_rmse, avg_mae)\n",
    "        )\n",
    "    if val_loader:\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def log_validation_results(engine):\n",
    "            evaluator.run(val_loader)\n",
    "            metrics = evaluator.state.metrics\n",
    "            avg_rmse = metrics['RMSE']\n",
    "            avg_mae = metrics['MAE']\n",
    "            history[iteration]['val']['rmse'].append(avg_rmse)\n",
    "            history[iteration]['val']['mae'].append(avg_mae)\n",
    "            tqdm.write(\n",
    "                \"Validation Results - Epoch: {}  Avg RMSE: {:.2f} Avg MAE: {:.2f}\"\n",
    "                .format(engine.state.epoch, avg_rmse, avg_mae))\n",
    "            pbar.n = pbar.last_print_n = 0\n",
    "            return avg_rmse\n",
    "\n",
    "    # train model \n",
    "    print('Training Fold Number: {}'.format(iteration))\n",
    "    trainer.run(train_loader, max_epochs=params['epochs'])\n",
    "    pbar.close()\n",
    "    \n",
    "    return model, history\n",
    "        \n",
    "        \n",
    "def create_metrics():\n",
    "    return {'train': {'loss': [], 'rmse': [], 'mae': []}, \n",
    "            'val': {'loss': [], 'rmse': [], 'mae': []}}\n",
    "\n",
    "\n",
    "def run_kfold(X, y, params, emb_szs, y_of_pub_features, age_features, use_early_stopping=False):\n",
    "    history = {}\n",
    "    kfold = StratifiedKFold(FOLDS)\n",
    "    for iteration, (train_index, val_index) in enumerate(kfold.split(X, y)):\n",
    "        iteration += 1\n",
    "        # split data\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        # min_max scale publisher features \n",
    "        X_train[y_of_pub_features] = min_max_scaler_yop.fit_transform(X_train[y_of_pub_features])\n",
    "        X_val[y_of_pub_features] = min_max_scaler_yop.transform(X_val[y_of_pub_features])\n",
    "        # min_max scale age feature\n",
    "        X_train[age_features] = min_max_scaler_age.fit_transform(X_train[age_features].values.reshape(-1, 1))\n",
    "        X_val[age_features] = min_max_scaler_age.transform(X_val[age_features].values.reshape(-1, 1))\n",
    "        # create Torch DataSet\n",
    "        train_torch_data_set_meta = MixedInputDataSet(X_train[cat_features], X_train[cont_features], y_train)\n",
    "        val_torch_data_set_meta = MixedInputDataSet(X_val[cat_features], X_val[cont_features], y_val)\n",
    "        # create Torch DataLoader \n",
    "        train_data_loader_meta = DataLoader(train_torch_data_set_meta, batch_size=params['batch_size'], shuffle=True)\n",
    "        val_data_loader_meta = DataLoader(val_torch_data_set_meta, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "        model, history = run_model(emb_szs=emb_szs, params=params, history=history, iteration=iteration,\n",
    "                                   train_loader=train_data_loader_meta, val_loader=val_data_loader_meta)\n",
    "\n",
    "    return history\n",
    "\n",
    "def print_mean_std(arr, metric=None):\n",
    "    mu, std = np.mean(arr), np.std(arr)\n",
    "    print('{}: {:.2f} (+/- {:.2f})'.format(metric, mu, std))\n",
    "    \n",
    "def compute_stats(history, optimized_metric='rmse'):\n",
    "    train_rmse, val_rmse, train_mae, val_mae = [], [], [], []\n",
    "    for fold in history.items():\n",
    "        if fold[0] in list(range(1, FOLDS+1)):\n",
    "            results = fold[1]\n",
    "            train_rmse.append(results['train']['rmse'][-1])\n",
    "            train_mae.append(results['train']['mae'][-1])\n",
    "            val_rmse.append(results['val']['rmse'][-1])\n",
    "            val_mae.append(results['val']['mae'][-1])\n",
    "            \n",
    "    for i, x in enumerate([train_rmse, val_rmse, train_mae, val_mae]):\n",
    "        if i <= 1: metric = 'rmse'\n",
    "        else: metric = 'mae'\n",
    "        print_mean_std(x, metric)\n",
    "    if optimized_metric == 'rmse':\n",
    "        return np.mean(val_rmse)\n",
    "    else: return np.mean(val_mae)\n",
    "        \n",
    "def run_one(X, y, params, emb_szs, y_of_pub_features, age_features, use_early_stopping):\n",
    "    print('\\n')\n",
    "    print('....Training Model with parameters....')\n",
    "    print('\\n')\n",
    "    pprint.pprint(params)\n",
    "    history = run_kfold(X, y, params, emb_szs=emb_c, \n",
    "                        y_of_pub_features=y_of_pub_features, age_features=age_features,\n",
    "                        use_early_stopping=use_early_stopping)\n",
    "    val_score = compute_stats(history)\n",
    "    return history, val_score\n",
    "\n",
    "def gridSearch():  \n",
    "    all_models = []\n",
    "    best_val_score = float('inf')\n",
    "    best_params = None\n",
    "    for n_factors in n_factor_list:\n",
    "        for layer_sizes in layer_sizes_list:\n",
    "            for drops in drops_list:\n",
    "                for emb_drop in emb_drop_list:\n",
    "                    for batch_size in batch_size_list:\n",
    "                        for epochs in epochs_list:\n",
    "                            for learning_rate in learning_rate_list:\n",
    "                                for weight_decay in weight_decay_list:\n",
    "                                    for optimizer in optimizer_list:\n",
    "                                        params = {}\n",
    "                                        params['n_factors'] = n_factors\n",
    "                                        params['batch_size'] = batch_size\n",
    "                                        params['epochs'] = epochs\n",
    "                                        params['learning_rate'] = learning_rate\n",
    "                                        params['layer_sizes'] = layer_sizes\n",
    "                                        params['drops'] = drops\n",
    "                                        params['emb_drop'] = emb_drop\n",
    "                                        params['weight_decay'] = weight_decay\n",
    "                                        params['optimizer'] = optimizer\n",
    "\n",
    "                                        history, val_score = run_one(X, y, params, emb_szs=emb_c, \n",
    "                                                            y_of_pub_features=y_of_pub_features, age_features=age_features,\n",
    "                                                            use_early_stopping=True)\n",
    "                                        all_models.append(history)\n",
    "                                        if val_score <= best_val_score:\n",
    "                                            best_val_score = val_score\n",
    "                                            best_params = params\n",
    "    return all_models, best_params, best_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['New_User_ID', 'New_Book_ID', 'Book_Author', 'Publisher', 'city', 'state', 'country']\n",
    "cont_features = ['Year_Of_Publication', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication', 'Age']\n",
    "target = 'Book_Rating'\n",
    "\n",
    "VALUE_THRESHOLD = 3\n",
    "\n",
    "ratings_df = ratings.copy()\n",
    "\n",
    "# books \n",
    "len_ratings = len(ratings_df)\n",
    "ratings_df = ratings_df.merge(books, on='ISBN', how='left')\n",
    "assert(len_ratings == len(ratings_df))\n",
    "\n",
    "# users \n",
    "len_users = len(users)\n",
    "locations = users.Location.str.split(',', expand=True)\n",
    "locations.columns = ['city', 'state', 'country', 'extra1', 'extra2', 'extra3', 'extra4', 'extra5', 'extra6']\n",
    "\n",
    "city_mappings = {'ny': 'newyork',\n",
    "            'newyorkcity': 'newyork',\n",
    "            'nyc': 'newyork',\n",
    "            'la': 'losangeles',\n",
    "            'sf': 'sanfrancisco'}\n",
    "state_mappings = {'northwestterritories': 'northernterritory'}\n",
    "country_mappings = {'uk': 'unitedkingdom'}\n",
    "\n",
    "locations, city_features = clean_text(locations, 'city', threshold=VALUE_THRESHOLD, missing_value='na', mappings=city_mappings)\n",
    "locations, state_features = clean_text(locations, 'state', threshold=VALUE_THRESHOLD, missing_value='na', mappings=state_mappings)\n",
    "locations, country_features = clean_text(locations, 'country', threshold=VALUE_THRESHOLD, missing_value='na', mappings=country_mappings)\n",
    "\n",
    "users_df = users.join(locations[['city', 'state', 'country']])\n",
    "assert(len_users == len(users_df))\n",
    "\n",
    "\n",
    "ratings_df = ratings_df.merge(users_df[['Age', 'User_ID'] + [city_features, state_features, country_features]], on='User_ID', how='left')\n",
    "ratings_df, y_of_pub_features = get_year_of_pub(ratings_df)\n",
    "ratings_df, age_features = get_age(ratings_df)\n",
    "ratings_df, author_feature = clean_text(ratings_df, 'Book_Author', threshold=VALUE_THRESHOLD, missing_value='na')\n",
    "play = ratings_df.copy()\n",
    "ratings_df, publisher_feature = clean_text(ratings_df, 'Publisher', threshold=VALUE_THRESHOLD, missing_value='na')\n",
    "\n",
    "emb_c = {n: len(c.astype('category').cat.categories)+1 for n,c in ratings_df[cat_features[2:]].items()}\n",
    "emb_c['n_users'] = n_users\n",
    "emb_c['n_books'] = n_books\n",
    "\n",
    "ratings_df = numericalize_col(ratings_df, 'Book_Author')\n",
    "ratings_df = numericalize_col(ratings_df, 'Publisher')\n",
    "ratings_df = numericalize_col(ratings_df, 'city')\n",
    "ratings_df = numericalize_col(ratings_df, 'state')\n",
    "ratings_df = numericalize_col(ratings_df, 'country')\n",
    "\n",
    "SAMPLE = False\n",
    "if SAMPLE: ratings_df = ratings_df.sample(10000)\n",
    "    \n",
    "# trainset, testset = train_test_split(ratings_df, test_size=0.2, random_state=100)\n",
    "trainset, testset = train_test_split(ratings_df, test_size=0.2)\n",
    "trainset.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "max_rating = float(trainset.Book_Rating.max())\n",
    "min_rating = float(trainset.Book_Rating.min())\n",
    "\n",
    "X = trainset[cat_features + cont_features]\n",
    "y = trainset[target]\n",
    "n_cont = len(cont_features)\n",
    "n_cat = len(cat_features)\n",
    "\n",
    "min_max_scaler_yop = MinMaxScaler()\n",
    "min_max_scaler_age = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "....Training Model with parameters....\n",
      "\n",
      "\n",
      "{'DEVICE': 'cuda',\n",
      " 'batch_size': 128,\n",
      " 'drops': [0.9, 0.9],\n",
      " 'emb_drop': 0.2,\n",
      " 'epochs': 10,\n",
      " 'layer_sizes': [256, 512],\n",
      " 'learning_rate': 0.001,\n",
      " 'n_factors': 50,\n",
      " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
      " 'user_book_n_factors': 100,\n",
      " 'weight_decay': 0.002}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d8277570a947bc8984f552579c5eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ITERATION - loss: 0.00', max=1807), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold Number: 1\n",
      "Training Results - Epoch: 1  Avg RMSE: 1.55 Avg MAE: 1.21\n",
      "Validation Results - Epoch: 1  Avg RMSE: 1.66 Avg MAE: 1.30\n",
      "Training Results - Epoch: 2  Avg RMSE: 1.56 Avg MAE: 1.21\n",
      "Validation Results - Epoch: 2  Avg RMSE: 1.68 Avg MAE: 1.32\n",
      "Training Results - Epoch: 3  Avg RMSE: 1.47 Avg MAE: 1.14\n",
      "Validation Results - Epoch: 3  Avg RMSE: 1.65 Avg MAE: 1.29\n",
      "Training Results - Epoch: 4  Avg RMSE: 1.50 Avg MAE: 1.16\n",
      "Validation Results - Epoch: 4  Avg RMSE: 1.67 Avg MAE: 1.30\n",
      "Training Results - Epoch: 5  Avg RMSE: 1.40 Avg MAE: 1.08\n",
      "Validation Results - Epoch: 5  Avg RMSE: 1.64 Avg MAE: 1.28\n",
      "Training Results - Epoch: 6  Avg RMSE: 1.43 Avg MAE: 1.10\n",
      "Validation Results - Epoch: 6  Avg RMSE: 1.67 Avg MAE: 1.30\n",
      "Training Results - Epoch: 7  Avg RMSE: 1.33 Avg MAE: 1.01\n",
      "Validation Results - Epoch: 7  Avg RMSE: 1.65 Avg MAE: 1.28\n",
      "Training Results - Epoch: 8  Avg RMSE: 1.37 Avg MAE: 1.05\n",
      "Validation Results - Epoch: 8  Avg RMSE: 1.68 Avg MAE: 1.31\n",
      "Training Results - Epoch: 9  Avg RMSE: 1.28 Avg MAE: 0.97\n",
      "Validation Results - Epoch: 9  Avg RMSE: 1.65 Avg MAE: 1.28\n",
      "Training Results - Epoch: 10  Avg RMSE: 1.34 Avg MAE: 1.02\n",
      "Validation Results - Epoch: 10  Avg RMSE: 1.68 Avg MAE: 1.30\n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59376a552c794c1c87a93d89cd338da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ITERATION - loss: 0.00', max=1807), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold Number: 2\n",
      "Training Results - Epoch: 1  Avg RMSE: 1.55 Avg MAE: 1.21\n",
      "Validation Results - Epoch: 1  Avg RMSE: 1.66 Avg MAE: 1.30\n",
      "Training Results - Epoch: 2  Avg RMSE: 1.56 Avg MAE: 1.22\n",
      "Validation Results - Epoch: 2  Avg RMSE: 1.69 Avg MAE: 1.33\n",
      "Training Results - Epoch: 3  Avg RMSE: 1.47 Avg MAE: 1.14\n",
      "Validation Results - Epoch: 3  Avg RMSE: 1.65 Avg MAE: 1.29\n",
      "Training Results - Epoch: 4  Avg RMSE: 1.50 Avg MAE: 1.16\n",
      "Validation Results - Epoch: 4  Avg RMSE: 1.68 Avg MAE: 1.31\n",
      "Training Results - Epoch: 5  Avg RMSE: 1.40 Avg MAE: 1.08\n",
      "Validation Results - Epoch: 5  Avg RMSE: 1.64 Avg MAE: 1.28\n",
      "Training Results - Epoch: 6  Avg RMSE: 1.44 Avg MAE: 1.12\n",
      "Validation Results - Epoch: 6  Avg RMSE: 1.67 Avg MAE: 1.31\n",
      "Training Results - Epoch: 7  Avg RMSE: 1.34 Avg MAE: 1.02\n",
      "Validation Results - Epoch: 7  Avg RMSE: 1.64 Avg MAE: 1.28\n",
      "Training Results - Epoch: 8  Avg RMSE: 1.38 Avg MAE: 1.06\n",
      "Validation Results - Epoch: 8  Avg RMSE: 1.67 Avg MAE: 1.30\n",
      "Training Results - Epoch: 9  Avg RMSE: 1.29 Avg MAE: 0.98\n",
      "Validation Results - Epoch: 9  Avg RMSE: 1.65 Avg MAE: 1.28\n",
      "Training Results - Epoch: 10  Avg RMSE: 1.34 Avg MAE: 1.03\n",
      "Validation Results - Epoch: 10  Avg RMSE: 1.68 Avg MAE: 1.30\n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7309926f7e3e474b8a54abfca505449a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ITERATION - loss: 0.00', max=1807), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold Number: 3\n",
      "Training Results - Epoch: 1  Avg RMSE: 1.55 Avg MAE: 1.21\n",
      "Validation Results - Epoch: 1  Avg RMSE: 1.66 Avg MAE: 1.30\n",
      "Training Results - Epoch: 2  Avg RMSE: 1.56 Avg MAE: 1.22\n",
      "Validation Results - Epoch: 2  Avg RMSE: 1.68 Avg MAE: 1.32\n",
      "Training Results - Epoch: 3  Avg RMSE: 1.47 Avg MAE: 1.14\n",
      "Validation Results - Epoch: 3  Avg RMSE: 1.65 Avg MAE: 1.29\n",
      "Training Results - Epoch: 4  Avg RMSE: 1.51 Avg MAE: 1.17\n",
      "Validation Results - Epoch: 4  Avg RMSE: 1.68 Avg MAE: 1.31\n",
      "Training Results - Epoch: 5  Avg RMSE: 1.41 Avg MAE: 1.08\n",
      "Validation Results - Epoch: 5  Avg RMSE: 1.64 Avg MAE: 1.28\n",
      "Training Results - Epoch: 6  Avg RMSE: 1.44 Avg MAE: 1.11\n",
      "Validation Results - Epoch: 6  Avg RMSE: 1.68 Avg MAE: 1.31\n",
      "Training Results - Epoch: 7  Avg RMSE: 1.34 Avg MAE: 1.02\n",
      "Validation Results - Epoch: 7  Avg RMSE: 1.65 Avg MAE: 1.28\n",
      "Training Results - Epoch: 8  Avg RMSE: 1.38 Avg MAE: 1.06\n",
      "Validation Results - Epoch: 8  Avg RMSE: 1.68 Avg MAE: 1.30\n",
      "Training Results - Epoch: 9  Avg RMSE: 1.29 Avg MAE: 0.97\n",
      "Validation Results - Epoch: 9  Avg RMSE: 1.65 Avg MAE: 1.28\n",
      "Training Results - Epoch: 10  Avg RMSE: 1.34 Avg MAE: 1.02\n",
      "Validation Results - Epoch: 10  Avg RMSE: 1.68 Avg MAE: 1.30\n",
      "rmse: 1.34 (+/- 0.00)\n",
      "rmse: 1.68 (+/- 0.00)\n",
      "mae: 1.02 (+/- 0.00)\n",
      "mae: 1.30 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 3\n",
    "PRINT_PERIOD = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "all_models = []\n",
    "\n",
    "# 1.63\n",
    "# n_factors = 50\n",
    "# layer_sizes = [256, 512]\n",
    "# drops = [0.9, 0.9]\n",
    "# emb_drop = 0.2\n",
    "# batch_size = 128\n",
    "# epochs = 5\n",
    "# learning_rate = 0.001\n",
    "# weight_decay = 2e-3\n",
    "# optimizer = torch.optim.Adam\n",
    "\n",
    "\n",
    "n_factors = 50\n",
    "user_book_n_factors = 100\n",
    "layer_sizes = [256, 512]\n",
    "drops = [0.9, 0.9]\n",
    "emb_drop = 0.2\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "weight_decay = 2e-3\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "params = {}\n",
    "params['n_factors'] = n_factors\n",
    "params['user_book_n_factors'] = user_book_n_factors\n",
    "params['batch_size'] = batch_size\n",
    "params['epochs'] = epochs\n",
    "params['learning_rate'] = learning_rate\n",
    "params['layer_sizes'] = layer_sizes\n",
    "params['drops'] = drops\n",
    "params['emb_drop'] = emb_drop\n",
    "params['weight_decay'] = weight_decay\n",
    "params['DEVICE'] = DEVICE\n",
    "params['optimizer'] = optimizer\n",
    "\n",
    "history, val_score = run_one(X, y, params, emb_szs=emb_c, \n",
    "                             y_of_pub_features=y_of_pub_features, age_features=age_features,\n",
    "                             use_early_stopping=False)\n",
    "# assign to best params \n",
    "best_params = history['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(ratings_df):\n",
    "    # books \n",
    "    len_ratings = len(ratings_df)\n",
    "    ratings_df = ratings_df.merge(books, on='ISBN', how='left')\n",
    "    assert(len_ratings == len(ratings_df))\n",
    "\n",
    "    # users \n",
    "    len_users = len(users)\n",
    "    locations = users.Location.str.split(',', expand=True)\n",
    "    locations.columns = ['city', 'state', 'country', 'extra1', 'extra2', 'extra3', 'extra4', 'extra5', 'extra6']\n",
    "\n",
    "    city_mappings = {'ny': 'newyork',\n",
    "                'newyorkcity': 'newyork',\n",
    "                'nyc': 'newyork',\n",
    "                'la': 'losangeles',\n",
    "                'sf': 'sanfrancisco'}\n",
    "    state_mappings = {'northwestterritories': 'northernterritory'}\n",
    "    country_mappings = {'uk': 'unitedkingdom'}\n",
    "\n",
    "    locations, city_features = clean_text(locations, 'city', threshold=VALUE_THRESHOLD, missing_value='na', mappings=city_mappings)\n",
    "    locations, state_features = clean_text(locations, 'state', threshold=VALUE_THRESHOLD, missing_value='na', mappings=state_mappings)\n",
    "    locations, country_features = clean_text(locations, 'country', threshold=VALUE_THRESHOLD, missing_value='na', mappings=country_mappings)\n",
    "\n",
    "    users_df = users.join(locations[['city', 'state', 'country']])\n",
    "    assert(len_users == len(users_df))\n",
    "\n",
    "\n",
    "    ratings_df = ratings_df.merge(users_df[['Age', 'User_ID'] + [city_features, state_features, country_features]], on='User_ID', how='left')\n",
    "    ratings_df, y_of_pub_features = get_year_of_pub(ratings_df)\n",
    "    ratings_df, age_features = get_age(ratings_df)\n",
    "    ratings_df, author_feature = clean_text(ratings_df, 'Book_Author', threshold=VALUE_THRESHOLD, missing_value='na')\n",
    "    play = ratings_df.copy()\n",
    "    ratings_df, publisher_feature = clean_text(ratings_df, 'Publisher', threshold=VALUE_THRESHOLD, missing_value='na')\n",
    "\n",
    "    emb_c = {n: len(c.astype('category').cat.categories)+1 for n,c in ratings_df[cat_features[2:]].items()}\n",
    "    emb_c['n_users'] = n_users\n",
    "    emb_c['n_books'] = n_books\n",
    "\n",
    "    ratings_df = numericalize_col(ratings_df, 'Book_Author')\n",
    "    ratings_df = numericalize_col(ratings_df, 'Publisher')\n",
    "    ratings_df = numericalize_col(ratings_df, 'city')\n",
    "    ratings_df = numericalize_col(ratings_df, 'state')\n",
    "    ratings_df = numericalize_col(ratings_df, 'country')\n",
    "\n",
    "    return ratings_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "from surprise import PredictionImpossible\n",
    "\n",
    "class NNModel(AlgoBase):\n",
    "    def __init__(self, params):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.params = params\n",
    "        self.book_data = BookDataSet(ratings, books, users)\n",
    "        \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        \n",
    "        self.N = trainset.n_users\n",
    "        self.D = trainset.n_items\n",
    "        self.trainset = trainset\n",
    "\n",
    "        training_df = pd.DataFrame(columns=['New_User_ID', 'New_Book_ID', 'Book_Rating', 'User_ID', 'ISBN'])\n",
    "        n_rat = trainset.n_ratings\n",
    "\n",
    "        load = True\n",
    "        if load:\n",
    "            training_df = pd.read_feather('data/NN_training_df') \n",
    "            print('training_df loaded')\n",
    "        else:\n",
    "            for i, (uid, iid, rating) in enumerate(trainset.all_ratings()):\n",
    "                raw_uid = trainset.to_raw_uid(uid)\n",
    "                raw_iid = trainset.to_raw_iid(iid)\n",
    "                training_df = training_df.append({'New_User_ID': int(uid), 'New_Book_ID': int(iid), \n",
    "                                                  'Book_Rating': float(rating), \n",
    "                                                  'User_ID': int(raw_uid), 'ISBN': raw_iid}, \n",
    "                                                  ignore_index=True)\n",
    "                if i % 25000 == 0:\n",
    "                    print('Percent processed: {:.2f}'.format((i / n_rat)*100.0))\n",
    "            save = True\n",
    "            if save:\n",
    "                training_df.to_feather('data/NN_training_df')\n",
    "                print('training_df saved to data/NN_training_df')\n",
    "                \n",
    "            # convert to ints \n",
    "        training_df['New_User_ID'] = training_df['New_User_ID'].astype(int)\n",
    "        training_df['New_Book_ID'] = training_df['New_Book_ID'].astype(int)\n",
    "        training_df['User_ID'] = training_df['User_ID'].astype(int)\n",
    "\n",
    "        print('Dataframe loaded')\n",
    "\n",
    "        self.training_df = generate_features(training_df)\n",
    "\n",
    "        self.min_max_scaler_yop = MinMaxScaler()\n",
    "        self.min_max_scaler_age = MinMaxScaler()\n",
    "        \n",
    "        self.training_df[y_of_pub_features] = self.min_max_scaler_yop.fit_transform(self.training_df[y_of_pub_features])\n",
    "        # min_max scale age feature\n",
    "        self.training_df[age_features] = self.min_max_scaler_age.fit_transform(self.training_df[age_features].values.reshape(-1, 1))\n",
    "        # create Torch DataSet\n",
    "        train_torch_data_set_meta = MixedInputDataSet(self.training_df[cat_features], self.training_df[cont_features], training_df.Book_Rating)\n",
    "\n",
    "        # create Torch DataLoader \n",
    "        self.train_data_loader_meta = DataLoader(train_torch_data_set_meta, batch_size=self.params['batch_size'], shuffle=True)\n",
    "\n",
    "        self.model, self.history = run_model(self.params['emb_szs'], params=self.params, history={}, \n",
    "                                   train_loader=self.train_data_loader_meta, val_loader=None, iteration=1)\n",
    "        \n",
    "        \n",
    "#         self.final_test_evaluate(testset)\n",
    "#         self.predictions_for_all_user_book_combo()\n",
    "        \n",
    "    # FOR ANTI \n",
    "    def predict_user(self, test_user_id=37, top_N=10):\n",
    "\n",
    "        book_features = ['New_Book_ID', 'Book_Author', 'Year_Of_Publication', 'Publisher', 'Book_Title']\n",
    "        rating_feature = ['Book_Rating', 'New_Book_ID']\n",
    "        user_features = ['New_User_ID', 'city', 'state', 'country', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication', 'Age' ]\n",
    "\n",
    "        # user info\n",
    "        test_user_df = self.training_df[self.training_df['New_User_ID'] == test_user_id]\n",
    "        test_user_info = test_user_df[user_features + rating_feature].reset_index()\n",
    "\n",
    "        # get books user has not rated \n",
    "        test_user_rated_books = list(test_user_df['New_Book_ID'].values)\n",
    "        test_user_anti_books = self.training_df.loc[~self.training_df.New_Book_ID.isin(test_user_rated_books), book_features].drop_duplicates(subset='New_Book_ID')\n",
    "\n",
    "        # combine. Need better way \n",
    "        for col in user_features:\n",
    "            test_user_anti_books[col] = test_user_info[col].values[0]\n",
    "        test_user_anti_books['Book_Rating'] = 0\n",
    "\n",
    "        # scale \n",
    "        test_user_anti_books[y_of_pub_features] = self.min_max_scaler_yop.transform(test_user_anti_books[y_of_pub_features])\n",
    "        test_user_anti_books[age_features] = self.min_max_scaler_age.transform(test_user_anti_books[age_features].values.reshape(-1, 1))\n",
    "\n",
    "        # create pytorch dataloader \n",
    "        test_user_loader = DataLoader(MixedInputDataSet(test_user_anti_books[cat_features], \n",
    "                                                        test_user_anti_books[cont_features], \n",
    "                                                        test_user_anti_books['Book_Rating']), \n",
    "                                                        batch_size=self.params['batch_size'], shuffle=False)\n",
    "        # get predictions for books not seen\n",
    "        test_user_preds = []\n",
    "        for (x, y) in test_user_loader:\n",
    "            x = x.cuda()\n",
    "            preds = self.model.forward(x)\n",
    "            test_user_preds.extend(preds.data.cpu().numpy().flatten())\n",
    "        # book title and pred mapping \n",
    "        book_pred_dict = dict(zip(test_user_anti_books['Book_Title'], test_user_preds))\n",
    "        # return top N books \n",
    "        return sorted(book_pred_dict.items(), key=itemgetter(1), reverse=True)[:top_N]\n",
    "    \n",
    "    def final_test_evaluate(self, testset):\n",
    "        self.testset = testset\n",
    "        self.testset[y_of_pub_features] = self.min_max_scaler_yop.fit_transform(self.testset[y_of_pub_features])\n",
    "        # min_max scale age feature\n",
    "        self.testset[age_features] = self.min_max_scaler_age.fit_transform(self.testset[age_features].values.reshape(-1, 1))\n",
    "        # create Torch DataSet\n",
    "        test_torch_data_set_meta = MixedInputDataSet(self.testset[cat_features], self.testset[cont_features], self.testset.Book_Rating)\n",
    "\n",
    "        # create Torch DataLoader \n",
    "        self.test_data_loader_meta = DataLoader(test_torch_data_set_meta, batch_size=self.params['batch_size'], shuffle=True)\n",
    "        \n",
    "        model_evaluator = create_supervised_evaluator(self.model, metrics={'RMSE': RootMeanSquaredError(),\n",
    "                                                            'MAE': MeanAbsoluteError()}, device=self.params['DEVICE'])\n",
    "                                                \n",
    "        model_evaluator.run(self.test_data_loader_meta)\n",
    "        metrics = model_evaluator.state.metrics\n",
    "        avg_rmse = metrics['RMSE']\n",
    "        avg_mae = metrics['MAE']\n",
    "        print(\"Testing Results - Avg RMSE: {:.2f} Avg MAE: {:.2f}\" .format(avg_rmse, avg_mae))\n",
    "        return avg_rmse\n",
    "    \n",
    "    def get_user_data(self, user_id):\n",
    "        book_features = ['New_Book_ID', 'Book_Author', 'Year_Of_Publication', 'Publisher', 'Book_Title']\n",
    "        rating_feature = ['Book_Rating', 'New_Book_ID']\n",
    "        user_features = ['New_User_ID', 'city', 'state', 'country', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication', 'Age' ]\n",
    "\n",
    "        # user info\n",
    "        test_user_df = self.training_df[self.training_df['New_User_ID'] == user_id]\n",
    "        test_user_info = test_user_df[user_features + rating_feature].reset_index()\n",
    "\n",
    "        # get books user has not rated \n",
    "        test_user_rated_books = list(test_user_df['New_Book_ID'].values)\n",
    "        test_user_anti_books = self.training_df[book_features].drop_duplicates(subset='New_Book_ID')\n",
    "\n",
    "        # combine. Need better way \n",
    "        for col in user_features:\n",
    "            test_user_anti_books[col] = test_user_info[col].values[0]\n",
    "        test_user_anti_books['Book_Rating'] = 0\n",
    "\n",
    "        # scale \n",
    "        test_user_anti_books[y_of_pub_features] = self.min_max_scaler_yop.transform(test_user_anti_books[y_of_pub_features])\n",
    "        test_user_anti_books[age_features] = self.min_max_scaler_age.transform(test_user_anti_books[age_features].values.reshape(-1, 1))\n",
    "        return test_user_anti_books\n",
    "    \n",
    "    def predictions_for_all_user_book_combo(self, prediction_file='NN_model_predicts.txt'):\n",
    "\n",
    "        with open(prediction_file, 'ab') as f:\n",
    "            \n",
    "            for uiid in range(self.trainset.n_users):\n",
    "                if (uiid % 50 == 0):\n",
    "                    print('Processing user {}'.format(uiid))\n",
    "                test_user_data = self.get_user_data(uiid)\n",
    "                test_user_loader = DataLoader(MixedInputDataSet(test_user_data[cat_features], \n",
    "                                                            test_user_data[cont_features], \n",
    "                                                            test_user_data['Book_Rating']), \n",
    "                                                            batch_size=self.params['batch_size'], shuffle=False)\n",
    "                # get predictions for books\n",
    "                test_user_preds = []\n",
    "                for (x, y) in test_user_loader:\n",
    "                    x = x.to(self.params['DEVICE'])\n",
    "                    preds = self.model.forward(x)\n",
    "                    test_user_preds.extend(preds.data.cpu().numpy().flatten())\n",
    "                assert len(test_user_preds) == self.D\n",
    "                \n",
    "                test_user_preds = np.array(test_user_preds)[None, :]\n",
    "                np.savetxt(f, test_user_preds, fmt='%.6f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "from surprise import PredictionImpossible\n",
    "\n",
    "class NNModel(AlgoBase):\n",
    "    def __init__(self, params):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.params = params\n",
    "        self.book_data = BookDataSet(ratings, books, users)\n",
    "        \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        \n",
    "        self.N = trainset.n_users\n",
    "        self.D = trainset.n_items\n",
    "        self.trainset = trainset\n",
    "\n",
    "        training_df = pd.DataFrame(columns=['New_User_ID', 'New_Book_ID', 'Book_Rating', 'User_ID', 'ISBN'])\n",
    "        n_rat = trainset.n_ratings\n",
    "\n",
    "        load = True\n",
    "        if load:\n",
    "            training_df = pd.read_feather('data/NN_training_df') \n",
    "#             training_df = pd.read_feather('data/NN_training_df_sample') \n",
    "            print('training_df loaded')\n",
    "        else:\n",
    "            for i, (uid, iid, rating) in enumerate(trainset.all_ratings()):\n",
    "                raw_uid = trainset.to_raw_uid(uid)\n",
    "                raw_iid = trainset.to_raw_iid(iid)\n",
    "                training_df = training_df.append({'New_User_ID': int(uid), 'New_Book_ID': int(iid), \n",
    "                                                  'Book_Rating': float(rating), \n",
    "                                                  'User_ID': int(raw_uid), 'ISBN': raw_iid}, \n",
    "                                                  ignore_index=True)\n",
    "                if i % 25000 == 0:\n",
    "                    print('Percent processed: {:.2f}'.format((i / n_rat)*100.0))\n",
    "            save = True\n",
    "            if save:\n",
    "                training_df.to_feather('data/NN_training_df')\n",
    "#                 training_df.to_feather('data/NN_training_df_sample')\n",
    "                print('training_df saved to data/NN_training_df')\n",
    "                \n",
    "            # convert to ints \n",
    "        training_df['New_User_ID'] = training_df['New_User_ID'].astype(int)\n",
    "        training_df['New_Book_ID'] = training_df['New_Book_ID'].astype(int)\n",
    "        training_df['User_ID'] = training_df['User_ID'].astype(int)\n",
    "\n",
    "        print('Dataframe loaded')\n",
    "\n",
    "        self.training_df = generate_features(training_df)\n",
    "\n",
    "        self.min_max_scaler_yop = MinMaxScaler()\n",
    "        self.min_max_scaler_age = MinMaxScaler()\n",
    "        \n",
    "        self.training_df[y_of_pub_features] = self.min_max_scaler_yop.fit_transform(self.training_df[y_of_pub_features])\n",
    "        # min_max scale age feature\n",
    "        self.training_df[age_features] = self.min_max_scaler_age.fit_transform(self.training_df[age_features].values.reshape(-1, 1))\n",
    "        # create Torch DataSet\n",
    "        train_torch_data_set_meta = MixedInputDataSet(self.training_df[cat_features], self.training_df[cont_features], training_df.Book_Rating)\n",
    "\n",
    "        # create Torch DataLoader \n",
    "        self.train_data_loader_meta = DataLoader(train_torch_data_set_meta, batch_size=self.params['batch_size'], shuffle=True)\n",
    "\n",
    "        self.model, self.history = run_model(self.params['emb_szs'], params=self.params, history={}, \n",
    "                                   train_loader=self.train_data_loader_meta, val_loader=None, iteration=1)\n",
    "        \n",
    "        \n",
    "#         self.final_test_evaluate(testset)\n",
    "        \n",
    "#         self.make_predictions()\n",
    "        \n",
    "    # FOR ANTI \n",
    "    def predict_user(self, test_user_id=37, top_N=10):\n",
    "\n",
    "        book_features = ['New_Book_ID', 'Book_Author', 'Year_Of_Publication', 'Publisher', 'Book_Title']\n",
    "        rating_feature = ['Book_Rating', 'New_Book_ID']\n",
    "        user_features = ['New_User_ID', 'city', 'state', 'country', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication', 'Age' ]\n",
    "\n",
    "        # user info\n",
    "        test_user_df = self.training_df[self.training_df['New_User_ID'] == test_user_id]\n",
    "        test_user_info = test_user_df[user_features + rating_feature].reset_index()\n",
    "\n",
    "        # get books user has not rated \n",
    "        test_user_rated_books = list(test_user_df['New_Book_ID'].values)\n",
    "        test_user_anti_books = self.training_df.loc[~self.training_df.New_Book_ID.isin(test_user_rated_books), book_features].drop_duplicates(subset='New_Book_ID')\n",
    "\n",
    "        # combine. Need better way \n",
    "        for col in user_features:\n",
    "            test_user_anti_books[col] = test_user_info[col].values[0]\n",
    "        test_user_anti_books['Book_Rating'] = 0\n",
    "\n",
    "        # scale \n",
    "        test_user_anti_books[y_of_pub_features] = self.min_max_scaler_yop.transform(test_user_anti_books[y_of_pub_features])\n",
    "        test_user_anti_books[age_features] = self.min_max_scaler_age.transform(test_user_anti_books[age_features].values.reshape(-1, 1))\n",
    "\n",
    "        # create pytorch dataloader \n",
    "        test_user_loader = DataLoader(MixedInputDataSet(test_user_anti_books[cat_features], \n",
    "                                                        test_user_anti_books[cont_features], \n",
    "                                                        test_user_anti_books['Book_Rating']), \n",
    "                                                        batch_size=self.params['batch_size'], shuffle=False)\n",
    "        # get predictions for books not seen\n",
    "        test_user_preds = []\n",
    "        for (x, y) in test_user_loader:\n",
    "            x = x.cuda()\n",
    "            preds = self.model.forward(x)\n",
    "            test_user_preds.extend(preds.data.cpu().numpy().flatten())\n",
    "        # book title and pred mapping \n",
    "        book_pred_dict = dict(zip(test_user_anti_books['Book_Title'], test_user_preds))\n",
    "        # return top N books \n",
    "        return sorted(book_pred_dict.items(), key=itemgetter(1), reverse=True)[:top_N]\n",
    "    \n",
    "    def final_test_evaluate(self, testset):\n",
    "        self.testset = testset\n",
    "        self.testset[y_of_pub_features] = self.min_max_scaler_yop.fit_transform(self.testset[y_of_pub_features])\n",
    "        # min_max scale age feature\n",
    "        self.testset[age_features] = self.min_max_scaler_age.fit_transform(self.testset[age_features].values.reshape(-1, 1))\n",
    "        # create Torch DataSet\n",
    "        test_torch_data_set_meta = MixedInputDataSet(self.testset[cat_features], self.testset[cont_features], self.testset.Book_Rating)\n",
    "\n",
    "        # create Torch DataLoader \n",
    "        self.test_data_loader_meta = DataLoader(test_torch_data_set_meta, batch_size=self.params['batch_size'], shuffle=True)\n",
    "        \n",
    "        model_evaluator = create_supervised_evaluator(self.model, metrics={'RMSE': RootMeanSquaredError(),\n",
    "                                                            'MAE': MeanAbsoluteError()}, device=self.params['DEVICE'])\n",
    "                                                \n",
    "        model_evaluator.run(self.test_data_loader_meta)\n",
    "        metrics = model_evaluator.state.metrics\n",
    "        avg_rmse = metrics['RMSE']\n",
    "        avg_mae = metrics['MAE']\n",
    "        print(\"Testing Results - Avg RMSE: {:.2f} Avg MAE: {:.2f}\" .format(avg_rmse, avg_mae))\n",
    "        return avg_rmse\n",
    "    \n",
    "    def get_user_data(self, user_id):\n",
    "        book_features = ['New_Book_ID', 'Book_Author', 'Year_Of_Publication', 'Publisher', 'Book_Title']\n",
    "        rating_feature = ['Book_Rating', 'New_Book_ID']\n",
    "        user_features = ['New_User_ID', 'city', 'state', 'country', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication', 'Age' ]\n",
    "\n",
    "        # user info\n",
    "        test_user_df = self.training_df[self.training_df['New_User_ID'] == user_id]\n",
    "        test_user_info = test_user_df[user_features + rating_feature].reset_index()\n",
    "\n",
    "        # get books user has not rated \n",
    "        test_user_rated_books = list(test_user_df['New_Book_ID'].values)\n",
    "        test_user_anti_books = self.training_df[book_features].drop_duplicates(subset='New_Book_ID')\n",
    "\n",
    "        # combine. Need better way \n",
    "        for col in user_features:\n",
    "            test_user_anti_books[col] = test_user_info[col].values[0]\n",
    "        test_user_anti_books['Book_Rating'] = 0\n",
    "\n",
    "        # scale \n",
    "        test_user_anti_books[y_of_pub_features] = self.min_max_scaler_yop.transform(test_user_anti_books[y_of_pub_features])\n",
    "        test_user_anti_books[age_features] = self.min_max_scaler_age.transform(test_user_anti_books[age_features].values.reshape(-1, 1))\n",
    "        return test_user_anti_books\n",
    "    \n",
    "    def make_predictions(self):\n",
    "\n",
    "        prediction_file = 'NN_model_predicts.txt'\n",
    "        with open(prediction_file,'ab') as f:\n",
    "            \n",
    "            for uiid in range(self.trainset.n_users):\n",
    "                if (uiid % 50 == 0):\n",
    "                    print('Processing user {}'.format(uiid))\n",
    "                test_user_data = self.get_user_data(uiid)\n",
    "                test_user_loader = DataLoader(MixedInputDataSet(test_user_data[cat_features], \n",
    "                                                            test_user_data[cont_features], \n",
    "                                                            test_user_data['Book_Rating']), \n",
    "                                                            batch_size=self.params['batch_size'], shuffle=False)\n",
    "                # get predictions for books\n",
    "                test_user_preds = []\n",
    "                for (x, y) in test_user_loader:\n",
    "                    x = x.to(self.params['DEVICE'])\n",
    "                    preds = self.model.forward(x)\n",
    "                    test_user_preds.extend(preds.data.cpu().numpy().flatten())\n",
    "                assert len(test_user_preds) == self.D\n",
    "                \n",
    "                test_user_preds = np.array(test_user_preds)[None, :]\n",
    "                np.savetxt(f, test_user_preds, fmt='%.6f')\n",
    "\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        return self \n",
    "#         if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "#             raise PredictionImpossible('User and/or item is unkown.')     \n",
    "#         return 8    \n",
    "#         rating = self.predictedRatings[u, i]\n",
    "        \n",
    "#         if (rating < 0.001):\n",
    "#             raise PredictionImpossible('No valid prediction exists.')\n",
    "            \n",
    "#         return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import * \n",
    "\n",
    "from BookData import BookDataSet\n",
    "from Evaluator import Evaluator\n",
    "from EvaluationData import CreateDataSets\n",
    "from surprise import KNNBasic\n",
    "\n",
    "# sample_ratings = popular_ratings(ratings, user_threshold=500, rating_threshold=500, book_threshold=1)\n",
    "# print('Sample Ratings Shape', sample_ratings.shape)\n",
    "# book_ratings = sample_ratings.pivot_table(index='User_ID', columns='ISBN', values='Book_Rating').fillna(0)\n",
    "\n",
    "data = BookDataSet(ratings, books, users)\n",
    "rankings = data.get_popularity_ranks()\n",
    "evaluator = Evaluator(data, rankings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customNN = NNModel(best_params)\n",
    "# evaluator.add_model(customNN, \"customNN\")\n",
    "# print('customRBM Model Created')\n",
    "# evaluator.evaluate(topN=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ANTI \n",
    "def predict_user(test_user_id=37, top_N=10):\n",
    "\n",
    "    book_features = ['New_Book_ID', 'Book_Author', 'Year_Of_Publication', 'Publisher', 'Book_Title', 'ISBN']\n",
    "    rating_feature = ['Book_Rating', 'New_Book_ID']\n",
    "    user_features = ['New_User_ID', 'city', 'state', 'country', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication', 'Age' ]\n",
    "\n",
    "    # user \n",
    "    test_user_df = ratings_df[ratings_df['New_User_ID'] == test_user_id]\n",
    "    test_user_info = test_user_df[user_features + rating_feature].reset_index()\n",
    "\n",
    "    # books \n",
    "    test_user_rated_books = list(test_user_df['New_Book_ID'].values)\n",
    "    test_user_anti_books = ratings_df.loc[~ratings_df.New_Book_ID.isin(test_user_rated_books), book_features].drop_duplicates(subset='New_Book_ID')\n",
    "\n",
    "    # combine. Need better way \n",
    "    for col in user_features:\n",
    "        test_user_anti_books[col] = test_user_info[col].values[0]\n",
    "    test_user_anti_books['Book_Rating'] = 0\n",
    "\n",
    "\n",
    "    test_user_anti_books[y_of_pub_features] = customNN.min_max_scaler_yop.transform(test_user_anti_books[y_of_pub_features])\n",
    "    test_user_anti_books[age_features] = customNN.min_max_scaler_age.transform(test_user_anti_books[age_features].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    test_user_loader = DataLoader(MixedInputDataSet(test_user_anti_books[cat_features], \n",
    "                                                    test_user_anti_books[cont_features], \n",
    "                                                    test_user_anti_books['Book_Rating']), \n",
    "                                                    batch_size=128, shuffle=False)\n",
    "\n",
    "    test_user_preds = []\n",
    "    for (x, y) in test_user_loader:\n",
    "        x = x.cuda()\n",
    "        preds = customNN.model.forward(x)\n",
    "        test_user_preds.extend(preds.data.cpu().numpy().flatten())\n",
    "\n",
    "\n",
    "    \n",
    "    book_pred_dict = dict(zip(test_user_anti_books['Book_Title'], test_user_preds))\n",
    "    return test_user_preds, sorted(book_pred_dict.items(), key=itemgetter(1), reverse=True)[:top_N]\n",
    "    \n",
    "    \n",
    "test_user_preds, Z = predict_user(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5330f2ce29c447d89f2802fe7a254eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ITERATION - loss: 0.00', max=5421), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold Number: 1\n",
      "Training Results - Epoch: 1  Avg RMSE: 1.84 Avg MAE: 1.51\n",
      "Training Results - Epoch: 2  Avg RMSE: 1.77 Avg MAE: 1.44\n",
      "Training Results - Epoch: 3  Avg RMSE: 1.66 Avg MAE: 1.29\n",
      "\r"
     ]
    }
   ],
   "source": [
    "trainset[y_of_pub_features] = min_max_scaler_yop.fit_transform(trainset[y_of_pub_features])\n",
    "testset[y_of_pub_features] = min_max_scaler_yop.transform(testset[y_of_pub_features])\n",
    "# min_max scale age feature\n",
    "trainset[age_features] = min_max_scaler_age.fit_transform(trainset[age_features].values.reshape(-1, 1))\n",
    "testset[age_features] = min_max_scaler_age.transform(testset[age_features].values.reshape(-1, 1))\n",
    "# create Torch DataSet\n",
    "train_torch_data_set_meta = MixedInputDataSet(trainset[cat_features], trainset[cont_features], trainset.Book_Rating)\n",
    "test_torch_data_set_meta = MixedInputDataSet(testset[cat_features], testset[cont_features], testset.Book_Rating)\n",
    "# create Torch DataLoader \n",
    "train_data_loader_meta = DataLoader(train_torch_data_set_meta, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "model, history = run_model(best_params['emb_szs'], params=best_params, history={}, \n",
    "                           train_loader=train_data_loader_meta, val_loader=None, iteration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn.init import kaiming_normal\n",
    "\n",
    "def init_embeddings(x):\n",
    "    x = x.weight.data\n",
    "    value = 2 / (x.size(1) + 1)\n",
    "    x.uniform_(-value, value)\n",
    "    \n",
    "class MultiInputNN(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, n_cat, emb_drop, out_sz, sizes, drops, y_range=None, use_bn=False, f=F.relu):\n",
    "        super().__init__()\n",
    "        # embedding layers\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(insize, outsize) for insize, outsize in emb_szs])\n",
    "        print(len(self.embeddings))\n",
    "        for layer in self.embeddings:\n",
    "            init_embeddings(layer)\n",
    "        self.num_categorical = sum([layer.embedding_dim for layer in self.embeddings])\n",
    "        self.num_numerical = n_cont\n",
    "        # linear layers\n",
    "        sizes = [self.num_categorical + self.num_numerical] + sizes\n",
    "        self.linear = nn.ModuleList([nn.Linear(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(size) for size in sizes[1:]])\n",
    "        for layer in self.linear:\n",
    "            kaiming_normal(layer.weight.data)\n",
    "        # dropout layers \n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drop_out = [nn.Dropout(drop) for drop in drops]\n",
    "        # output layer\n",
    "        self.output = nn.Linear(sizes[-1], 1)\n",
    "        kaiming_normal(self.output.weight.data)\n",
    "        self.f = f\n",
    "        self.bn = nn.BatchNorm1d(self.num_numerical)\n",
    "        self.use_bn = use_bn\n",
    "        self.y_range = y_range\n",
    "        self.n_cat = n_cat\n",
    "\n",
    "#     def forward(self, x_cat, x_cont):\n",
    "    def forward(self, X):\n",
    "        x_cat, x_cont = X[:, :self.n_cat], X[:, self.n_cat:]\n",
    "        print(x_cat.size(), x_cont.size())\n",
    "        x_cat, x_cont = x_cat.long(), x_cont.float()\n",
    "        if self.num_categorical > 0:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            X = [emb_layer(x_cat[:,i]) for i, emb_layer in enumerate(self.embeddings)]\n",
    "            X = torch.cat(X, dim=1)\n",
    "            X = self.emb_drop(X)\n",
    "        if self.num_numerical > 0:\n",
    "            X2 = self.bn(x_cont)\n",
    "            X = torch.cat([X, X2], dim=1) if self.num_categorical != 0 else X2\n",
    "        for linear, drop, norm in zip(self.linear, self.drop_out, self.bns):\n",
    "            X = self.f(linear(X))\n",
    "            if self.use_bn: \n",
    "                X = norm(X)\n",
    "            X = drop(X)\n",
    "        X = self.output(X)\n",
    "        if self.y_range:\n",
    "            X = F.sigmoid(X)\n",
    "            X = X * (self.y_range[1] - self.y_range[0])\n",
    "            X = X + self.y_range[0]\n",
    "        return X\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "....Training Model with parameters....\n",
      "\n",
      "\n",
      "{'batch_size': 32,\n",
      " 'drops': [0.6, 0.75],\n",
      " 'emb_drop': 0.3,\n",
      " 'epochs': 5,\n",
      " 'layer_sizes': [256, 100],\n",
      " 'learning_rate': 0.0001,\n",
      " 'n_factors': 50}\n",
      "{(60, 50), (541, 50), (196, 50), (419, 50), (77805, 50), (404, 50), (185973, 50)}\n",
      "7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a923d0309a254cad80f5b35d088b8f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ITERATION - loss: 0.00', max=7228), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold Number: 1\n",
      "torch.Size([32, 7]) torch.Size([32, 4])\n",
      "> <ipython-input-10-cb0699d22634>(54)forward()\n",
      "-> X = [emb_layer(x_cat[:,i]) for i, emb_layer in enumerate(self.embeddings)]\n",
      "(Pdb) enumerate(self.embeddings)\n",
      "<enumerate object at 0x7fb32468c7e0>\n",
      "(Pdb) list(enumerate(self.embeddings)_\n",
      "*** Error in argument: '(enumerate(self.embeddings)_'\n",
      "(Pdb) list(enumerate(self.embeddings))\n",
      "*** Error in argument: '(enumerate(self.embeddings))'\n",
      "(Pdb) self. embeddings\n",
      "ModuleList(\n",
      "  (0): Embedding(60, 50)\n",
      "  (1): Embedding(541, 50)\n",
      "  (2): Embedding(196, 50)\n",
      "  (3): Embedding(419, 50)\n",
      "  (4): Embedding(77805, 50)\n",
      "  (5): Embedding(404, 50)\n",
      "  (6): Embedding(185973, 50)\n",
      ")\n",
      "(Pdb) len(x_cat)\n",
      "32\n",
      "(Pdb) x_cat[:,0]\n",
      "tensor([48711,  2815, 21740, 15159, 76362, 27602, 75200, 73309, 43810, 18031,\n",
      "        75441,  8269, 54775, 10071, 21993,  9115, 64536, 24732, 25524, 49442,\n",
      "        24147, 75674,  7505, 39210, 14469, 43652, 11599, 21209, 24321, 16991,\n",
      "        14332, 29242], device='cuda:0')\n",
      "(Pdb) x_cat[:,6]\n",
      "tensor([19, 41, 19, 55,  7, 49, 55,  7, 55, 19, 55, 55,  0, 55, 55, 55, 51, 34,\n",
      "         2, 55, 55,  7, 54, 55, 55, 55, 55, 30,  0, 27, 55, 55],\n",
      "       device='cuda:0')\n",
      "(Pdb) [(x_cat[:,i]) for i, emb_layer in enumerate(self.embeddings)]\n",
      "*** NameError: name 'x_cat' is not defined\n",
      "(Pdb) x_cat\n",
      "tensor([[ 48711, 136758,    377,     84,    253,     20,     19],\n",
      "        [  2815,  11294,    347,    353,    253,     76,     41],\n",
      "        [ 21740,   5991,    470,    249,    253,    108,     19],\n",
      "        [ 15159,  56754,    377,    314,    253,     93,     55],\n",
      "        [ 76362, 183933,    377,    273,    253,    121,      7],\n",
      "        [ 27602,  89695,    377,    219,     28,     37,     49],\n",
      "        [ 75200, 182237,    377,    112,    253,     31,     55],\n",
      "        [ 73309, 106079,    377,    175,    253,    121,      7],\n",
      "        [ 43810,   2783,    419,    326,    253,     93,     55],\n",
      "        [ 18031,  60638,    377,    219,    413,    111,     19],\n",
      "        [ 75441,   3593,      1,     29,    253,    119,     55],\n",
      "        [  8269,  37173,    377,    283,    253,     49,     55],\n",
      "        [ 54775,   3300,    471,     80,    253,     54,      0],\n",
      "        [ 10071,   4219,    367,     29,    189,    164,     55],\n",
      "        [ 21993,  10714,    245,    264,    253,     91,     55],\n",
      "        [  9115,  39827,    377,    189,    328,    180,     55],\n",
      "        [ 64536, 165001,    377,    219,    253,    175,     51],\n",
      "        [ 24732,  12067,    377,    369,    115,    123,     34],\n",
      "        [ 25524,  15529,    377,    219,    370,    106,      2],\n",
      "        [ 49442, 138176,    377,    320,    214,     31,     55],\n",
      "        [ 24147,  10368,    377,     29,    253,     91,     55],\n",
      "        [ 75674,  48962,     82,    130,    253,      5,      7],\n",
      "        [  7505,  34328,    377,    295,    253,     47,     54],\n",
      "        [ 39210, 117823,    377,     31,    253,    122,     55],\n",
      "        [ 14469,   1535,    142,    264,     49,     87,     55],\n",
      "        [ 43652, 126055,    377,    230,    253,     54,     55],\n",
      "        [ 11599,  48219,    155,    377,    253,     91,     55],\n",
      "        [ 21209,  72837,    173,    341,    253,    149,     30],\n",
      "        [ 24321,  11360,    220,     75,    253,      0,      0],\n",
      "        [ 16991,  62597,    377,    219,    380,    127,     27],\n",
      "        [ 14332,  54436,    377,    219,    265,     79,     55],\n",
      "        [ 29242,  98172,    377,    179,    253,    122,     55]],\n",
      "       device='cuda:0')\n",
      "(Pdb) [(x_cat[:,i]) for i, emb_layer in enumerate(self.embeddings)\n",
      "*** SyntaxError: unexpected EOF while parsing\n",
      "(Pdb) [(x_cat[:,i]) for i, emb_layer in enumerate(self.embeddings)]\n",
      "*** NameError: name 'x_cat' is not defined\n",
      "(Pdb) x_cat\n",
      "tensor([[ 48711, 136758,    377,     84,    253,     20,     19],\n",
      "        [  2815,  11294,    347,    353,    253,     76,     41],\n",
      "        [ 21740,   5991,    470,    249,    253,    108,     19],\n",
      "        [ 15159,  56754,    377,    314,    253,     93,     55],\n",
      "        [ 76362, 183933,    377,    273,    253,    121,      7],\n",
      "        [ 27602,  89695,    377,    219,     28,     37,     49],\n",
      "        [ 75200, 182237,    377,    112,    253,     31,     55],\n",
      "        [ 73309, 106079,    377,    175,    253,    121,      7],\n",
      "        [ 43810,   2783,    419,    326,    253,     93,     55],\n",
      "        [ 18031,  60638,    377,    219,    413,    111,     19],\n",
      "        [ 75441,   3593,      1,     29,    253,    119,     55],\n",
      "        [  8269,  37173,    377,    283,    253,     49,     55],\n",
      "        [ 54775,   3300,    471,     80,    253,     54,      0],\n",
      "        [ 10071,   4219,    367,     29,    189,    164,     55],\n",
      "        [ 21993,  10714,    245,    264,    253,     91,     55],\n",
      "        [  9115,  39827,    377,    189,    328,    180,     55],\n",
      "        [ 64536, 165001,    377,    219,    253,    175,     51],\n",
      "        [ 24732,  12067,    377,    369,    115,    123,     34],\n",
      "        [ 25524,  15529,    377,    219,    370,    106,      2],\n",
      "        [ 49442, 138176,    377,    320,    214,     31,     55],\n",
      "        [ 24147,  10368,    377,     29,    253,     91,     55],\n",
      "        [ 75674,  48962,     82,    130,    253,      5,      7],\n",
      "        [  7505,  34328,    377,    295,    253,     47,     54],\n",
      "        [ 39210, 117823,    377,     31,    253,    122,     55],\n",
      "        [ 14469,   1535,    142,    264,     49,     87,     55],\n",
      "        [ 43652, 126055,    377,    230,    253,     54,     55],\n",
      "        [ 11599,  48219,    155,    377,    253,     91,     55],\n",
      "        [ 21209,  72837,    173,    341,    253,    149,     30],\n",
      "        [ 24321,  11360,    220,     75,    253,      0,      0],\n",
      "        [ 16991,  62597,    377,    219,    380,    127,     27],\n",
      "        [ 14332,  54436,    377,    219,    265,     79,     55],\n",
      "        [ 29242,  98172,    377,    179,    253,    122,     55]],\n",
      "       device='cuda:0')\n",
      "(Pdb) X = [emb_layer(x_cat[:,i]) for i, emb_layer in enumerate(self.embeddings)]\n",
      "*** NameError: name 'x_cat' is not defined\n",
      "(Pdb) emb_layer(x_cat[:,0])\n",
      "*** NameError: name 'emb_layer' is not defined\n",
      "(Pdb) emb_layer\n",
      "*** NameError: name 'emb_layer' is not defined\n",
      "(Pdb) self.embeddings(x_cat[:,0])\n",
      "*** NotImplementedError\n",
      "(Pdb) self.embeddings[0](x_cat[:,0])\n",
      "*** RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:214\n",
      "(Pdb) self.embeddings[0]\n",
      "Embedding(60, 50)\n",
      "(Pdb) x_cat[:,0]\n",
      "*** RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCTensorCopy.cu:206\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 3\n",
    "PRINT_PERIOD = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "all_models = []\n",
    "\n",
    "n_factor_list = [50, 100]\n",
    "layer_sizes_list = [[256, 100], [128, 64]]\n",
    "drops_list = [[0.6, 0.75], [0.9, 0.9]]\n",
    "emb_drop_list = [0.3, 0.2]\n",
    "batch_size_list = [32]\n",
    "epochs_list = [5]\n",
    "learning_rate_list = [0.0001]\n",
    "weight_decay_list = [2e-4, 2e-3]\n",
    "optimizer_list = [torch.optim.Adam]\n",
    "\n",
    "\n",
    "all_models, best_params, best_val_score = gridSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'batch_size': 32,\n",
       "  'drops': [0.6, 0.5],\n",
       "  'emb_drop': 0.1,\n",
       "  'emb_szs': {'Book_Author': 9673,\n",
       "   'Publisher': 3039,\n",
       "   'n_books': 185973,\n",
       "   'n_users': 77805},\n",
       "  'epochs': 3,\n",
       "  'features': ['New_User_ID',\n",
       "   'New_Book_ID',\n",
       "   'Book_Author',\n",
       "   'Publisher',\n",
       "   'Year_Of_Publication',\n",
       "   'User_Mean_Year_Of_Publication',\n",
       "   'User_Std_Year_Of_Publication',\n",
       "   'Age'],\n",
       "  'layer_sizes': [128, 64],\n",
       "  'learning_rate': 0.001,\n",
       "  'n_factors': 100},\n",
       " 1.6804326449196447)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params, best_val_score\n",
    "# best_params = gridSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dd2f959d6844989694f491c49d9ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ITERATION - loss: 0.00', max=167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold Number: 1\n",
      "Training Results - Epoch: 1  Avg RMSE: 1.56 Avg MAE: 1.19\n",
      "Validation Results - Epoch: 1  Avg RMSE: 1.82 Avg MAE: 1.43\n",
      "Training Results - Epoch: 2  Avg RMSE: 1.13 Avg MAE: 0.88\n",
      "Validation Results - Epoch: 2  Avg RMSE: 1.91 Avg MAE: 1.50\n",
      "Training Results - Epoch: 3  Avg RMSE: 0.78 Avg MAE: 0.59\n",
      "Validation Results - Epoch: 3  Avg RMSE: 1.86 Avg MAE: 1.45\n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0982deb74e4bd08df2e00b248587f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ITERATION - loss: 0.00', max=167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold Number: 2\n",
      "Training Results - Epoch: 1  Avg RMSE: 1.57 Avg MAE: 1.19\n",
      "Validation Results - Epoch: 1  Avg RMSE: 1.80 Avg MAE: 1.42\n",
      "Training Results - Epoch: 2  Avg RMSE: 1.14 Avg MAE: 0.89\n",
      "Validation Results - Epoch: 2  Avg RMSE: 1.91 Avg MAE: 1.50\n",
      "Training Results - Epoch: 3  Avg RMSE: 0.80 Avg MAE: 0.60\n",
      "Validation Results - Epoch: 3  Avg RMSE: 1.85 Avg MAE: 1.44\n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628ff6b6947a466a8c5e6a9f0ec4cb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ITERATION - loss: 0.00', max=167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold Number: 3\n",
      "Training Results - Epoch: 1  Avg RMSE: 1.50 Avg MAE: 1.14\n",
      "Validation Results - Epoch: 1  Avg RMSE: 1.80 Avg MAE: 1.44\n",
      "Training Results - Epoch: 2  Avg RMSE: 1.15 Avg MAE: 0.90\n",
      "Validation Results - Epoch: 2  Avg RMSE: 1.90 Avg MAE: 1.52\n",
      "Training Results - Epoch: 3  Avg RMSE: 0.76 Avg MAE: 0.58\n",
      "Validation Results - Epoch: 3  Avg RMSE: 1.83 Avg MAE: 1.44\n",
      "\r"
     ]
    }
   ],
   "source": [
    "FOLDS = 3\n",
    "PRINT_PERIOD = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "all_models = []\n",
    "n_factors = 100\n",
    "layer_sizes = [256, 100]\n",
    "drops = [0.5, 0.5]\n",
    "emb_drop = 0.1\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "params = {}\n",
    "params['n_factors'] = n_factors\n",
    "params['batch_size'] = batch_size\n",
    "params['epochs'] = epochs\n",
    "params['learning_rate'] = learning_rate\n",
    "params['layer_sizes'] = layer_sizes\n",
    "params['drops'] = drops\n",
    "params['emb_drop'] = emb_drop\n",
    "\n",
    "\n",
    "history = run_kfold(X, y, params, emb_szs=emb_c, y_of_pub_features=y_of_pub_features, age_features=age_features)\n",
    "\n",
    "\n",
    "# run_one(X, y, params, emb_szs=emb_c, y_of_pub_features=y_of_pub_features, age_features=age_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd083b3ea58>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd0be0adc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(train_metrics['loss'], label='train loss')\n",
    "plt.plot(val_metrics['loss'], label='val loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset[y_of_pub_features] = min_max_scaler_yop.transform(testset[y_of_pub_features])\n",
    "# testset[age_features] = min_max_scaler_age.transform(testset[age_features].values.reshape(-1, 1))\n",
    "# train_torch_data_set_meta = MixedInputDataSet(trainset[cat_features], trainset[cont_features], trainset[target])\n",
    "# test_torch_data_set_meta = MixedInputDataSet(testset[cat_features], testset[cont_features], testset[target])\n",
    "\n",
    "# train_data_loader_meta = DataLoader(train_torch_data_set_meta, batch_size=batch_size, shuffle=True)\n",
    "# test_data_loader_meta = DataLoader(test_torch_data_set_meta, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def init_embeddings(x):\n",
    "    x = x.weight.data\n",
    "    value = 2 / (x.size(1) + 1)\n",
    "    x.uniform_(-value, value)\n",
    "    \n",
    "\n",
    "    \n",
    "class MiniNetPlusMeta(nn.Module):\n",
    "#     def __init__(self, emb_szs, hidden=256, hidden2=100, n_factors=50, y_range=(1, 10), n_cont=0, n_cat=2):\n",
    "    def __init__(self, emb_szs, layer_sizes=[256, 100], n_factors=50, drops=[0.5, 0.5], emb_drop=0.1, \n",
    "                         y_range=(1, 10), n_cont=0, n_cat=2):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(emb_szs['n_users'], n_factors)\n",
    "        self.book_embedding = nn.Embedding(emb_szs['n_books'], n_factors)\n",
    "        self.author_embedding = nn.Embedding(emb_szs['Book_Author'], n_factors)\n",
    "        self.publisher_embedding = nn.Embedding(emb_szs['Publisher'], n_factors)\n",
    "        self.embeddings = [self.user_embedding, self.book_embedding, self.author_embedding, self.publisher_embedding]\n",
    "        \n",
    "#         self.linear1 = nn.Linear((n_factors * n_cat) + n_cont, hidden)\n",
    "#         self.linear2 = nn.Linear(hidden, hidden2)\n",
    "#         self.linear3 = nn.Linear(hidden2, 1)\n",
    "#         self.linears = [self.linear1, self.linear2, self.linear3]\n",
    "#         self.bn = nn.BatchNorm1d(n_cont)\n",
    "#         self.emb_drop = nn.Dropout(0.1)\n",
    "#         self.drop1 = nn.Dropout(0.5)\n",
    "#         self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear1 = nn.Linear((n_factors * n_cat) + n_cont, layer_sizes[0])\n",
    "        self.linear2 = nn.Linear(layer_sizes[0], layer_sizes[1])\n",
    "        self.linear3 = nn.Linear(layer_sizes[1], 1)\n",
    "        self.linears = [self.linear1, self.linear2, self.linear3]\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drop1 = nn.Dropout(drops[0])\n",
    "        self.drop2 = nn.Dropout(drops[1])\n",
    "        \n",
    "        for layer in self.embeddings:\n",
    "            init_embeddings(layer)\n",
    "            \n",
    "        for layer in self.linears:\n",
    "            nn.init.kaiming_normal(layer.weight.data)\n",
    "        \n",
    "        self.n_cont = n_cont\n",
    "        self.n_cat = n_cat\n",
    "        self.y_range = y_range\n",
    "    \n",
    "    def forward(self, X):\n",
    "        cats, conts = X[:, :self.n_cat], X[:, self.n_cat:]\n",
    "        cats, conts = cats.long(), conts.float()\n",
    "        cat_list = [self.embeddings[x](cats[:,x]) for x in range(self.n_cat)]\n",
    "        X = torch.cat(cat_list, dim=1)\n",
    "        X = self.emb_drop(X)\n",
    "        if self.n_cont != 0:\n",
    "            X2 = self.bn(conts)\n",
    "            X = torch.cat([X, X2], dim=1) if self.n_cont != 0 else X2\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = self.drop1(X)\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.drop2(X)\n",
    "        X = self.linear3(X)\n",
    "        X = F.sigmoid(X)\n",
    "        X = X * (self.y_range[1] - self.y_range[0])\n",
    "        X = X + self.y_range[0]\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 100\n",
    "max_rating = float(trainset.Book_Rating.max())\n",
    "min_rating = float(trainset.Book_Rating.min())\n",
    "\n",
    "n_cont = train_torch_data_set_meta.N_conts\n",
    "n_cat = train_torch_data_set_meta.N_cats\n",
    "\n",
    "model = MiniNetPlusMeta(emb_c, n_factors=n_factors, \n",
    "                        y_range=(min_rating, max_rating), n_cont=n_cont, n_cat=n_cat).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniNetPlusMeta(\n",
       "  (user_embedding): Embedding(77805, 100)\n",
       "  (book_embedding): Embedding(185973, 100)\n",
       "  (author_embedding): Embedding(59240, 100)\n",
       "  (publisher_embedding): Embedding(11095, 100)\n",
       "  (linear1): Linear(in_features=404, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.1)\n",
       "  (drop1): Dropout(p=0.5)\n",
       "  (drop2): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ignite\n",
    "# from ignite.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "# from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "# import torch\n",
    "# from ignite.engine.engine import Engine, State, Events\n",
    "\n",
    "# def apply_to_tensor(input_, func):\n",
    "#     \"\"\"\n",
    "#     Apply a function on a tensor or mapping, or sequence of tensors.\n",
    "#     \"\"\"\n",
    "#     return apply_to_type(input_, torch.Tensor, func)\n",
    "\n",
    "\n",
    "# def apply_to_type(input_, input_type, func):\n",
    "#     \"\"\"\n",
    "#     Apply a function on a object of `input_type` or mapping, or sequence of objects of `input_type`.\n",
    "#     \"\"\"\n",
    "#     if isinstance(input_, input_type):\n",
    "#         return func(input_)\n",
    "#     elif isinstance(input_, string_classes):\n",
    "#         return input_\n",
    "#     elif isinstance(input_, collections.Mapping):\n",
    "#         return {k: apply_to_type(sample, input_type, func) for k, sample in input_.items()}\n",
    "#     elif isinstance(input_, collections.Sequence):\n",
    "#         return [apply_to_type(sample, input_type, func) for sample in input_]\n",
    "#     else:\n",
    "#         raise TypeError((\"input must contain {}, dicts or lists; found {}\"\n",
    "#                          .format(input_type, type(input_))))\n",
    "\n",
    "\n",
    "# def convert_tensor(input_, device=None, non_blocking=False):\n",
    "#     \"\"\"\n",
    "#     Move tensors to relevant device.\n",
    "#     \"\"\"\n",
    "#     def _func(tensor):\n",
    "#         return tensor.to(device=device, non_blocking=non_blocking) if device else tensor\n",
    "\n",
    "#     return apply_to_tensor(input_, _func)\n",
    "\n",
    "# def _prepare_batch(batch, device=None, non_blocking=False):\n",
    "#     \"\"\"\n",
    "#     Prepare batch for training: pass to a device with options\n",
    "#     \"\"\"\n",
    "#     x, y = batch\n",
    "#     return (convert_tensor(x, device=device, non_blocking=non_blocking),\n",
    "#             convert_tensor(y, device=device, non_blocking=non_blocking))\n",
    "\n",
    "\n",
    "# all_lr = []\n",
    "\n",
    "# def create_supervised_trainer(model, optimizer, loss_fn,\n",
    "#                               device=None, non_blocking=False,\n",
    "#                               prepare_batch=_prepare_batch):\n",
    "#     if device:\n",
    "#         model.to(device)\n",
    "\n",
    "#     def _update(engine, batch):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n",
    "# #         users, books, conts = x[:,0], x[:,1], x[:,2:]\n",
    "# #         users, books, conts, y = users.long(), books.long(), conts.float(), y.float()\n",
    "#         y_pred = model(x)\n",
    "# #         y_pred = model(users, books, conts)\n",
    "# #         y_pred = y_pred.squeeze()\n",
    "#         loss = loss_fn(y_pred, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             all_lr.append(param_group['lr'])\n",
    "#         # scheduler\n",
    "#         scheduler.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     return Engine(_update)\n",
    "\n",
    "# n_batches = train_data_loader_meta.dataset.N // batch_size\n",
    "# cycle_len = 1\n",
    "# crit = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(get_trainable(model.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "# import visdom\n",
    "\n",
    "# def create_plot_window(vis, xlabel, ylabel, title):\n",
    "#     return vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n",
    "\n",
    "\n",
    "# vis = visdom.Visdom()\n",
    "\n",
    "\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# # scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "# PRINT_PERIOD = 2000\n",
    "# trainer = create_supervised_trainer(model, optimizer, crit, device=device)\n",
    "# evaluator = create_supervised_evaluator(model, metrics={'RMSE': RootMeanSquaredError(), 'MAE': MeanAbsoluteError()}, \n",
    "#                                        device=device)\n",
    "\n",
    "# train_loss_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Loss')\n",
    "# train_avg_mae_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Average MAE')\n",
    "# train_avg_rmse_window = create_plot_window(vis, '#Iterations', 'Accuracy', 'Training Average RMSE')\n",
    "# val_avg_mae_window = create_plot_window(vis, '#Epochs', 'Loss', 'Validation Average MAE')\n",
    "# val_avg_rmse_window = create_plot_window(vis, '#Epochs', 'Accuracy', 'Validation Average RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "import visdom\n",
    "\n",
    "n_batches = train_data_loader_meta.dataset.N // batch_size\n",
    "cycle_len = 1\n",
    "crit = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(get_trainable(model.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "def create_plot_window(vis, xlabel, ylabel, title):\n",
    "    return vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "PRINT_PERIOD = 2000\n",
    "trainer = create_supervised_trainer(model, optimizer, crit, device=device)\n",
    "evaluator = create_supervised_evaluator(model, metrics={'RMSE': RootMeanSquaredError(), 'MAE': MeanAbsoluteError()}, \n",
    "                                       device=device)\n",
    "\n",
    "train_loss_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Loss')\n",
    "train_avg_mae_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Average MAE')\n",
    "train_avg_rmse_window = create_plot_window(vis, '#Iterations', 'Accuracy', 'Training Average RMSE')\n",
    "val_avg_mae_window = create_plot_window(vis, '#Epochs', 'Loss', 'Validation Average MAE')\n",
    "val_avg_rmse_window = create_plot_window(vis, '#Epochs', 'Accuracy', 'Validation Average RMSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import datetime    \n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_ignite_{now}\")\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_train_loss(engine):\n",
    "    if (engine.state.iteration - 1) % PRINT_PERIOD == 0:\n",
    "        print(f'epoch: {engine.state.epoch} -- batch: {engine.state.iteration} -- loss: {engine.state.output:.2f}')\n",
    "        vis.line(X=np.array([engine.state.iteration]),\n",
    "                     Y=np.array([engine.state.output]),\n",
    "                     update='append', win=train_loss_window)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(train_data_loader_meta)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f'Train Results -- Epoch: {trainer.state.epoch}, RMSE: {metrics[\"RMSE\"]:.2f}, MAE: {metrics[\"MAE\"]:.2f}')\n",
    "    summary_writer.add_scalar('training/rmse', metrics['RMSE'])\n",
    "    summary_writer.add_scalar('training/mae', metrics['MAE'])\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics[\"RMSE\"]]),\n",
    "                 win=train_avg_rmse_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics['MAE']]),\n",
    "                 win=train_avg_mae_window, update='append')\n",
    "    \n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(test_data_loader_meta)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f'Validation Results -- Epoch: {trainer.state.epoch}, RMSE: {metrics[\"RMSE\"]:.2f}, MAE: {metrics[\"MAE\"]:.2f}')\n",
    "    summary_writer.add_scalar('validation/rmse', metrics['RMSE'])\n",
    "    summary_writer.add_scalar('validation/mae', metrics['MAE'])\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics[\"RMSE\"]]),\n",
    "                 win=val_avg_rmse_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics['MAE']]),\n",
    "                 win=val_avg_mae_window, update='append')   \n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.run(train_data_loader_meta, max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(model, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.MSELoss()\n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)\n",
    "\n",
    "\n",
    "def get_data(X, y):\n",
    "    # first two need to be user_id and book_id, meta after \n",
    "    user_ids, book_ids, conts = X[:, 0], X[:, 1], X[:, 2:]\n",
    "    user_ids = Variable(user_ids.long()).cuda()\n",
    "    book_ids = Variable(book_ids.long()).cuda()\n",
    "    conts = Variable(conts.float()).cuda()\n",
    "    y = Variable(y.float()).cuda()\n",
    "    return user_ids, book_ids, conts, y\n",
    "\n",
    "def trainNet(model, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = train_data_loader_meta\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(model, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, (data) in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            X, y = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            user_ids, book_ids, conts, y = get_data(X, y)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = model(user_ids, book_ids, conts)\n",
    "            loss_size = loss(outputs, y)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data[0]\n",
    "            total_train_loss += loss_size.data[0]\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for (X, y) in test_data_loader_meta:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            user_ids, book_ids, conts, y = get_data(X, y)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = model(user_ids, book_ids, conts)\n",
    "            val_loss_size = loss(val_outputs, y)\n",
    "            total_val_loss += val_loss_size.data[0]\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}, RMSE = {:.2f}\".format(total_val_loss / len(test_data_loader_meta), np.sqrt(total_val_loss / len(test_data_loader_meta))))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
    "    \n",
    "    \n",
    "trainNet(model, batch_size=128, n_epochs=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(model, train_data_loader_meta, loss=nn.MSELoss(), epochs=3, save=True, val_loader=test_data_loader_meta, \n",
    "#                                                     metrics=[mse, rmse, mae], cycle_len=1, print_period=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm_notebook, tnrange, tqdm\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import kaiming_normal\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import RMSprop\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cosine_annealing_lr import CosineAnnealingLR\n",
    "\n",
    "def fit(model, train_loader, loss, opt_fn=None, learning_rate=1e-3, batch_size=64, epochs=1, cycle_len=1, val_loader=None, metrics=None, \n",
    "                save=False, save_path='tmp/checkpoint.pth.tar', pre_saved=False, print_period=1000):\n",
    "        \n",
    "    if opt_fn:\n",
    "        optimizer = opt_fn(model.parameters(), lr=learning_rate)\n",
    "    else:  \n",
    "        optimizer = RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # for stepper \n",
    "    n_batches = int(len(train_loader.dataset) // train_loader.batch_size)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "    global all_lr\n",
    "    all_lr = []\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    if pre_saved:\n",
    "        checkpoint = torch.load(save_path)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print('...restoring model...')\n",
    "    begin = True\n",
    "    \n",
    "    for epoch_ in tnrange(1, epochs+1, desc='Epoch'):\n",
    "        \n",
    "        if pre_saved:      \n",
    "            if begin:\n",
    "                epoch = start_epoch\n",
    "                begin = False\n",
    "        else:\n",
    "            epoch = epoch_\n",
    "        \n",
    "        # training\n",
    "        train_loss = train(model, train_loader, optimizer, scheduler, loss, print_period)\n",
    "        \n",
    "        print_output = [epoch, train_loss]\n",
    "        \n",
    "        # validation\n",
    "        if val_loader:\n",
    "            val_loss = validate(model, val_loader, optimizer, loss, metrics)\n",
    "            if val_loss[0] < best_val_loss:\n",
    "                best_val_loss = val_loss[0]\n",
    "                \n",
    "                # save model     \n",
    "                if save:\n",
    "                    if save_path:\n",
    "                        ensure_dir(save_path)\n",
    "                        state = {\n",
    "                            'epoch': epoch,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_val_loss': best_val_loss,\n",
    "                            'optimizer': optimizer.state_dict()\n",
    "                        }\n",
    "                        save_checkpoint(state, save_path=save_path)\n",
    "                        \n",
    "            for i in val_loss: print_output.append(i)\n",
    "\n",
    "        # epoch, train loss, val loss, metrics (optional)\n",
    "        print('\\n', print_output)\n",
    "        # sys.stdout.write('\\r' + str(print_output))\n",
    "\n",
    "        # reset scheduler\n",
    "        if epoch_ % cycle_len == 0:\n",
    "            scheduler = scheduler._reset(epoch, T_max=n_batches*cycle_len)\n",
    "        \n",
    "        epoch += 1\n",
    "    \n",
    "    \n",
    "def get_data(X, y):\n",
    "    # first two need to be user_id and book_id, meta after \n",
    "    user_ids, book_ids, conts = X[:, 0], X[:, 1], X[:, 2:]\n",
    "    user_ids = Variable(user_ids.long()).cuda()\n",
    "    book_ids = Variable(book_ids.long()).cuda()\n",
    "    conts = Variable(conts.float()).cuda()\n",
    "    y = Variable(y.float()).cuda()\n",
    "    return user_ids, book_ids, conts, y\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, loss, print_period=1000):\n",
    "\n",
    "    # change this to show expontially weighted moving average\n",
    "    # avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\n",
    "    epoch_loss = 0.\n",
    "    n_batches = int(train_loader.dataset.N / train_loader.batch_size)\n",
    "    model.train()\n",
    "    \n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        user_ids, book_ids, conts, y = get_data(X, y)\n",
    "\n",
    "        y_hat = model(user_ids, book_ids, conts)\n",
    "        l = loss(y_hat, y)\n",
    "        epoch_loss += l.data[0]\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler\n",
    "        scheduler.step()\n",
    "        all_lr.append(scheduler.get_lr())\n",
    "\n",
    "        if i != 0 and i % print_period == 0:\n",
    "            # sys.stdout.write('\\r' + 'iteration: {} of n_batches: {}'.format(i, n_batches))\n",
    "            # sys.stdout.flush()\n",
    "            # print('iteration: {} of n_batches: {}'.format(i, n_batches))\n",
    "            statement = '[{}/{} ({:.0f}%)]'.format(i, n_batches, (i / n_batches)*100.)\n",
    "            sys.stdout.write('\\r' + statement)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "#     train_loss = epoch_loss / n_batches\n",
    "    train_loss = epoch_loss / train_loader.dataset.N\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validate(model, val_loader, optimizer, loss, metrics=None):\n",
    "    model.eval()\n",
    "    n_batches = int(val_loader.dataset.N / val_loader.batch_size)\n",
    "    total_loss = 0.\n",
    "    metric_scores = {}\n",
    "    if metrics:\n",
    "        for metric in metrics:\n",
    "            metric_scores[str(metric)] = []\n",
    "            \n",
    "    for i, (X_test, y_test) in enumerate(val_loader):\n",
    "        user_ids, book_ids, conts, y_test = get_data(X_test, y_test)\n",
    "        y_hat = model(user_ids, book_ids, conts)\n",
    "\n",
    "        l = loss(y_hat, y_test)\n",
    "        total_loss += l.data[0]\n",
    "\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                metric_scores[str(metric)].append(metric(y_test.data.cpu().numpy(), y_hat.data.cpu().numpy()))\n",
    "    if metrics:\n",
    "        final_metrics = []\n",
    "        for metric in metrics:\n",
    "#             final_metrics.append(np.sum(metric_scores[str(metric)]) / n_batches)\n",
    "            final_metrics.append(np.sum(metric_scores[str(metric)]) / val_loader.dataset.N)\n",
    "#         return total_loss / n_batches, final_metrics\n",
    "        return total_loss / val_loader.dataset.N, final_metrics\n",
    "    else:\n",
    "#         return total_loss / n_batches\n",
    "        return total_loss / val_loader.dataset.N\n",
    "\n",
    "\n",
    "def save_checkpoint(state, save_path='tmp/checkpoint.pth.tar'):\n",
    "    torch.save(state, save_path)\n",
    "\n",
    "# def predict(model, df, cat_flds, cont_flds):\n",
    "#     model.eval()\n",
    "\n",
    "#     cats = np.asarray(df[cat_flds], dtype=np.int64)\n",
    "#     conts = np.asarray(df[cont_flds], dtype=np.float32)\n",
    "#     x_cat = Variable(torch.from_numpy(cats))\n",
    "#     x_cont = Variable(torch.from_numpy(conts))\n",
    "#     pred = model(x_cat, x_cont)\n",
    "#     return pred.data.numpy().flatten()\n",
    "\n",
    "def load_model(model, save_path='tmp/checkpoint.pth.tar'):\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model\n",
    "\n",
    "def save_model(model, save_path='tmp/checkpoint.pth.tar'):\n",
    "    model.save_state_dict(save_path)\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "u_uniq = ratings.User_ID.unique()\n",
    "user2idx = {o:i for i,o in enumerate(u_uniq)}\n",
    "ratings['New_User_ID'] = ratings.User_ID.apply(lambda x: user2idx[x])\n",
    "\n",
    "m_uniq = ratings.ISBN.unique()\n",
    "book2idx = {o:i for i,o in enumerate(m_uniq)}\n",
    "ratings['New_Book_ID'] = ratings.ISBN.apply(lambda x: book2idx[x])\n",
    "\n",
    "n_users = int(ratings.New_User_ID.nunique())\n",
    "n_books = int(ratings.New_Book_ID.nunique())\n",
    "\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_trainable.<locals>.<genexpr> at 0x7f95866617d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trainable(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TorchDataSet(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.X = ratings[['New_User_ID', 'New_Book_ID']].values\n",
    "        self.y = ratings['Book_Rating'].values\n",
    "        self.N = len(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "batch_size = 32\n",
    "train_torch_data_set = TorchDataSet(train)\n",
    "test_torch_data_set = TorchDataSet(test)\n",
    "train_data_loader = DataLoader(train_torch_data_set, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_torch_data_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "n_factors = 50\n",
    "max_rating = float(train.Book_Rating.max())\n",
    "min_rating = float(train.Book_Rating.min())\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def rmse(x, y): \n",
    "    return np.sqrt(mse(x, y))\n",
    "\n",
    "def mae(x, y): \n",
    "    return np.abs((x-y)).mean()\n",
    "\n",
    "\n",
    "metrics=[mse, rmse, mae]\n",
    "\n",
    "\n",
    "class MiniNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books, hidden=100):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.linear1 = nn.Linear(n_factors * 2, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, 1)\n",
    "        self.drop1 = nn.Dropout(0.75)\n",
    "        self.drop2 = nn.Dropout(0.75)\n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_embedding.weight.data.uniform_(-0.01,0.01)       \n",
    "    \n",
    "    def forward(self, users, books):\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "        X = self.drop1(torch.cat([u, b], dim=1))\n",
    "        X = self.drop2(F.relu(self.linear1(X)))\n",
    "        return F.sigmoid(self.linear2(X)) * (max_rating - min_rating+1) + min_rating-0.5\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "class EmbeddingDotBias(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.book_bias = nn.Embedding(n_books, 1)\n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_embedding.weight.data.uniform_(-0.01,0.01)       \n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_bias.weight.data.uniform_(-0.01,0.01)\n",
    "    \n",
    "    def forward(self, users, books):\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "#         print(self.user_bias(users).size())\n",
    "        u_b = self.user_bias(users).squeeze()\n",
    "#         print(u.size(), u_b.size())\n",
    "        b_b = self.book_bias(books).squeeze()\n",
    "        X = ( (u * b).sum(1) ) + u_b + b_b\n",
    "        X = F.sigmoid(X) * (max_rating - min_rating) + max_rating\n",
    "        return X.view(-1, 1)\n",
    "    \n",
    "    \n",
    "class EmbeddingDot(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.user_embedding.weight.data.uniform_(0, 0.05)\n",
    "        self.book_embedding.weight.data.uniform_(0, 0.05)\n",
    "    \n",
    "    def forward(self, users, books):\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "        return (u * b).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TorchDataSet(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.X = ratings[['New_User_ID', 'New_Book_ID']].values\n",
    "        self.y = ratings['Book_Rating'].astype(np.float32).values\n",
    "        self.N = len(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_torch_data_set = TorchDataSet(train)\n",
    "test_torch_data_set = TorchDataSet(test)\n",
    "train_data_loader = DataLoader(train_torch_data_set, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_torch_data_set, batch_size=32, shuffle=False)\n",
    "\n",
    "class EmbeddingDotBias(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.book_bias = nn.Embedding(n_books, 1)\n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_embedding.weight.data.uniform_(-0.01,0.01)       \n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_bias.weight.data.uniform_(-0.01,0.01)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        users, books = X[:, 0], X[:, 1]\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "        u_b = self.user_bias(users).squeeze()\n",
    "        b_b = self.book_bias(books).squeeze()\n",
    "        X = ( (u * b).sum(1) ) + u_b + b_b\n",
    "        X = F.sigmoid(X) * (max_rating - min_rating) + max_rating\n",
    "        return X.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = EmbeddingDotBias(n_users, n_books).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from ignite.engine import Engine, create_supervised_evaluator, Events\n",
    "\n",
    "import torch\n",
    "\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "\n",
    "def apply_to_tensor(input_, func):\n",
    "    \"\"\"Apply a function on a tensor or mapping, or sequence of tensors.\n",
    "    \"\"\"\n",
    "    return apply_to_type(input_, torch.Tensor, func)\n",
    "\n",
    "\n",
    "def apply_to_type(input_, input_type, func):\n",
    "    \"\"\"Apply a function on a object of `input_type` or mapping, or sequence of objects of `input_type`.\n",
    "    \"\"\"\n",
    "    if isinstance(input_, input_type):\n",
    "        return func(input_)\n",
    "    elif isinstance(input_, string_classes):\n",
    "        return input_\n",
    "    elif isinstance(input_, collections.Mapping):\n",
    "        return {k: apply_to_type(sample, input_type, func) for k, sample in input_.items()}\n",
    "    elif isinstance(input_, collections.Sequence):\n",
    "        return [apply_to_type(sample, input_type, func) for sample in input_]\n",
    "    else:\n",
    "        raise TypeError((\"input must contain {}, dicts or lists; found {}\"\n",
    "                         .format(input_type, type(input_))))\n",
    "\n",
    "\n",
    "def convert_tensor(input_, device=None, non_blocking=False):\n",
    "    \"\"\"Move tensors to relevant device.\"\"\"\n",
    "    def _func(tensor):\n",
    "        return tensor.to(device=device, non_blocking=non_blocking) if device else tensor\n",
    "\n",
    "    return apply_to_tensor(input_, _func)\n",
    "\n",
    "def _prepare_batch(batch, device=None, non_blocking=False):\n",
    "    \"\"\"Prepare batch for training: pass to a device with options\n",
    "    \"\"\"\n",
    "    x, y = batch\n",
    "    return (convert_tensor(x, device=device, non_blocking=non_blocking),\n",
    "            convert_tensor(y, device=device, non_blocking=non_blocking))\n",
    "\n",
    "\n",
    "all_lr = []\n",
    "\n",
    "def create_supervised_trainer(model, optimizer, loss_fn,\n",
    "                              device=None, non_blocking=False,\n",
    "                              prepare_batch=_prepare_batch):\n",
    "    if device:\n",
    "        model.to(device)\n",
    "\n",
    "    def _update(engine, batch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            all_lr.append(param_group['lr'])\n",
    "        # scheduler\n",
    "        scheduler.step()\n",
    "        return loss.item()\n",
    "\n",
    "    return Engine(_update)\n",
    "\n",
    "n_batches = train_data_loader.dataset.N // batch_size\n",
    "cycle_len = 1\n",
    "crit = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(get_trainable(model.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "import visdom\n",
    "\n",
    "def create_plot_window(vis, xlabel, ylabel, title):\n",
    "    return vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n",
    "\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "PRINT_PERIOD = 2000\n",
    "trainer = create_supervised_trainer(model, optimizer, crit, device=device)\n",
    "evaluator = create_supervised_evaluator(model, metrics={'RMSE': RootMeanSquaredError(), 'MAE': MeanAbsoluteError()}, \n",
    "                                       device=device)\n",
    "\n",
    "train_loss_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Loss')\n",
    "train_avg_mae_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Average MAE')\n",
    "train_avg_rmse_window = create_plot_window(vis, '#Iterations', 'Accuracy', 'Training Average RMSE')\n",
    "val_avg_mae_window = create_plot_window(vis, '#Epochs', 'Loss', 'Validation Average MAE')\n",
    "val_avg_rmse_window = create_plot_window(vis, '#Epochs', 'Accuracy', 'Validation Average RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import datetime    \n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_ignite_{now}\")\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_train_loss(engine):\n",
    "    if (engine.state.iteration - 1) % PRINT_PERIOD == 0:\n",
    "        print(f'epoch: {engine.state.epoch} -- batch: {engine.state.iteration} -- loss: {engine.state.output:.2f}')\n",
    "        vis.line(X=np.array([engine.state.iteration]),\n",
    "                     Y=np.array([engine.state.output]),\n",
    "                     update='append', win=train_loss_window)\n",
    "        \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(train_data_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f'Train Results -- Epoch: {trainer.state.epoch}, RMSE: {metrics[\"RMSE\"]:.2f}, MAE: {metrics[\"MAE\"]:.2f}')\n",
    "    summary_writer.add_scalar('training/rmse', metrics['RMSE'])\n",
    "    summary_writer.add_scalar('training/mae', metrics['MAE'])\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics[\"RMSE\"]]),\n",
    "                 win=val_avg_rmse_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics['MAE']]),\n",
    "                 win=val_avg_mae_window, update='append')\n",
    "    \n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(test_data_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f'Validation Results -- Epoch: {trainer.state.epoch}, RMSE: {metrics[\"RMSE\"]:.2f}, MAE: {metrics[\"MAE\"]:.2f}')\n",
    "    summary_writer.add_scalar('validation/rmse', metrics['RMSE'])\n",
    "    summary_writer.add_scalar('validation/mae', metrics['MAE'])\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics[\"RMSE\"]]),\n",
    "                 win=train_avg_rmse_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics['MAE']]),\n",
    "                 win=train_avg_mae_window, update='append')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 -- batch: 1 -- loss: 55.54\n",
      "epoch: 1 -- batch: 2001 -- loss: 48.73\n",
      "epoch: 1 -- batch: 4001 -- loss: 44.13\n",
      "epoch: 1 -- batch: 6001 -- loss: 33.47\n",
      "epoch: 1 -- batch: 8001 -- loss: 45.13\n",
      "epoch: 1 -- batch: 10001 -- loss: 39.71\n",
      "Train Results -- Epoch: 1, RMSE: 6.22, MAE: 5.85\n",
      "Validation Results -- Epoch: 1, RMSE: 6.32, MAE: 5.95\n",
      "epoch: 2 -- batch: 12001 -- loss: 38.95\n",
      "epoch: 2 -- batch: 14001 -- loss: 36.80\n",
      "epoch: 2 -- batch: 16001 -- loss: 38.10\n",
      "epoch: 2 -- batch: 18001 -- loss: 32.88\n",
      "epoch: 2 -- batch: 20001 -- loss: 32.83\n",
      "Train Results -- Epoch: 2, RMSE: 5.31, MAE: 4.75\n",
      "Validation Results -- Epoch: 2, RMSE: 5.64, MAE: 5.10\n",
      "epoch: 3 -- batch: 22001 -- loss: 29.66\n",
      "epoch: 3 -- batch: 24001 -- loss: 24.86\n",
      "epoch: 3 -- batch: 26001 -- loss: 20.27\n",
      "epoch: 3 -- batch: 28001 -- loss: 19.02\n",
      "epoch: 3 -- batch: 30001 -- loss: 26.78\n",
      "epoch: 3 -- batch: 32001 -- loss: 31.84\n",
      "Train Results -- Epoch: 3, RMSE: 4.63, MAE: 3.95\n",
      "Validation Results -- Epoch: 3, RMSE: 5.20, MAE: 4.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f9516e0d828>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_data_loader, max_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'window_36be1b775885d2'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visdom\n",
    "import numpy as np\n",
    "vis = visdom.Visdom()\n",
    "# vis.text('Hello, world!')\n",
    "vis.image(np.ones((3, 10, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"schedule\": [\n",
    "          {\"learning_rate\": 0.1, \"epochs\": 1},\n",
    "          {\"learning_rate\": 0.2, \"epochs\": 1},\n",
    "          {\"learning_rate\": 0.3, \"epochs\": 1},\n",
    "          {\"learning_rate\": 0.4, \"epochs\": 17},\n",
    "          {\"learning_rate\": 0.04, \"epochs\": 14},\n",
    "          {\"learning_rate\": 0.004, \"epochs\": 8},\n",
    "          {\"learning_rate\": 0.0004, \"epochs\": 3}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class _LRScheduler(object):\n",
    "    def __init__(self, optimizer, last_epoch=-1):\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        self.step(last_epoch + 1)\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "    def get_lr(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "class CosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.optimizer = optimizer\n",
    "        super(CosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + np.cos(np.pi * self.last_epoch / self.T_max)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "    \n",
    "    def _reset(self, epoch, T_max):\n",
    "        \"\"\"\n",
    "        Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        return CosineAnnealingLR(self.optimizer, self.T_max, self.eta_min, last_epoch=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIM = Adam(\n",
    "    params=[\n",
    "        {\"params\": MODEL.features.parameters(), 'lr': 0.001},\n",
    "        {\"params\": MODEL.classifier.parameters(), 'lr': 0.001},\n",
    "        {\"params\": MODEL.final_classifiers.parameters(), 'lr': 0.001},\n",
    "    ],\n",
    ")\n",
    "\n",
    "def lambda_lr_features(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.1 * (0.75 ** (epoch - 3))\n",
    "\n",
    "def lambda_lr_classifier(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.75 ** (epoch - 3)\n",
    "\n",
    "def lambda_lr_final_classifiers(epoch):\n",
    "    if epoch < 5:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.88 ** (epoch - 3)\n",
    "\n",
    "LR_SCHEDULERS = [\n",
    "    LambdaLR(OPTIM, lr_lambda=[lambda_lr_features, lambda_lr_classifier, lambda_lr_final_classifiers])\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
