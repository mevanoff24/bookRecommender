{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix, csr_matrix, save_npz, load_npz\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_feather(filepath, **kwargs):\n",
    "    '''\n",
    "    input: (path to feather file)\n",
    "    read feather file to pandas dataframe\n",
    "    output: (pandas dataframe)\n",
    "    '''\n",
    "    return pd.read_feather(filepath, **kwargs)\n",
    "\n",
    "ratings = load_feather(DATA_PATH/'ratings_explicit_clean.feather')\n",
    "ratings.drop(['index'], axis=1, inplace=True)\n",
    "users = load_feather(DATA_PATH/'users_clean.feather')\n",
    "books = load_feather(DATA_PATH/'books_clean.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_uniq = ratings.User_ID.unique()\n",
    "user2idx = {o:i for i,o in enumerate(u_uniq)}\n",
    "ratings['New_User_ID'] = ratings.User_ID.apply(lambda x: user2idx[x])\n",
    "\n",
    "m_uniq = ratings.ISBN.unique()\n",
    "book2idx = {o:i for i,o in enumerate(m_uniq)}\n",
    "ratings['New_Book_ID'] = ratings.ISBN.apply(lambda x: book2idx[x])\n",
    "\n",
    "n_users = int(ratings.New_User_ID.nunique())\n",
    "n_books = int(ratings.New_Book_ID.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x, y):\n",
    "    return np.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def rmse(x, y): \n",
    "    return np.sqrt(mse(x, y))\n",
    "\n",
    "def mae(x, y): \n",
    "    return np.abs((x-y)).mean()\n",
    "\n",
    "# Training helpers\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "metrics=[mse, rmse, mae]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std of Year_Of_Publication for User\n",
    "def get_year_of_pub(ratings):\n",
    "    ratings['Year_Of_Publication'] = ratings.Year_Of_Publication.fillna(round(ratings.Year_Of_Publication.mean()))\n",
    "    user_group = ratings.groupby('User_ID', as_index=False)['Year_Of_Publication'].agg(\n",
    "                            {'User_Mean_Year_Of_Publication': 'mean', 'User_Std_Year_Of_Publication': 'std'})\n",
    "    user_group['User_Mean_Year_Of_Publication'] = round(user_group['User_Mean_Year_Of_Publication'], 2)\n",
    "    user_group['User_Std_Year_Of_Publication'] = round(user_group['User_Std_Year_Of_Publication'].fillna(0), 2)\n",
    "    ratings = ratings.merge(user_group, on='User_ID', how='left')\n",
    "    features = ['Year_Of_Publication', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication']\n",
    "    return ratings, features\n",
    "\n",
    "def get_age(ratings):\n",
    "    ratings['Age'] = ratings.Age.fillna(round(ratings.Age.median()))\n",
    "    features = ['Age']\n",
    "    return ratings, features\n",
    "\n",
    "def clean_text(ratings, col, missing_value=None):\n",
    "    '''\n",
    "    removes punct and lowers and joins text to single string\n",
    "    '''\n",
    "    ratings[col] = ratings[col].str.lower().replace(r'[^A-Za-z]', '', regex=True)\n",
    "    if missing_value: ratings[col] = ratings[col].fillna(missing_value)\n",
    "    return ratings, col\n",
    "\n",
    "def numericalize_col(ratings, col):\n",
    "    '''\n",
    "    convert string to category codes \n",
    "    '''\n",
    "    ratings[col] = ratings[col].astype('category').cat.codes\n",
    "    return ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Book_Author': 59240, 'Publisher': 11095, 'n_users': 77805, 'n_books': 185973}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    \n",
    "# class TorchDataSetMeta(Dataset):\n",
    "#     def __init__(self, ratings):\n",
    "#         self.X = ratings[['New_User_ID', 'New_Book_ID', 'Book_Author', \n",
    "#                           'Year_Of_Publication', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication',\n",
    "#                           'Age']].values\n",
    "#         self.X\n",
    "# #         self.X = ratings.drop('Book_Rating', axis=1).values\n",
    "#         self.y = ratings['Book_Rating'].values\n",
    "#         self.N = len(self.y)\n",
    "#         self.D = self.X.shape[1]\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.y[idx]\n",
    "    \n",
    "class MixedInputDataSet(Dataset):\n",
    "    def __init__(self, cats, conts, y):\n",
    "        self.cats = np.asarray(cats, dtype=np.int64)\n",
    "        self.conts = np.asarray(conts, dtype=np.float32)\n",
    "        self.N_cats = self.cats.shape[1]\n",
    "        self.N_conts = self.conts.shape[1]\n",
    "        self.X = np.hstack((self.cats, self.conts))\n",
    "        self.N = len(y)\n",
    "        y = np.zeros((n,1)) if y is None else y[:,None]\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "            \n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "# \n",
    "\n",
    "cat_features = ['New_User_ID', 'New_Book_ID', 'Book_Author', 'Publisher']\n",
    "cont_features = ['Year_Of_Publication', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication', 'Age']\n",
    "target = 'Book_Rating'\n",
    "\n",
    "\n",
    "ratings_df = ratings.copy()\n",
    "ratings_df = ratings_df.merge(books, on='ISBN', how='left')\n",
    "ratings_df = ratings_df.merge(users[['Age', 'User_ID']], on='User_ID', how='left')\n",
    "ratings_df, y_of_pub_features = get_year_of_pub(ratings_df)\n",
    "ratings_df, age_features = get_age(ratings_df)\n",
    "ratings_df, author_feature = clean_text(ratings_df, 'Book_Author', missing_value='')\n",
    "ratings_df, publisher_feature = clean_text(ratings_df, 'Publisher', missing_value='')\n",
    "\n",
    "emb_c = {n: len(c.astype('category').cat.categories)+1 for n,c in ratings_df[cat_features[2:]].items()}\n",
    "emb_c['n_users'] = n_users\n",
    "emb_c['n_books'] = n_books\n",
    "print(emb_c)\n",
    "\n",
    "ratings_df['Book_Author'].fillna('', inplace=True)\n",
    "ratings_df['Publisher'].fillna('', inplace=True)\n",
    "ratings_df = numericalize_col(ratings_df, 'Book_Author')\n",
    "ratings_df = numericalize_col(ratings_df, 'Publisher')\n",
    "\n",
    "trainset, testset = train_test_split(ratings_df, test_size=0.2, random_state=100)\n",
    "\n",
    "# \n",
    "min_max_scaler_yop = MinMaxScaler()\n",
    "trainset[y_of_pub_features] = min_max_scaler_yop.fit_transform(trainset[y_of_pub_features])\n",
    "testset[y_of_pub_features] = min_max_scaler_yop.transform(testset[y_of_pub_features])\n",
    "\n",
    "min_max_scaler_age = MinMaxScaler()\n",
    "trainset[age_features] = min_max_scaler_age.fit_transform(trainset[age_features].values.reshape(-1, 1))\n",
    "testset[age_features] = min_max_scaler_age.transform(testset[age_features].values.reshape(-1, 1))\n",
    "\n",
    "# train_torch_data_set_meta = TorchDataSetMeta(trainset)\n",
    "# test_torch_data_set_meta = TorchDataSetMeta(testset)\n",
    "\n",
    "batch_size = 32\n",
    "device = 'cuda'\n",
    "train_torch_data_set_meta = MixedInputDataSet(trainset[cat_features], trainset[cont_features], trainset[target])\n",
    "test_torch_data_set_meta = MixedInputDataSet(testset[cat_features], testset[cont_features], testset[target])\n",
    "\n",
    "train_data_loader_meta = DataLoader(train_torch_data_set_meta, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader_meta = DataLoader(test_torch_data_set_meta, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Rating</th>\n",
       "      <th>New_User_ID</th>\n",
       "      <th>New_Book_ID</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>Age</th>\n",
       "      <th>User_Mean_Year_Of_Publication</th>\n",
       "      <th>User_Std_Year_Of_Publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362941</th>\n",
       "      <td>229551</td>\n",
       "      <td>0809237849</td>\n",
       "      <td>10</td>\n",
       "      <td>64601</td>\n",
       "      <td>165195</td>\n",
       "      <td>Men and Other Reptiles</td>\n",
       "      <td>10085</td>\n",
       "      <td>0.980922</td>\n",
       "      <td>6235</td>\n",
       "      <td>en</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>0.975171</td>\n",
       "      <td>0.011789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        User_ID        ISBN  Book_Rating  New_User_ID  New_Book_ID  \\\n",
       "362941   229551  0809237849           10        64601       165195   \n",
       "\n",
       "                    Book_Title  Book_Author  Year_Of_Publication  Publisher  \\\n",
       "362941  Men and Other Reptiles        10085             0.980922       6235   \n",
       "\n",
       "       language       Age  User_Mean_Year_Of_Publication  \\\n",
       "362941       en  0.242105                       0.975171   \n",
       "\n",
       "        User_Std_Year_Of_Publication  \n",
       "362941                      0.011789  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    \n",
    "# class TorchDataSetMeta(Dataset):\n",
    "#     def __init__(self, ratings):\n",
    "#         self.X = ratings[['New_User_ID', 'New_Book_ID', 'Book_Author', \n",
    "#                           'Year_Of_Publication', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication',\n",
    "#                           'Age']].values\n",
    "#         self.X\n",
    "# #         self.X = ratings.drop('Book_Rating', axis=1).values\n",
    "#         self.y = ratings['Book_Rating'].values\n",
    "#         self.N = len(self.y)\n",
    "#         self.D = self.X.shape[1]\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.y[idx]\n",
    "    \n",
    "class MixedInputDataSet(Dataset):\n",
    "    def __init__(self, cats, conts, y):\n",
    "        self.cats = np.asarray(cats, dtype=np.int64)\n",
    "        self.conts = np.asarray(conts, dtype=np.float32)\n",
    "        self.N_cats = self.cats.shape[1]\n",
    "        self.N_conts = self.conts.shape[1]\n",
    "        self.X = np.hstack((self.cats, self.conts))\n",
    "        self.N = len(y)\n",
    "        y = np.zeros((n,1)) if y is None else y[:,None]\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "            \n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "# \n",
    "\n",
    "cat_features = ['New_User_ID', 'New_Book_ID', 'Book_Author', 'Publisher']\n",
    "cont_features = ['Year_Of_Publication', 'User_Mean_Year_Of_Publication', 'User_Std_Year_Of_Publication', 'Age']\n",
    "target = 'Book_Rating'\n",
    "\n",
    "\n",
    "ratings_df = ratings.copy()\n",
    "ratings_df = ratings_df.merge(books, on='ISBN', how='left')\n",
    "ratings_df = ratings_df.merge(users[['Age', 'User_ID']], on='User_ID', how='left')\n",
    "ratings_df, y_of_pub_features = get_year_of_pub(ratings_df)\n",
    "ratings_df, age_features = get_age(ratings_df)\n",
    "ratings_df, author_feature = clean_text(ratings_df, 'Book_Author', missing_value='')\n",
    "ratings_df, publisher_feature = clean_text(ratings_df, 'Publisher', missing_value='')\n",
    "\n",
    "emb_c = {n: len(c.astype('category').cat.categories)+1 for n,c in ratings_df[cat_features[2:]].items()}\n",
    "emb_c['n_users'] = n_users\n",
    "emb_c['n_books'] = n_books\n",
    "print(emb_c)\n",
    "\n",
    "ratings_df['Book_Author'].fillna('', inplace=True)\n",
    "ratings_df['Publisher'].fillna('', inplace=True)\n",
    "ratings_df = numericalize_col(ratings_df, 'Book_Author')\n",
    "ratings_df = numericalize_col(ratings_df, 'Publisher')\n",
    "\n",
    "trainset, testset = train_test_split(ratings_df, test_size=0.2, random_state=100)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# \n",
    "min_max_scaler_yop = MinMaxScaler()\n",
    "trainset[y_of_pub_features] = min_max_scaler_yop.fit_transform(trainset[y_of_pub_features])\n",
    "testset[y_of_pub_features] = min_max_scaler_yop.transform(testset[y_of_pub_features])\n",
    "\n",
    "min_max_scaler_age = MinMaxScaler()\n",
    "trainset[age_features] = min_max_scaler_age.fit_transform(trainset[age_features].values.reshape(-1, 1))\n",
    "testset[age_features] = min_max_scaler_age.transform(testset[age_features].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "device = 'cuda'\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "X = trainset[cat_features + cont_features]\n",
    "y = trainset[target]\n",
    "\n",
    "kfold = StratifiedKFold(3)\n",
    "for train_index, val_index in skf.split(X, y):\n",
    "    # need to do scaling corectly \n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    train_torch_data_set_meta = MixedInputDataSet(X_train[cat_features], X_train[cont_features], trainset[target])\n",
    "    val_torch_data_set_meta = MixedInputDataSet(X_val[cat_features], X_val[cont_features], y_val)\n",
    "    train_data_loader_meta = DataLoader(train_torch_data_set_meta, batch_size=batch_size, shuffle=True)\n",
    "    val_data_loader_meta = DataLoader(val_torch_data_set_meta, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# train_torch_data_set_meta = MixedInputDataSet(trainset[cat_features], trainset[cont_features], trainset[target])\n",
    "# test_torch_data_set_meta = MixedInputDataSet(testset[cat_features], testset[cont_features], testset[target])\n",
    "\n",
    "# train_data_loader_meta = DataLoader(train_torch_data_set_meta, batch_size=batch_size, shuffle=True)\n",
    "# test_data_loader_meta = DataLoader(test_torch_data_set_meta, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_features), len(cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "n_factors = 100\n",
    "max_rating = float(trainset.Book_Rating.max())\n",
    "min_rating = float(trainset.Book_Rating.min())\n",
    "\n",
    "\n",
    "def init_embeddings(x):\n",
    "    x = x.weight.data\n",
    "    value = 2 / (x.size(1) + 1)\n",
    "    x.uniform_(-value, value)\n",
    "    \n",
    "\n",
    "    \n",
    "class MiniNetPlusMeta(nn.Module):\n",
    "    def __init__(self, emb_szs, hidden=256, hidden2=100, y_range=(1, 10),\n",
    "                 n_cont=train_torch_data_set_meta.N_conts, n_cat=train_torch_data_set_meta.N_cats):\n",
    "#     def __init__(self, emb_szs, hidden_sizes=[256, 100], y_range=(1, 10),\n",
    "#                  n_cont=train_torch_data_set_meta.N_conts, n_cat=train_torch_data_set_meta.N_cats):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(emb_szs['n_users'], n_factors)\n",
    "        self.book_embedding = nn.Embedding(emb_szs['n_books'], n_factors)\n",
    "        self.author_embedding = nn.Embedding(emb_szs['Book_Author'], n_factors)\n",
    "        self.publisher_embedding = nn.Embedding(emb_szs['Publisher'], n_factors)\n",
    "        self.embeddings = [self.user_embedding, self.book_embedding, self.author_embedding, self.publisher_embedding]\n",
    "        \n",
    "#         self.embeddings = nn.ModuleList([nn.Embedding(x, n_factors) for x in emb_szs.values()])\n",
    "#         self.linears = nn.ModuleList([nn.Linear((n_factors * n_cat) + n_cont, hidden_sizes[0]), \n",
    "#                                       nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#                                       nn.Linear(hidden_sizes[1], 1)])\n",
    "        \n",
    "        self.linear1 = nn.Linear((n_factors * n_cat) + n_cont, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, hidden2)\n",
    "        self.linear3 = nn.Linear(hidden2, 1)\n",
    "        self.linears = [self.linear1, self.linear2, self.linear3]\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.emb_drop = nn.Dropout(0.1)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        for layer in self.embeddings:\n",
    "            init_embeddings(layer)\n",
    "            \n",
    "        for layer in self.linears:\n",
    "            nn.init.kaiming_normal(layer.weight.data)\n",
    "        \n",
    "        self.n_cont = n_cont\n",
    "        self.n_cat = n_cat\n",
    "        self.y_range = y_range\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         cats, conts = X[:, :self.n_cat], X[:, self.n_cat:]\n",
    "#         cats, conts = cats.long(), conts.float()\n",
    "#         cat_list = [self.embeddings[x](cats[:,x]) for x in range(self.n_cat)]\n",
    "#         X = torch.cat(cat_list, dim=1)\n",
    "#         X = self.emb_drop(X)\n",
    "#         if self.n_cont != 0:\n",
    "#             X2 = self.bn(conts)\n",
    "#             X = torch.cat([X, X2], dim=1) if self.n_cont != 0 else X2\n",
    "#         X = self.drop1(X)\n",
    "#         X = self.drop2(F.relu(self.linear1(X)))\n",
    "#         return F.sigmoid(self.linear2(X)) * (max_rating - min_rating+1) + min_rating-0.5\n",
    "    \n",
    "    def forward(self, X):\n",
    "        cats, conts = X[:, :self.n_cat], X[:, self.n_cat:]\n",
    "        cats, conts = cats.long(), conts.float()\n",
    "        cat_list = [self.embeddings[x](cats[:,x]) for x in range(self.n_cat)]\n",
    "        X = torch.cat(cat_list, dim=1)\n",
    "        X = self.emb_drop(X)\n",
    "        if self.n_cont != 0:\n",
    "            X2 = self.bn(conts)\n",
    "            X = torch.cat([X, X2], dim=1) if self.n_cont != 0 else X2\n",
    "#         X = self.drop1(X)\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = self.drop1(X)\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.drop2(X)\n",
    "        X = self.linear3(X)\n",
    "#         X = self.drop2(F.relu(self.linear1(X)))\n",
    "#         return F.sigmoid(self.linear2(X)) * (max_rating - min_rating+1) + min_rating-0.5\n",
    "\n",
    "        X = F.sigmoid(X)\n",
    "        X = X * (self.y_range[1] - self.y_range[0])\n",
    "        X = X + self.y_range[0]\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniNetPlusMeta(emb_c).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniNetPlusMeta(\n",
       "  (user_embedding): Embedding(77805, 100)\n",
       "  (book_embedding): Embedding(185973, 100)\n",
       "  (author_embedding): Embedding(59240, 100)\n",
       "  (publisher_embedding): Embedding(11095, 100)\n",
       "  (linear1): Linear(in_features=404, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.1)\n",
       "  (drop1): Dropout(p=0.5)\n",
       "  (drop2): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ignite\n",
    "# from ignite.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "# from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "# import torch\n",
    "# from ignite.engine.engine import Engine, State, Events\n",
    "\n",
    "# def apply_to_tensor(input_, func):\n",
    "#     \"\"\"\n",
    "#     Apply a function on a tensor or mapping, or sequence of tensors.\n",
    "#     \"\"\"\n",
    "#     return apply_to_type(input_, torch.Tensor, func)\n",
    "\n",
    "\n",
    "# def apply_to_type(input_, input_type, func):\n",
    "#     \"\"\"\n",
    "#     Apply a function on a object of `input_type` or mapping, or sequence of objects of `input_type`.\n",
    "#     \"\"\"\n",
    "#     if isinstance(input_, input_type):\n",
    "#         return func(input_)\n",
    "#     elif isinstance(input_, string_classes):\n",
    "#         return input_\n",
    "#     elif isinstance(input_, collections.Mapping):\n",
    "#         return {k: apply_to_type(sample, input_type, func) for k, sample in input_.items()}\n",
    "#     elif isinstance(input_, collections.Sequence):\n",
    "#         return [apply_to_type(sample, input_type, func) for sample in input_]\n",
    "#     else:\n",
    "#         raise TypeError((\"input must contain {}, dicts or lists; found {}\"\n",
    "#                          .format(input_type, type(input_))))\n",
    "\n",
    "\n",
    "# def convert_tensor(input_, device=None, non_blocking=False):\n",
    "#     \"\"\"\n",
    "#     Move tensors to relevant device.\n",
    "#     \"\"\"\n",
    "#     def _func(tensor):\n",
    "#         return tensor.to(device=device, non_blocking=non_blocking) if device else tensor\n",
    "\n",
    "#     return apply_to_tensor(input_, _func)\n",
    "\n",
    "# def _prepare_batch(batch, device=None, non_blocking=False):\n",
    "#     \"\"\"\n",
    "#     Prepare batch for training: pass to a device with options\n",
    "#     \"\"\"\n",
    "#     x, y = batch\n",
    "#     return (convert_tensor(x, device=device, non_blocking=non_blocking),\n",
    "#             convert_tensor(y, device=device, non_blocking=non_blocking))\n",
    "\n",
    "\n",
    "# all_lr = []\n",
    "\n",
    "# def create_supervised_trainer(model, optimizer, loss_fn,\n",
    "#                               device=None, non_blocking=False,\n",
    "#                               prepare_batch=_prepare_batch):\n",
    "#     if device:\n",
    "#         model.to(device)\n",
    "\n",
    "#     def _update(engine, batch):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n",
    "# #         users, books, conts = x[:,0], x[:,1], x[:,2:]\n",
    "# #         users, books, conts, y = users.long(), books.long(), conts.float(), y.float()\n",
    "#         y_pred = model(x)\n",
    "# #         y_pred = model(users, books, conts)\n",
    "# #         y_pred = y_pred.squeeze()\n",
    "#         loss = loss_fn(y_pred, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             all_lr.append(param_group['lr'])\n",
    "#         # scheduler\n",
    "#         scheduler.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     return Engine(_update)\n",
    "\n",
    "# n_batches = train_data_loader_meta.dataset.N // batch_size\n",
    "# cycle_len = 1\n",
    "# crit = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(get_trainable(model.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "# import visdom\n",
    "\n",
    "# def create_plot_window(vis, xlabel, ylabel, title):\n",
    "#     return vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n",
    "\n",
    "\n",
    "# vis = visdom.Visdom()\n",
    "\n",
    "\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# # scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "# PRINT_PERIOD = 2000\n",
    "# trainer = create_supervised_trainer(model, optimizer, crit, device=device)\n",
    "# evaluator = create_supervised_evaluator(model, metrics={'RMSE': RootMeanSquaredError(), 'MAE': MeanAbsoluteError()}, \n",
    "#                                        device=device)\n",
    "\n",
    "# train_loss_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Loss')\n",
    "# train_avg_mae_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Average MAE')\n",
    "# train_avg_rmse_window = create_plot_window(vis, '#Iterations', 'Accuracy', 'Training Average RMSE')\n",
    "# val_avg_mae_window = create_plot_window(vis, '#Epochs', 'Loss', 'Validation Average MAE')\n",
    "# val_avg_rmse_window = create_plot_window(vis, '#Epochs', 'Accuracy', 'Validation Average RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "import visdom\n",
    "\n",
    "n_batches = train_data_loader_meta.dataset.N // batch_size\n",
    "cycle_len = 1\n",
    "crit = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(get_trainable(model.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "def create_plot_window(vis, xlabel, ylabel, title):\n",
    "    return vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "PRINT_PERIOD = 2000\n",
    "trainer = create_supervised_trainer(model, optimizer, crit, device=device)\n",
    "evaluator = create_supervised_evaluator(model, metrics={'RMSE': RootMeanSquaredError(), 'MAE': MeanAbsoluteError()}, \n",
    "                                       device=device)\n",
    "\n",
    "train_loss_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Loss')\n",
    "train_avg_mae_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Average MAE')\n",
    "train_avg_rmse_window = create_plot_window(vis, '#Iterations', 'Accuracy', 'Training Average RMSE')\n",
    "val_avg_mae_window = create_plot_window(vis, '#Epochs', 'Loss', 'Validation Average MAE')\n",
    "val_avg_rmse_window = create_plot_window(vis, '#Epochs', 'Accuracy', 'Validation Average RMSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import datetime    \n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_ignite_{now}\")\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_train_loss(engine):\n",
    "    if (engine.state.iteration - 1) % PRINT_PERIOD == 0:\n",
    "        print(f'epoch: {engine.state.epoch} -- batch: {engine.state.iteration} -- loss: {engine.state.output:.2f}')\n",
    "        vis.line(X=np.array([engine.state.iteration]),\n",
    "                     Y=np.array([engine.state.output]),\n",
    "                     update='append', win=train_loss_window)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(train_data_loader_meta)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f'Train Results -- Epoch: {trainer.state.epoch}, RMSE: {metrics[\"RMSE\"]:.2f}, MAE: {metrics[\"MAE\"]:.2f}')\n",
    "    summary_writer.add_scalar('training/rmse', metrics['RMSE'])\n",
    "    summary_writer.add_scalar('training/mae', metrics['MAE'])\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics[\"RMSE\"]]),\n",
    "                 win=train_avg_rmse_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics['MAE']]),\n",
    "                 win=train_avg_mae_window, update='append')\n",
    "    \n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(test_data_loader_meta)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f'Validation Results -- Epoch: {trainer.state.epoch}, RMSE: {metrics[\"RMSE\"]:.2f}, MAE: {metrics[\"MAE\"]:.2f}')\n",
    "    summary_writer.add_scalar('validation/rmse', metrics['RMSE'])\n",
    "    summary_writer.add_scalar('validation/mae', metrics['MAE'])\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics[\"RMSE\"]]),\n",
    "                 win=val_avg_rmse_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics['MAE']]),\n",
    "                 win=val_avg_mae_window, update='append')   \n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 -- batch: 1 -- loss: 6.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-584f8a69d1e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch[%s] Complete. Time taken: %02d:%02d:%02d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_terminate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current run is terminating due to exception: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_terminate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ignite/engine/__init__.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(train_data_loader_meta, max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(model, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.MSELoss()\n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)\n",
    "\n",
    "\n",
    "def get_data(X, y):\n",
    "    # first two need to be user_id and book_id, meta after \n",
    "    user_ids, book_ids, conts = X[:, 0], X[:, 1], X[:, 2:]\n",
    "    user_ids = Variable(user_ids.long()).cuda()\n",
    "    book_ids = Variable(book_ids.long()).cuda()\n",
    "    conts = Variable(conts.float()).cuda()\n",
    "    y = Variable(y.float()).cuda()\n",
    "    return user_ids, book_ids, conts, y\n",
    "\n",
    "def trainNet(model, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = train_data_loader_meta\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(model, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, (data) in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            X, y = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            user_ids, book_ids, conts, y = get_data(X, y)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = model(user_ids, book_ids, conts)\n",
    "            loss_size = loss(outputs, y)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data[0]\n",
    "            total_train_loss += loss_size.data[0]\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for (X, y) in test_data_loader_meta:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            user_ids, book_ids, conts, y = get_data(X, y)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = model(user_ids, book_ids, conts)\n",
    "            val_loss_size = loss(val_outputs, y)\n",
    "            total_val_loss += val_loss_size.data[0]\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}, RMSE = {:.2f}\".format(total_val_loss / len(test_data_loader_meta), np.sqrt(total_val_loss / len(test_data_loader_meta))))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
    "    \n",
    "    \n",
    "trainNet(model, batch_size=128, n_epochs=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(model, train_data_loader_meta, loss=nn.MSELoss(), epochs=3, save=True, val_loader=test_data_loader_meta, \n",
    "#                                                     metrics=[mse, rmse, mae], cycle_len=1, print_period=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm_notebook, tnrange, tqdm\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import kaiming_normal\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import RMSprop\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cosine_annealing_lr import CosineAnnealingLR\n",
    "\n",
    "def fit(model, train_loader, loss, opt_fn=None, learning_rate=1e-3, batch_size=64, epochs=1, cycle_len=1, val_loader=None, metrics=None, \n",
    "                save=False, save_path='tmp/checkpoint.pth.tar', pre_saved=False, print_period=1000):\n",
    "        \n",
    "    if opt_fn:\n",
    "        optimizer = opt_fn(model.parameters(), lr=learning_rate)\n",
    "    else:  \n",
    "        optimizer = RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # for stepper \n",
    "    n_batches = int(len(train_loader.dataset) // train_loader.batch_size)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "    global all_lr\n",
    "    all_lr = []\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    if pre_saved:\n",
    "        checkpoint = torch.load(save_path)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print('...restoring model...')\n",
    "    begin = True\n",
    "    \n",
    "    for epoch_ in tnrange(1, epochs+1, desc='Epoch'):\n",
    "        \n",
    "        if pre_saved:      \n",
    "            if begin:\n",
    "                epoch = start_epoch\n",
    "                begin = False\n",
    "        else:\n",
    "            epoch = epoch_\n",
    "        \n",
    "        # training\n",
    "        train_loss = train(model, train_loader, optimizer, scheduler, loss, print_period)\n",
    "        \n",
    "        print_output = [epoch, train_loss]\n",
    "        \n",
    "        # validation\n",
    "        if val_loader:\n",
    "            val_loss = validate(model, val_loader, optimizer, loss, metrics)\n",
    "            if val_loss[0] < best_val_loss:\n",
    "                best_val_loss = val_loss[0]\n",
    "                \n",
    "                # save model     \n",
    "                if save:\n",
    "                    if save_path:\n",
    "                        ensure_dir(save_path)\n",
    "                        state = {\n",
    "                            'epoch': epoch,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_val_loss': best_val_loss,\n",
    "                            'optimizer': optimizer.state_dict()\n",
    "                        }\n",
    "                        save_checkpoint(state, save_path=save_path)\n",
    "                        \n",
    "            for i in val_loss: print_output.append(i)\n",
    "\n",
    "        # epoch, train loss, val loss, metrics (optional)\n",
    "        print('\\n', print_output)\n",
    "        # sys.stdout.write('\\r' + str(print_output))\n",
    "\n",
    "        # reset scheduler\n",
    "        if epoch_ % cycle_len == 0:\n",
    "            scheduler = scheduler._reset(epoch, T_max=n_batches*cycle_len)\n",
    "        \n",
    "        epoch += 1\n",
    "    \n",
    "    \n",
    "def get_data(X, y):\n",
    "    # first two need to be user_id and book_id, meta after \n",
    "    user_ids, book_ids, conts = X[:, 0], X[:, 1], X[:, 2:]\n",
    "    user_ids = Variable(user_ids.long()).cuda()\n",
    "    book_ids = Variable(book_ids.long()).cuda()\n",
    "    conts = Variable(conts.float()).cuda()\n",
    "    y = Variable(y.float()).cuda()\n",
    "    return user_ids, book_ids, conts, y\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, loss, print_period=1000):\n",
    "\n",
    "    # change this to show expontially weighted moving average\n",
    "    # avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\n",
    "    epoch_loss = 0.\n",
    "    n_batches = int(train_loader.dataset.N / train_loader.batch_size)\n",
    "    model.train()\n",
    "    \n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        user_ids, book_ids, conts, y = get_data(X, y)\n",
    "\n",
    "        y_hat = model(user_ids, book_ids, conts)\n",
    "        l = loss(y_hat, y)\n",
    "        epoch_loss += l.data[0]\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler\n",
    "        scheduler.step()\n",
    "        all_lr.append(scheduler.get_lr())\n",
    "\n",
    "        if i != 0 and i % print_period == 0:\n",
    "            # sys.stdout.write('\\r' + 'iteration: {} of n_batches: {}'.format(i, n_batches))\n",
    "            # sys.stdout.flush()\n",
    "            # print('iteration: {} of n_batches: {}'.format(i, n_batches))\n",
    "            statement = '[{}/{} ({:.0f}%)]'.format(i, n_batches, (i / n_batches)*100.)\n",
    "            sys.stdout.write('\\r' + statement)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "#     train_loss = epoch_loss / n_batches\n",
    "    train_loss = epoch_loss / train_loader.dataset.N\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validate(model, val_loader, optimizer, loss, metrics=None):\n",
    "    model.eval()\n",
    "    n_batches = int(val_loader.dataset.N / val_loader.batch_size)\n",
    "    total_loss = 0.\n",
    "    metric_scores = {}\n",
    "    if metrics:\n",
    "        for metric in metrics:\n",
    "            metric_scores[str(metric)] = []\n",
    "            \n",
    "    for i, (X_test, y_test) in enumerate(val_loader):\n",
    "        user_ids, book_ids, conts, y_test = get_data(X_test, y_test)\n",
    "        y_hat = model(user_ids, book_ids, conts)\n",
    "\n",
    "        l = loss(y_hat, y_test)\n",
    "        total_loss += l.data[0]\n",
    "\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                metric_scores[str(metric)].append(metric(y_test.data.cpu().numpy(), y_hat.data.cpu().numpy()))\n",
    "    if metrics:\n",
    "        final_metrics = []\n",
    "        for metric in metrics:\n",
    "#             final_metrics.append(np.sum(metric_scores[str(metric)]) / n_batches)\n",
    "            final_metrics.append(np.sum(metric_scores[str(metric)]) / val_loader.dataset.N)\n",
    "#         return total_loss / n_batches, final_metrics\n",
    "        return total_loss / val_loader.dataset.N, final_metrics\n",
    "    else:\n",
    "#         return total_loss / n_batches\n",
    "        return total_loss / val_loader.dataset.N\n",
    "\n",
    "\n",
    "def save_checkpoint(state, save_path='tmp/checkpoint.pth.tar'):\n",
    "    torch.save(state, save_path)\n",
    "\n",
    "# def predict(model, df, cat_flds, cont_flds):\n",
    "#     model.eval()\n",
    "\n",
    "#     cats = np.asarray(df[cat_flds], dtype=np.int64)\n",
    "#     conts = np.asarray(df[cont_flds], dtype=np.float32)\n",
    "#     x_cat = Variable(torch.from_numpy(cats))\n",
    "#     x_cont = Variable(torch.from_numpy(conts))\n",
    "#     pred = model(x_cat, x_cont)\n",
    "#     return pred.data.numpy().flatten()\n",
    "\n",
    "def load_model(model, save_path='tmp/checkpoint.pth.tar'):\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model\n",
    "\n",
    "def save_model(model, save_path='tmp/checkpoint.pth.tar'):\n",
    "    model.save_state_dict(save_path)\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "u_uniq = ratings.User_ID.unique()\n",
    "user2idx = {o:i for i,o in enumerate(u_uniq)}\n",
    "ratings['New_User_ID'] = ratings.User_ID.apply(lambda x: user2idx[x])\n",
    "\n",
    "m_uniq = ratings.ISBN.unique()\n",
    "book2idx = {o:i for i,o in enumerate(m_uniq)}\n",
    "ratings['New_Book_ID'] = ratings.ISBN.apply(lambda x: book2idx[x])\n",
    "\n",
    "n_users = int(ratings.New_User_ID.nunique())\n",
    "n_books = int(ratings.New_Book_ID.nunique())\n",
    "\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_trainable.<locals>.<genexpr> at 0x7f95866617d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trainable(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TorchDataSet(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.X = ratings[['New_User_ID', 'New_Book_ID']].values\n",
    "        self.y = ratings['Book_Rating'].values\n",
    "        self.N = len(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "batch_size = 32\n",
    "train_torch_data_set = TorchDataSet(train)\n",
    "test_torch_data_set = TorchDataSet(test)\n",
    "train_data_loader = DataLoader(train_torch_data_set, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_torch_data_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "n_factors = 50\n",
    "max_rating = float(train.Book_Rating.max())\n",
    "min_rating = float(train.Book_Rating.min())\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def rmse(x, y): \n",
    "    return np.sqrt(mse(x, y))\n",
    "\n",
    "def mae(x, y): \n",
    "    return np.abs((x-y)).mean()\n",
    "\n",
    "\n",
    "metrics=[mse, rmse, mae]\n",
    "\n",
    "\n",
    "class MiniNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books, hidden=100):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.linear1 = nn.Linear(n_factors * 2, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, 1)\n",
    "        self.drop1 = nn.Dropout(0.75)\n",
    "        self.drop2 = nn.Dropout(0.75)\n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_embedding.weight.data.uniform_(-0.01,0.01)       \n",
    "    \n",
    "    def forward(self, users, books):\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "        X = self.drop1(torch.cat([u, b], dim=1))\n",
    "        X = self.drop2(F.relu(self.linear1(X)))\n",
    "        return F.sigmoid(self.linear2(X)) * (max_rating - min_rating+1) + min_rating-0.5\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "class EmbeddingDotBias(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.book_bias = nn.Embedding(n_books, 1)\n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_embedding.weight.data.uniform_(-0.01,0.01)       \n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_bias.weight.data.uniform_(-0.01,0.01)\n",
    "    \n",
    "    def forward(self, users, books):\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "#         print(self.user_bias(users).size())\n",
    "        u_b = self.user_bias(users).squeeze()\n",
    "#         print(u.size(), u_b.size())\n",
    "        b_b = self.book_bias(books).squeeze()\n",
    "        X = ( (u * b).sum(1) ) + u_b + b_b\n",
    "        X = F.sigmoid(X) * (max_rating - min_rating) + max_rating\n",
    "        return X.view(-1, 1)\n",
    "    \n",
    "    \n",
    "class EmbeddingDot(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.user_embedding.weight.data.uniform_(0, 0.05)\n",
    "        self.book_embedding.weight.data.uniform_(0, 0.05)\n",
    "    \n",
    "    def forward(self, users, books):\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "        return (u * b).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TorchDataSet(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.X = ratings[['New_User_ID', 'New_Book_ID']].values\n",
    "        self.y = ratings['Book_Rating'].astype(np.float32).values\n",
    "        self.N = len(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_torch_data_set = TorchDataSet(train)\n",
    "test_torch_data_set = TorchDataSet(test)\n",
    "train_data_loader = DataLoader(train_torch_data_set, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_torch_data_set, batch_size=32, shuffle=False)\n",
    "\n",
    "class EmbeddingDotBias(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.book_bias = nn.Embedding(n_books, 1)\n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_embedding.weight.data.uniform_(-0.01,0.01)       \n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_bias.weight.data.uniform_(-0.01,0.01)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        users, books = X[:, 0], X[:, 1]\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "        u_b = self.user_bias(users).squeeze()\n",
    "        b_b = self.book_bias(books).squeeze()\n",
    "        X = ( (u * b).sum(1) ) + u_b + b_b\n",
    "        X = F.sigmoid(X) * (max_rating - min_rating) + max_rating\n",
    "        return X.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = EmbeddingDotBias(n_users, n_books).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from ignite.engine import Engine, create_supervised_evaluator, Events\n",
    "\n",
    "import torch\n",
    "\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "\n",
    "def apply_to_tensor(input_, func):\n",
    "    \"\"\"Apply a function on a tensor or mapping, or sequence of tensors.\n",
    "    \"\"\"\n",
    "    return apply_to_type(input_, torch.Tensor, func)\n",
    "\n",
    "\n",
    "def apply_to_type(input_, input_type, func):\n",
    "    \"\"\"Apply a function on a object of `input_type` or mapping, or sequence of objects of `input_type`.\n",
    "    \"\"\"\n",
    "    if isinstance(input_, input_type):\n",
    "        return func(input_)\n",
    "    elif isinstance(input_, string_classes):\n",
    "        return input_\n",
    "    elif isinstance(input_, collections.Mapping):\n",
    "        return {k: apply_to_type(sample, input_type, func) for k, sample in input_.items()}\n",
    "    elif isinstance(input_, collections.Sequence):\n",
    "        return [apply_to_type(sample, input_type, func) for sample in input_]\n",
    "    else:\n",
    "        raise TypeError((\"input must contain {}, dicts or lists; found {}\"\n",
    "                         .format(input_type, type(input_))))\n",
    "\n",
    "\n",
    "def convert_tensor(input_, device=None, non_blocking=False):\n",
    "    \"\"\"Move tensors to relevant device.\"\"\"\n",
    "    def _func(tensor):\n",
    "        return tensor.to(device=device, non_blocking=non_blocking) if device else tensor\n",
    "\n",
    "    return apply_to_tensor(input_, _func)\n",
    "\n",
    "def _prepare_batch(batch, device=None, non_blocking=False):\n",
    "    \"\"\"Prepare batch for training: pass to a device with options\n",
    "    \"\"\"\n",
    "    x, y = batch\n",
    "    return (convert_tensor(x, device=device, non_blocking=non_blocking),\n",
    "            convert_tensor(y, device=device, non_blocking=non_blocking))\n",
    "\n",
    "\n",
    "all_lr = []\n",
    "\n",
    "def create_supervised_trainer(model, optimizer, loss_fn,\n",
    "                              device=None, non_blocking=False,\n",
    "                              prepare_batch=_prepare_batch):\n",
    "    if device:\n",
    "        model.to(device)\n",
    "\n",
    "    def _update(engine, batch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            all_lr.append(param_group['lr'])\n",
    "        # scheduler\n",
    "        scheduler.step()\n",
    "        return loss.item()\n",
    "\n",
    "    return Engine(_update)\n",
    "\n",
    "n_batches = train_data_loader.dataset.N // batch_size\n",
    "cycle_len = 1\n",
    "crit = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(get_trainable(model.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "import visdom\n",
    "\n",
    "def create_plot_window(vis, xlabel, ylabel, title):\n",
    "    return vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n",
    "\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "PRINT_PERIOD = 2000\n",
    "trainer = create_supervised_trainer(model, optimizer, crit, device=device)\n",
    "evaluator = create_supervised_evaluator(model, metrics={'RMSE': RootMeanSquaredError(), 'MAE': MeanAbsoluteError()}, \n",
    "                                       device=device)\n",
    "\n",
    "train_loss_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Loss')\n",
    "train_avg_mae_window = create_plot_window(vis, '#Iterations', 'Loss', 'Training Average MAE')\n",
    "train_avg_rmse_window = create_plot_window(vis, '#Iterations', 'Accuracy', 'Training Average RMSE')\n",
    "val_avg_mae_window = create_plot_window(vis, '#Epochs', 'Loss', 'Validation Average MAE')\n",
    "val_avg_rmse_window = create_plot_window(vis, '#Epochs', 'Accuracy', 'Validation Average RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import datetime    \n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_ignite_{now}\")\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_train_loss(engine):\n",
    "    if (engine.state.iteration - 1) % PRINT_PERIOD == 0:\n",
    "        print(f'epoch: {engine.state.epoch} -- batch: {engine.state.iteration} -- loss: {engine.state.output:.2f}')\n",
    "        vis.line(X=np.array([engine.state.iteration]),\n",
    "                     Y=np.array([engine.state.output]),\n",
    "                     update='append', win=train_loss_window)\n",
    "        \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(train_data_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f'Train Results -- Epoch: {trainer.state.epoch}, RMSE: {metrics[\"RMSE\"]:.2f}, MAE: {metrics[\"MAE\"]:.2f}')\n",
    "    summary_writer.add_scalar('training/rmse', metrics['RMSE'])\n",
    "    summary_writer.add_scalar('training/mae', metrics['MAE'])\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics[\"RMSE\"]]),\n",
    "                 win=val_avg_rmse_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics['MAE']]),\n",
    "                 win=val_avg_mae_window, update='append')\n",
    "    \n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(test_data_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f'Validation Results -- Epoch: {trainer.state.epoch}, RMSE: {metrics[\"RMSE\"]:.2f}, MAE: {metrics[\"MAE\"]:.2f}')\n",
    "    summary_writer.add_scalar('validation/rmse', metrics['RMSE'])\n",
    "    summary_writer.add_scalar('validation/mae', metrics['MAE'])\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics[\"RMSE\"]]),\n",
    "                 win=train_avg_rmse_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([metrics['MAE']]),\n",
    "                 win=train_avg_mae_window, update='append')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 -- batch: 1 -- loss: 55.54\n",
      "epoch: 1 -- batch: 2001 -- loss: 48.73\n",
      "epoch: 1 -- batch: 4001 -- loss: 44.13\n",
      "epoch: 1 -- batch: 6001 -- loss: 33.47\n",
      "epoch: 1 -- batch: 8001 -- loss: 45.13\n",
      "epoch: 1 -- batch: 10001 -- loss: 39.71\n",
      "Train Results -- Epoch: 1, RMSE: 6.22, MAE: 5.85\n",
      "Validation Results -- Epoch: 1, RMSE: 6.32, MAE: 5.95\n",
      "epoch: 2 -- batch: 12001 -- loss: 38.95\n",
      "epoch: 2 -- batch: 14001 -- loss: 36.80\n",
      "epoch: 2 -- batch: 16001 -- loss: 38.10\n",
      "epoch: 2 -- batch: 18001 -- loss: 32.88\n",
      "epoch: 2 -- batch: 20001 -- loss: 32.83\n",
      "Train Results -- Epoch: 2, RMSE: 5.31, MAE: 4.75\n",
      "Validation Results -- Epoch: 2, RMSE: 5.64, MAE: 5.10\n",
      "epoch: 3 -- batch: 22001 -- loss: 29.66\n",
      "epoch: 3 -- batch: 24001 -- loss: 24.86\n",
      "epoch: 3 -- batch: 26001 -- loss: 20.27\n",
      "epoch: 3 -- batch: 28001 -- loss: 19.02\n",
      "epoch: 3 -- batch: 30001 -- loss: 26.78\n",
      "epoch: 3 -- batch: 32001 -- loss: 31.84\n",
      "Train Results -- Epoch: 3, RMSE: 4.63, MAE: 3.95\n",
      "Validation Results -- Epoch: 3, RMSE: 5.20, MAE: 4.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f9516e0d828>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_data_loader, max_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'window_36be1b775885d2'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visdom\n",
    "import numpy as np\n",
    "vis = visdom.Visdom()\n",
    "# vis.text('Hello, world!')\n",
    "vis.image(np.ones((3, 10, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"schedule\": [\n",
    "          {\"learning_rate\": 0.1, \"epochs\": 1},\n",
    "          {\"learning_rate\": 0.2, \"epochs\": 1},\n",
    "          {\"learning_rate\": 0.3, \"epochs\": 1},\n",
    "          {\"learning_rate\": 0.4, \"epochs\": 17},\n",
    "          {\"learning_rate\": 0.04, \"epochs\": 14},\n",
    "          {\"learning_rate\": 0.004, \"epochs\": 8},\n",
    "          {\"learning_rate\": 0.0004, \"epochs\": 3}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class _LRScheduler(object):\n",
    "    def __init__(self, optimizer, last_epoch=-1):\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        self.step(last_epoch + 1)\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "    def get_lr(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "class CosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.optimizer = optimizer\n",
    "        super(CosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + np.cos(np.pi * self.last_epoch / self.T_max)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "    \n",
    "    def _reset(self, epoch, T_max):\n",
    "        \"\"\"\n",
    "        Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        return CosineAnnealingLR(self.optimizer, self.T_max, self.eta_min, last_epoch=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIM = Adam(\n",
    "    params=[\n",
    "        {\"params\": MODEL.features.parameters(), 'lr': 0.001},\n",
    "        {\"params\": MODEL.classifier.parameters(), 'lr': 0.001},\n",
    "        {\"params\": MODEL.final_classifiers.parameters(), 'lr': 0.001},\n",
    "    ],\n",
    ")\n",
    "\n",
    "def lambda_lr_features(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.1 * (0.75 ** (epoch - 3))\n",
    "\n",
    "def lambda_lr_classifier(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.75 ** (epoch - 3)\n",
    "\n",
    "def lambda_lr_final_classifiers(epoch):\n",
    "    if epoch < 5:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.88 ** (epoch - 3)\n",
    "\n",
    "LR_SCHEDULERS = [\n",
    "    LambdaLR(OPTIM, lr_lambda=[lambda_lr_features, lambda_lr_classifier, lambda_lr_final_classifiers])\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
