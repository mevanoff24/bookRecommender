{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "Content Based\n",
    "- based on attributes of items / users. Doesn't take into consideration behavior or interations of different users or visitors. The only thing you can use is other attributes of products. Very limitting. Not able to recommend you something that is completely different, just recommends you something that is 'similar' that you have been interativing with in the past. \n",
    "\n",
    "\n",
    "Collaborative Filtering\n",
    "- more powerful since it can help you explore diverse content based on other similar users. Collaborative methods work with the interaction matrix that can also be called rating matrix in the rare case when users provide explicit rating of items. \n",
    "\n",
    "\n",
    "User based k-Nearest Neighbors\n",
    "- use cosine similarity, Jaccard similarity, etc. (correlation similarity) of rows (users) or columns (items) and recommends items that k — nearest neighbors enjoyed. \n",
    "- Lazy algorithm. Tough to scale \n",
    "\n",
    "\n",
    "Matrix Factorization\n",
    "- Matrix factorization attempts to reduce dimensionality of the interaction matrix (rating matrix) and approximate it by two or more small matrices (q and p) with k latent components. The Matrix Factorization techniques are usually more effective, because they allow users to discover the latent (hidden)features underlying the interactions between users and items (books). Takes into account ratings for individual users. For example, if one user always gives higher ratings or low ratings, this algorithm can take this into account and make a relative contribution to the system.\n",
    "- Optimizations\n",
    " - stochastic gradient descent. Most popular training algorithm is a stochastic gradient descent minimizing loss by gradient updates of both columns and rows of p a q matrices. The p and q matrixes are updated independently \n",
    " - ALS. Alternating Least Squares method that iteratively optimizes matrix p and matrix q by general least squares step. Iterily, fix P and optimize q, fix q and optimize p, etc... Need like 10 iterations. \n",
    " \n",
    " \n",
    "Association rules\n",
    "- Association rules can also be used for recommendation. Items that are frequently consumed together are connected with an edge in the graph. You can see clusters of best sellers (densely connected items that almost everybody interacted with) and small separated clusters of niche content. Mining rules is not very scalable. The APRIORI algorithm explores the state space of possible frequent itemsets and eliminates branches of the search space, that are not frequent. Frequent itemsets are used to generate rules and these rules generate recommendations.\n",
    "\n",
    "Neural Network\n",
    "- Can use auto-encoder to reconstruct the information (bottleneck), typically smaller size, same number of inputs and outputs. The benefit of this over basic Matrix Factorization is that we can have multiple layer for interations and can model non-lineararities. This can be used as the rating matrix then another algorithm can be run on top (KNN). \n",
    "- Nice because you can use both Content Based and Collaborative Filtering in one hybrid algorithm. As you can train auto-encoder to encode attributes, then you perform matrix factorization on interations and you train it together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "The actual 'ranking' of the model is not the main goal (only in acedimia). Ideally there is no 'correct' or 'ideal' metric in recommender systems. A business is ideally trying to optimizer it revenue or proxy for it, such as clicks, revenue, impressions, etc... Therefore in the real world we should A/B test the top results to determine what suits the business more. \n",
    "\n",
    "\n",
    "#### Online Testing \n",
    "The only way to determine is my deploying your algorithm and collecting data based on how users behave. Explore - exploit delemmma. Put recommendations in front of different users and measure if they buy, watch, or have interest in the recommendations you have presented. \n",
    "- Surragate problem\n",
    "\n",
    "At the end of the day, the only evaluation metric that matters is the results from your online A/B testing. \n",
    "\n",
    "Need to consider 'surprise' and 'diversity'. \n",
    "\n",
    "\n",
    "#### Offline Testing \n",
    "Root Mean Squared Error\n",
    "- The typical metric for recommender systems. How far off is the prediction. Punishes larger errors more. \n",
    "- Netfix (couldn't use it in the real world)\n",
    "\n",
    "\n",
    "Precision / Recall on top-N\n",
    "- Since you are showing a list of books that are recommended, how many of them are irrelevant\n",
    "- precision -- Precision at k is the proportion of recommended books in the top-k set that are relevant. Its interpretation is as follows. Suppose that my precision at 10 in a top-10 recommendation problem is 80%. This means that 80% of the recommendation I make are relevant to the user.\n",
    "\n",
    "Precision@k = (# of recommended items @k that are relevant) / (# of recommended items @k).\n",
    "\n",
    "- recall -- Recall at k is the proportion of relevant items found in the top-k recommendations. Suppose that we computed recall at 10 and found it is 40% in our top-10 recommendation system. This means that 40% of the total number of the relevant items appear in the top-k results.\n",
    "\n",
    "Recall@k = (# of recommended items @k that are relevant) / (total # of relevant items)\n",
    "\n",
    "\n",
    "Discounted Comulitve Gain\n",
    "- DCG takes also the position into consideration assuming that relevance of items logarithmically decreases.\n",
    "\n",
    "\n",
    "\n",
    "Multi-Objective optimization\n",
    "- can penalize best sellers, can increase the diversity of recommendations, and the precision is increased \n",
    "\n",
    "\n",
    "\n",
    "TopN Hit rate\n",
    "- Sum up number of hits in the topN\n",
    "- RMSE and Hit Rates aren't always related \n",
    "\n",
    "ARHR (average reciprocal hit rate) \n",
    "- more user focused metric, since more important to get the top value right than the bottom value. Sum up the reciprocal rank of each hit. For example 3 - 1/3 /  2 - 1/2 /  1 - 1 \n",
    "\n",
    "\n",
    "Need to consider \n",
    "- coverage \n",
    "- diversity - use average similarity scores between topN to measure diversity diversity = (1 - similarity score)\n",
    "- novelty - mean populatarity rank of recommended items \n",
    "- churn\n",
    "- responsiveness - how quickly does new user behavior influnce your recommendations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold Start Problem \n",
    "\n",
    "\n",
    "Since we won't have interations everytime, thus sometimes we have items or user completely without the interactions or even just one or two interations. Therefore you are not able to use the collaborative filtering approach. So you typically use content based methods. \n",
    "\n",
    "If you have an article you can measure the 'similarity' of different articles with NLP techniques, BOW, Tf-Idf, LSA, word vectors, Average Tf-idf word vectors, etc..\n",
    "\n",
    "Therefore items clustered based on their interaction similarity and attribute similarity are often aligned.\n",
    "\n",
    "\n",
    "You can use neural network to predict interaction similarity from attributes similarity and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feather(filepath, **kwargs):\n",
    "    '''\n",
    "    input: (path to feather file)\n",
    "    read feather file to pandas dataframe\n",
    "    output: (pandas dataframe)\n",
    "    '''\n",
    "    return pd.read_feather(filepath, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = load_feather(DATA_PATH/'ratings_explicit_clean.feather')\n",
    "users = load_feather(DATA_PATH/'users_clean.feather')\n",
    "books = load_feather(DATA_PATH/'books_clean.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276736</td>\n",
       "      <td>3257224281</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276737</td>\n",
       "      <td>0600570967</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID        ISBN  Book_Rating\n",
       "0   276726  0155061224            5\n",
       "1   276729  052165615X            3\n",
       "2   276729  0521795028            6\n",
       "3   276736  3257224281            8\n",
       "4   276737  0600570967            6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.drop(['index'], axis=1, inplace=True)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>W. W. Norton &amp; Company</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book_Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book_Author  Year_Of_Publication                Publisher language  \n",
       "0    Mark P. O. Morford               2002.0  Oxford University Press       en  \n",
       "1  Richard Bruce Wright               2001.0    HarperFlamingo Canada       it  \n",
       "2          Carlo D'Este               1991.0          HarperPerennial       en  \n",
       "3      Gina Bari Kolata               1999.0     Farrar Straus Giroux       en  \n",
       "4       E. J. W. Barber               1999.0   W. W. Norton & Company       en  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID                            Location   Age\n",
       "0        1                  nyc, new york, usa   NaN\n",
       "1        2           stockton, california, usa  18.0\n",
       "2        3     moscow, yukon territory, russia   NaN\n",
       "3        4           porto, v.n.gaia, portugal  17.0\n",
       "4        5  farnborough, hants, united kingdom   NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% of individuals have rated 1.0 number of books or fewer.\n",
      "The mean number of books rated is 5.57.\n",
      "The total number of user-book interactions in the dataset is 433671.\n",
      "The number of unique books: 271360\n",
      "The number of users: 278858\n",
      "The number of unique books that have at least 1 rating: 149836\n",
      "The most rated book in the dataset was rated 707 times.\n",
      "The book name of the most rated book is \"The Lovely Bones: A Novel\"\n"
     ]
    }
   ],
   "source": [
    "print('50% of individuals have rated {} number of books or fewer.'.format(ratings.groupby(['User_ID'])['Book_Rating'].count().median()))\n",
    "print('The mean number of books rated is {}.'.format(round(ratings.groupby(['User_ID'])['Book_Rating'].count().mean(), 2)))\n",
    "print('The total number of user-book interactions in the dataset is {}.'.format(ratings.groupby(['User_ID', 'ISBN']).count().sum().values[0]))\n",
    "print('The number of unique books: {}'.format(books.ISBN.nunique()))\n",
    "print('The number of users: {}'.format(users.User_ID.nunique()))\n",
    "print('The number of unique books that have at least 1 rating: {}'.format(books.ISBN[books.ISBN.isin(ratings.ISBN)].nunique()))\n",
    "print('The most rated book in the dataset was rated {} times.'.format(ratings.ISBN.value_counts().max()))\n",
    "print('The book name of the most rated book is \"{}\"'.format(books.loc[books.ISBN == ratings.ISBN.value_counts().index[0], 'Book_Title'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-personalized recommenders\n",
    "\n",
    "I am not going to take into account what you have liked and disliked before in order to suggest new things to you\n",
    "For example, if I go to amazon.com, I should expect to see things that I would like to buy regardless if I have been to amazon.com before. \n",
    "I can use the behavior of the population as a whole in order to infer what you might like \n",
    "Other people like this, so you will probably like it to! \n",
    "\n",
    "\n",
    "### Average Book Rating \n",
    "\n",
    "\n",
    "Average Rating issues\n",
    "- Book with one 5 star would be put higher than a book with 1000 ratings with 4.95 stars. Therefore we need to build a confidence interval (we are typical strategy for ranking is to be pessimistic and use the lower bound). Suppose 2 books have 4 stars on average. Book 1 has 3 rating and Book 2 has 100, if you use the upper bound, Book 1 would be ranked higher than Book 2, which is obviously not desired. The law of large number states that the more reviews the ‘narrower’ the confidence interval (distribution), so the ‘lower bound’ will be higher! Central limit theorem . Higher number of rating > smaller CI > higher lower bound. Some issues with low rated books. In this case popularity is what really increases this score. \n",
    "- 95% of the normal distribution with CDF function. \n",
    "- Explore / exploit dilemma \n",
    "- Since we only have a rating limit of 10, we don't include smoothing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non personalized recommenders\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "class NonPersonalizedRecommendation:\n",
    "    '''Non-Personalized Recommendations'''\n",
    "    \n",
    "    def __init__(self, rating_limit=100, top_k=30, langs=[]):\n",
    "        '''\n",
    "        input: (\n",
    "            top_k: number of recommendations to return: int\n",
    "            rating_limit: lower bound for ratings: int\n",
    "            top_k: number of recommendations to return: int\n",
    "            langs: list of subsetted languages: list\n",
    "        )\n",
    "        '''\n",
    "        #  we don't include smoothing, need a value above 10\n",
    "        if rating_limit < 10: \n",
    "            raise ValueError('choose value about 10: Your value: {}'.format(rating_limit))\n",
    "        self.rating_limit = rating_limit\n",
    "        self.top_k = top_k \n",
    "        self.langs = langs\n",
    "        self.init_subset = 100\n",
    "    \n",
    "    def _compute_lower_bound(self, isbn):\n",
    "        '''\n",
    "        input: (isbn: Book ISBN: str)\n",
    "        computes a 95% confidence interval based on book ratings\n",
    "        output: (lower bound confidence interval score: float)\n",
    "        '''\n",
    "        return sms.DescrStatsW(ratings.Book_Rating[ratings.ISBN == isbn].values).tconfint_mean()[0]\n",
    "\n",
    "    def _compute_average_ratings(self, ratings, rating_limit):\n",
    "        '''\n",
    "        input: (\n",
    "            ratings: pandas dataframe\n",
    "            rating_limit: lower bound for ratings: int\n",
    "        )\n",
    "        groups books based on ISBN, computes stats and returns subseted ratings df\n",
    "        output: (pandas dataframe)\n",
    "        '''\n",
    "        average_ratings = ratings.groupby('ISBN')['Book_Rating'].agg(['mean', 'count', 'std'])\n",
    "        average_ratings = average_ratings[average_ratings['count'] > self.rating_limit]\n",
    "        return average_ratings\n",
    "\n",
    "    def _get_top_books(self, average_ratings, books, col):\n",
    "        '''\n",
    "        input: (\n",
    "            average_ratings: pandas dataframe\n",
    "            books: pandas dataframe\n",
    "            col: column to sort by: str\n",
    "        )\n",
    "        sort dataframe by 'col' and joins book information to dataframe\n",
    "        output: (pandas dataframe)\n",
    "        '''\n",
    "        top_books = average_ratings.sort_values([col], ascending=False)[:self.init_subset]    \n",
    "        top_books = top_books.merge(books, left_index=True, right_on='ISBN', how='left')\n",
    "        return top_books.dropna()\n",
    "\n",
    "    def simple_recommendation(self, ratings, books):\n",
    "        '''\n",
    "        input: (\n",
    "           ratings: pandas dataframe\n",
    "           books: pandas dataframe\n",
    "        )\n",
    "        calls `compute_average_ratings` and `get_top_books`\n",
    "        returns top_k books based on mean value\n",
    "        output: (pandas dataframe)\n",
    "        '''\n",
    "        average_ratings = self._compute_average_ratings(ratings, self.rating_limit)\n",
    "        top_books = self._get_top_books(average_ratings, books, 'mean')\n",
    "#         top_books = top_books[['Book_Title', 'Book_Author', 'mean', 'count']]\n",
    "        return top_books.sort_values(['mean'], ascending=False)[:self.top_k]\n",
    "\n",
    "    def confidence_recommendation(self, ratings, books):\n",
    "        '''\n",
    "        input: (\n",
    "           ratings: pandas dataframe\n",
    "           books: pandas dataframe\n",
    "        )\n",
    "        calls `compute_average_ratings` and `get_top_books`\n",
    "        subsets data by language\n",
    "        returns top_k books based on lower bound confidence interval score\n",
    "        output: (pandas dataframe)\n",
    "        '''\n",
    "        average_ratings = self._compute_average_ratings(ratings, self.rating_limit)\n",
    "        average_ratings['score'] = average_ratings.index.map(lambda x: self._compute_lower_bound(x))\n",
    "\n",
    "        top_books = average_ratings.sort_values(['score'], ascending=False)[:self.init_subset]\n",
    "        if (len(self.langs) > 0): books = books[(books.language.isin(self.langs))]\n",
    "        top_books = self._get_top_books(average_ratings, books, 'score')\n",
    "        top_books = top_books[['Book_Title', 'Book_Author', 'mean', 'count', 'score']]\n",
    "        return top_books.sort_values(['score'], ascending=False)[:self.top_k]\n",
    "    \n",
    "    def diversity_score(self, model):\n",
    "        interactions_top = ratings[ratings.ISBN.isin(list(model.ISBN))].pivot(index='User_ID', columns='ISBN')['Book_Rating']\n",
    "        mean_corr = 0\n",
    "        for book in list(model.ISBN):\n",
    "            mean_corr += interactions_top.corrwith(interactions_top[book]).mean()    \n",
    "        mean_corr /= len(list(model.ISBN))\n",
    "        print(1 - mean_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NonPersonalizedRecommendation(rating_limit=10, top_k=10, langs=['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>std</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1888054557</td>\n",
       "      <td>Postmarked Yesteryear: 30 Rare Holiday Postcards</td>\n",
       "      <td>Pamela E. Apkarian-Russell</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Collectors Press</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64267</th>\n",
       "      <td>9.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>0.277350</td>\n",
       "      <td>0836213319</td>\n",
       "      <td>Dilbert: A Book of Postcards</td>\n",
       "      <td>Scott Adams</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Andrews McMeel Pub</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79370</th>\n",
       "      <td>9.869565</td>\n",
       "      <td>23</td>\n",
       "      <td>0.344350</td>\n",
       "      <td>0439425220</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets Postca...</td>\n",
       "      <td>J. K. Rowling</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10038</th>\n",
       "      <td>9.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>0.425815</td>\n",
       "      <td>0394800389</td>\n",
       "      <td>Fox in Socks (I Can Read It All by Myself Begi...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>Random House Children's Books</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>9.750000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.638666</td>\n",
       "      <td>0060256656</td>\n",
       "      <td>The Giving Tree</td>\n",
       "      <td>Shel Silverstein</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>9.727273</td>\n",
       "      <td>11</td>\n",
       "      <td>0.646670</td>\n",
       "      <td>0312099045</td>\n",
       "      <td>Route 66 Postcards: Greetings from the Mother ...</td>\n",
       "      <td>Michael Wallis</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>St. Martin's Press</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12762</th>\n",
       "      <td>9.720000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.613732</td>\n",
       "      <td>0618002235</td>\n",
       "      <td>The Two Towers (The Lord of the Rings, Part 2)</td>\n",
       "      <td>J. R. R. Tolkien</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Houghton Mifflin Company</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12761</th>\n",
       "      <td>9.625000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.718795</td>\n",
       "      <td>0618002243</td>\n",
       "      <td>The Return of the King (The Lord of The Rings,...</td>\n",
       "      <td>J. R. R. Tolkien</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Houghton Mifflin Company</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4066</th>\n",
       "      <td>9.600000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.680557</td>\n",
       "      <td>0836218221</td>\n",
       "      <td>The Authoritative Calvin and Hobbes (Calvin an...</td>\n",
       "      <td>Bill Watterson</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>Andrews McMeel Publishing</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15187</th>\n",
       "      <td>9.583333</td>\n",
       "      <td>24</td>\n",
       "      <td>0.717282</td>\n",
       "      <td>0836220889</td>\n",
       "      <td>Calvin and Hobbes</td>\n",
       "      <td>Bill Watterson</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>Andrews McMeel Publishing</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean  count       std        ISBN  \\\n",
       "5871   10.000000     11  0.000000  1888054557   \n",
       "64267   9.923077     13  0.277350  0836213319   \n",
       "79370   9.869565     23  0.344350  0439425220   \n",
       "10038   9.785714     14  0.425815  0394800389   \n",
       "13300   9.750000     20  0.638666  0060256656   \n",
       "5872    9.727273     11  0.646670  0312099045   \n",
       "12762   9.720000     25  0.613732  0618002235   \n",
       "12761   9.625000     16  0.718795  0618002243   \n",
       "4066    9.600000     20  0.680557  0836218221   \n",
       "15187   9.583333     24  0.717282  0836220889   \n",
       "\n",
       "                                              Book_Title  \\\n",
       "5871    Postmarked Yesteryear: 30 Rare Holiday Postcards   \n",
       "64267                       Dilbert: A Book of Postcards   \n",
       "79370  Harry Potter and the Chamber of Secrets Postca...   \n",
       "10038  Fox in Socks (I Can Read It All by Myself Begi...   \n",
       "13300                                    The Giving Tree   \n",
       "5872   Route 66 Postcards: Greetings from the Mother ...   \n",
       "12762     The Two Towers (The Lord of the Rings, Part 2)   \n",
       "12761  The Return of the King (The Lord of The Rings,...   \n",
       "4066   The Authoritative Calvin and Hobbes (Calvin an...   \n",
       "15187                                  Calvin and Hobbes   \n",
       "\n",
       "                      Book_Author  Year_Of_Publication  \\\n",
       "5871   Pamela E. Apkarian-Russell               2001.0   \n",
       "64267                 Scott Adams               1996.0   \n",
       "79370               J. K. Rowling               2002.0   \n",
       "10038                   Dr. Seuss               1965.0   \n",
       "13300            Shel Silverstein               1964.0   \n",
       "5872               Michael Wallis               1993.0   \n",
       "12762            J. R. R. Tolkien               1999.0   \n",
       "12761            J. R. R. Tolkien               1999.0   \n",
       "4066               Bill Watterson               1990.0   \n",
       "15187              Bill Watterson               1987.0   \n",
       "\n",
       "                           Publisher language  \n",
       "5871                Collectors Press       en  \n",
       "64267             Andrews McMeel Pub       en  \n",
       "79370                     Scholastic       en  \n",
       "10038  Random House Children's Books       en  \n",
       "13300       HarperCollins Publishers       en  \n",
       "5872              St. Martin's Press       en  \n",
       "12762       Houghton Mifflin Company       en  \n",
       "12761       Houghton Mifflin Company       en  \n",
       "4066       Andrews McMeel Publishing       en  \n",
       "15187      Andrews McMeel Publishing       en  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_N_mean_model = model.simple_recommendation(ratings, books)\n",
    "top_N_mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>Postmarked Yesteryear: 30 Rare Holiday Postcards</td>\n",
       "      <td>Pamela E. Apkarian-Russell</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64267</th>\n",
       "      <td>Dilbert: A Book of Postcards</td>\n",
       "      <td>Scott Adams</td>\n",
       "      <td>9.923077</td>\n",
       "      <td>13</td>\n",
       "      <td>9.755476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79370</th>\n",
       "      <td>Harry Potter and the Chamber of Secrets Postca...</td>\n",
       "      <td>J. K. Rowling</td>\n",
       "      <td>9.869565</td>\n",
       "      <td>23</td>\n",
       "      <td>9.720657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10038</th>\n",
       "      <td>Fox in Socks (I Can Read It All by Myself Begi...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>9.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>9.539856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12762</th>\n",
       "      <td>The Two Towers (The Lord of the Rings, Part 2)</td>\n",
       "      <td>J. R. R. Tolkien</td>\n",
       "      <td>9.720000</td>\n",
       "      <td>25</td>\n",
       "      <td>9.466664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>The Giving Tree</td>\n",
       "      <td>Shel Silverstein</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>20</td>\n",
       "      <td>9.451095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30743</th>\n",
       "      <td>My Sister's Keeper : A Novel (Picoult, Jodi)</td>\n",
       "      <td>Jodi Picoult</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>22</td>\n",
       "      <td>9.319490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>Route 66 Postcards: Greetings from the Mother ...</td>\n",
       "      <td>Michael Wallis</td>\n",
       "      <td>9.727273</td>\n",
       "      <td>11</td>\n",
       "      <td>9.292834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4066</th>\n",
       "      <td>The Authoritative Calvin and Hobbes (Calvin an...</td>\n",
       "      <td>Bill Watterson</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>20</td>\n",
       "      <td>9.281489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15187</th>\n",
       "      <td>Calvin and Hobbes</td>\n",
       "      <td>Bill Watterson</td>\n",
       "      <td>9.583333</td>\n",
       "      <td>24</td>\n",
       "      <td>9.280452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Book_Title  \\\n",
       "5871    Postmarked Yesteryear: 30 Rare Holiday Postcards   \n",
       "64267                       Dilbert: A Book of Postcards   \n",
       "79370  Harry Potter and the Chamber of Secrets Postca...   \n",
       "10038  Fox in Socks (I Can Read It All by Myself Begi...   \n",
       "12762     The Two Towers (The Lord of the Rings, Part 2)   \n",
       "13300                                    The Giving Tree   \n",
       "30743       My Sister's Keeper : A Novel (Picoult, Jodi)   \n",
       "5872   Route 66 Postcards: Greetings from the Mother ...   \n",
       "4066   The Authoritative Calvin and Hobbes (Calvin an...   \n",
       "15187                                  Calvin and Hobbes   \n",
       "\n",
       "                      Book_Author       mean  count      score  \n",
       "5871   Pamela E. Apkarian-Russell  10.000000     11  10.000000  \n",
       "64267                 Scott Adams   9.923077     13   9.755476  \n",
       "79370               J. K. Rowling   9.869565     23   9.720657  \n",
       "10038                   Dr. Seuss   9.785714     14   9.539856  \n",
       "12762            J. R. R. Tolkien   9.720000     25   9.466664  \n",
       "13300            Shel Silverstein   9.750000     20   9.451095  \n",
       "30743                Jodi Picoult   9.545455     22   9.319490  \n",
       "5872               Michael Wallis   9.727273     11   9.292834  \n",
       "4066               Bill Watterson   9.600000     20   9.281489  \n",
       "15187              Bill Watterson   9.583333     24   9.280452  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.confidence_recommendation(ratings, books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ISBN</th>\n",
       "      <th>9022906116</th>\n",
       "      <th>0000000000</th>\n",
       "      <th>00000000000</th>\n",
       "      <th>0000913154</th>\n",
       "      <th>0001046438</th>\n",
       "      <th>000104687X</th>\n",
       "      <th>0001047213</th>\n",
       "      <th>0001047973</th>\n",
       "      <th>0001048082</th>\n",
       "      <th>000105337X</th>\n",
       "      <th>...</th>\n",
       "      <th>O425126064</th>\n",
       "      <th>O425155404</th>\n",
       "      <th>O439060737</th>\n",
       "      <th>O446611638</th>\n",
       "      <th>O590418262</th>\n",
       "      <th>O9088446X</th>\n",
       "      <th>X000000000</th>\n",
       "      <th>ZR903CX0003</th>\n",
       "      <th>\\0432534220\\\"\"</th>\n",
       "      <th>\\2842053052\\\"\"</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11676</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16634</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16795</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ISBN      9022906116  0000000000  00000000000  0000913154  0001046438  \\\n",
       "User_ID                                                                 \n",
       "3757             NaN         NaN          NaN         NaN         NaN   \n",
       "7346             NaN         NaN          NaN         NaN         NaN   \n",
       "11676            7.0         9.0          8.0         NaN         NaN   \n",
       "16634            NaN         NaN          NaN         NaN         NaN   \n",
       "16795            NaN         NaN          NaN         NaN         NaN   \n",
       "\n",
       "ISBN     000104687X  0001047213  0001047973  0001048082  000105337X  \\\n",
       "User_ID                                                               \n",
       "3757            NaN         NaN         NaN         NaN         NaN   \n",
       "7346            NaN         NaN         NaN         NaN         NaN   \n",
       "11676           NaN         NaN         NaN         NaN         NaN   \n",
       "16634           NaN         NaN         NaN         NaN         NaN   \n",
       "16795           NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "ISBN          ...        O425126064  O425155404  O439060737  O446611638  \\\n",
       "User_ID       ...                                                         \n",
       "3757          ...               NaN         NaN         NaN         NaN   \n",
       "7346          ...               NaN         NaN         NaN         NaN   \n",
       "11676         ...               8.0         4.0        10.0         7.0   \n",
       "16634         ...               NaN         NaN         NaN         NaN   \n",
       "16795         ...               NaN         NaN         NaN         NaN   \n",
       "\n",
       "ISBN     O590418262  O9088446X  X000000000  ZR903CX0003  \\0432534220\\\"\"  \\\n",
       "User_ID                                                                   \n",
       "3757            NaN        NaN         NaN          NaN             NaN   \n",
       "7346            NaN        NaN         NaN          NaN             NaN   \n",
       "11676           NaN        8.0        10.0          1.0             6.0   \n",
       "16634           NaN        NaN         NaN          NaN             NaN   \n",
       "16795           NaN        NaN         NaN          NaN             NaN   \n",
       "\n",
       "ISBN     \\2842053052\\\"\"  \n",
       "User_ID                  \n",
       "3757                NaN  \n",
       "7346                NaN  \n",
       "11676               7.0  \n",
       "16634               NaN  \n",
       "16795               NaN  \n",
       "\n",
       "[5 rows x 42635 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar movie \n",
    "counts_users = ratings['User_ID'].value_counts()\n",
    "ratings_subset = ratings[ratings['User_ID'].isin(counts_users[counts_users >= 300].index)]\n",
    "counts_ratings = ratings['Book_Rating'].value_counts()\n",
    "ratings_subset = ratings_subset[ratings['Book_Rating'].isin(counts_ratings[counts_ratings >= 200].index)]\n",
    "book_ratings = ratings_subset.pivot_table(index='User_ID', columns='ISBN', values='Book_Rating')\n",
    "book_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISBN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0060928336</th>\n",
       "      <td>7.887500</td>\n",
       "      <td>320</td>\n",
       "      <td>1.634224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0142001740</th>\n",
       "      <td>8.452769</td>\n",
       "      <td>307</td>\n",
       "      <td>1.488590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0312195516</th>\n",
       "      <td>8.182768</td>\n",
       "      <td>383</td>\n",
       "      <td>1.694777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0316666343</th>\n",
       "      <td>8.185290</td>\n",
       "      <td>707</td>\n",
       "      <td>1.529098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0385504209</th>\n",
       "      <td>8.435318</td>\n",
       "      <td>487</td>\n",
       "      <td>1.668957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean  count       std\n",
       "ISBN                                 \n",
       "0060928336  7.887500    320  1.634224\n",
       "0142001740  8.452769    307  1.488590\n",
       "0312195516  8.182768    383  1.694777\n",
       "0316666343  8.185290    707  1.529098\n",
       "0385504209  8.435318    487  1.668957"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_ratings = ratings.groupby('ISBN')['Book_Rating'].agg(['mean', 'count', 'std'])\n",
    "average_ratings = average_ratings[average_ratings['count'] >= 300]\n",
    "average_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corr</th>\n",
       "      <th>count</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>487.0</td>\n",
       "      <td>0385504209</td>\n",
       "      <td>The Da Vinci Code</td>\n",
       "      <td>Dan Brown</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0142001740</td>\n",
       "      <td>The Secret Life of Bees</td>\n",
       "      <td>Sue Monk Kidd</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Penguin Books</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.244542</td>\n",
       "      <td>707.0</td>\n",
       "      <td>0316666343</td>\n",
       "      <td>The Lovely Bones: A Novel</td>\n",
       "      <td>Alice Sebold</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Little, Brown</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>581.0</td>\n",
       "      <td>0971880107</td>\n",
       "      <td>Wild Animus</td>\n",
       "      <td>Rich Shapero</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Too Far</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>383.0</td>\n",
       "      <td>0312195516</td>\n",
       "      <td>The Red Tent (Bestselling Backlist)</td>\n",
       "      <td>Anita Diamant</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Picador USA</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>313.0</td>\n",
       "      <td>059035342X</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>J. K. Rowling</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Arthur A. Levine Books</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          corr  count        ISBN  \\\n",
       "748   1.000000  487.0  0385504209   \n",
       "356   1.000000  307.0  0142001740   \n",
       "408   0.244542  707.0  0316666343   \n",
       "26   -1.000000  581.0  0971880107   \n",
       "522  -1.000000  383.0  0312195516   \n",
       "2143 -1.000000  313.0  059035342X   \n",
       "\n",
       "                                             Book_Title    Book_Author  \\\n",
       "748                                   The Da Vinci Code      Dan Brown   \n",
       "356                             The Secret Life of Bees  Sue Monk Kidd   \n",
       "408                           The Lovely Bones: A Novel   Alice Sebold   \n",
       "26                                          Wild Animus   Rich Shapero   \n",
       "522                 The Red Tent (Bestselling Backlist)  Anita Diamant   \n",
       "2143  Harry Potter and the Sorcerer's Stone (Harry P...  J. K. Rowling   \n",
       "\n",
       "      Year_Of_Publication               Publisher language  \n",
       "748                2003.0               Doubleday       en  \n",
       "356                2003.0           Penguin Books       en  \n",
       "408                2002.0           Little, Brown       en  \n",
       "26                 2004.0                 Too Far       et  \n",
       "522                1998.0             Picador USA       en  \n",
       "2143               1999.0  Arthur A. Levine Books       en  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple item based collaborative filtering \n",
    "# The Da Vinci Code 0385504209 for example \n",
    "sample_book = book_ratings['0385504209']\n",
    "similar_books = pd.DataFrame(book_ratings.corrwith(sample_book), columns=['corr'])\n",
    "similar_books = similar_books.dropna()\n",
    "similar_books = similar_books.sort_values('corr', ascending=False)\n",
    "similar_books = similar_books.join(average_ratings['count'])\n",
    "similar_books[similar_books['count'] >= 300].sort_values('corr', ascending=False).head(10).merge(books, left_index=True, right_on='ISBN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book_ratings.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing book 0452283205\n",
      "computing book 3423242345\n",
      "computing book 3442442354\n",
      "computing book 3442449898\n",
      "computing book 345319988X\n",
      "computing book 3492227694\n",
      "computing book 3492229905\n",
      "computing book 3499233746\n",
      "computing book 3548255558\n",
      "computing book 3596144868\n",
      "sorting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46526</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Starting Out in the Evening</td>\n",
       "      <td>0425168697</td>\n",
       "      <td>Brian Morton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13683</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Secret Prey</td>\n",
       "      <td>0425168298</td>\n",
       "      <td>John Sandford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5478</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Sudden Mischief</td>\n",
       "      <td>042516828X</td>\n",
       "      <td>Robert B. Parker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13876</th>\n",
       "      <td>0.0</td>\n",
       "      <td>State of Siege (Tom Clancy's Op-Center, 6)</td>\n",
       "      <td>0425168220</td>\n",
       "      <td>Tom Clancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26806</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Conspiracy in Death</td>\n",
       "      <td>0425168131</td>\n",
       "      <td>J. D. Robb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Deadbeat (P. I. Mysteries)</td>\n",
       "      <td>042516781X</td>\n",
       "      <td>Leo Atkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>0.0</td>\n",
       "      <td>An Instance of the Fingerpost</td>\n",
       "      <td>0425167720</td>\n",
       "      <td>Iain Pears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175328</th>\n",
       "      <td>0.0</td>\n",
       "      <td>No Place for Memories</td>\n",
       "      <td>0425167364</td>\n",
       "      <td>Sherry Lewis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Here on Earth</td>\n",
       "      <td>0425167313</td>\n",
       "      <td>Alice Hoffman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116694</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Election: A Novel</td>\n",
       "      <td>0425167283</td>\n",
       "      <td>Tom Perrotta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0                                  Book_Title        ISBN  \\\n",
       "46526   0.0                 Starting Out in the Evening  0425168697   \n",
       "13683   0.0                                 Secret Prey  0425168298   \n",
       "5478    0.0                             Sudden Mischief  042516828X   \n",
       "13876   0.0  State of Siege (Tom Clancy's Op-Center, 6)  0425168220   \n",
       "26806   0.0                         Conspiracy in Death  0425168131   \n",
       "217048  0.0                  Deadbeat (P. I. Mysteries)  042516781X   \n",
       "1959    0.0               An Instance of the Fingerpost  0425167720   \n",
       "175328  0.0                       No Place for Memories  0425167364   \n",
       "1945    0.0                               Here on Earth  0425167313   \n",
       "116694  0.0                           Election: A Novel  0425167283   \n",
       "\n",
       "             Book_Author  \n",
       "46526       Brian Morton  \n",
       "13683      John Sandford  \n",
       "5478    Robert B. Parker  \n",
       "13876         Tom Clancy  \n",
       "26806         J. D. Robb  \n",
       "217048        Leo Atkins  \n",
       "1959          Iain Pears  \n",
       "175328      Sherry Lewis  \n",
       "1945       Alice Hoffman  \n",
       "116694      Tom Perrotta  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ideally I would save full correlation matrix to make this run faster\n",
    "sims_candidates = pd.Series()\n",
    "# sample_user = users.sample()\n",
    "sample_user = 16867\n",
    "user_ratings = ratings[ratings.User_ID == sample_user]\n",
    "for book, rating in zip(user_ratings.ISBN, user_ratings.Book_Rating):\n",
    "    print('computing book', book)\n",
    "    # compute similarity scores\n",
    "    try:\n",
    "        sims = book_ratings.corrwith(book_ratings[book], drop=True)\n",
    "        # scale score by how well it is rated\n",
    "        sims = sims.map(lambda x: (x + 1e-04) * rating)\n",
    "        # append to series\n",
    "        sims_candidates = sims_candidates.append(sims)\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "print('sorting')\n",
    "sims_candidates = sims_candidates.groupby(sims_candidates.index).sum()\n",
    "sims_candidates.sort_values(inplace=True, ascending=False)\n",
    "# sims_candidates.drop([i for i in list(user_ratings.ISBN.values) if i in book_ratings.columns])\n",
    "sims_candidates = pd.DataFrame(sims_candidates).merge(books[['Book_Title', 'ISBN', 'Book_Author']], left_index=True, right_on='ISBN')\n",
    "sims_candidates.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Rating</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16867</td>\n",
       "      <td>0140622373</td>\n",
       "      <td>0</td>\n",
       "      <td>A Little Princess (Penguin Popular Classics)</td>\n",
       "      <td>Frances Hodgson Burnett</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Penguin Books Ltd</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16867</td>\n",
       "      <td>0345337662</td>\n",
       "      <td>0</td>\n",
       "      <td>Interview with the Vampire</td>\n",
       "      <td>Anne Rice</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>Ballantine Books</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16867</td>\n",
       "      <td>0452283205</td>\n",
       "      <td>8</td>\n",
       "      <td>Falling Angels</td>\n",
       "      <td>Tracy Chevalier</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Plume Books</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16867</td>\n",
       "      <td>2253142743</td>\n",
       "      <td>0</td>\n",
       "      <td>Simisola</td>\n",
       "      <td>Ruth Rendell</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>LGF</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16867</td>\n",
       "      <td>3401020919</td>\n",
       "      <td>0</td>\n",
       "      <td>Sara in Avonlea. Die Ankunft / Marillas Geheim...</td>\n",
       "      <td>Lucy Maud. Montgomery</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Arena</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16867</td>\n",
       "      <td>3404202635</td>\n",
       "      <td>0</td>\n",
       "      <td>Die Pandemia- Saga I. Der Weg nach Kinvale. Fa...</td>\n",
       "      <td>Dave Duncan</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>LÃ?Â¼bbe</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16867</td>\n",
       "      <td>3423242345</td>\n",
       "      <td>7</td>\n",
       "      <td>Vor der ElfendÃ?Â¤mmerung.</td>\n",
       "      <td>Jean-Louis Fetjaine</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Dtv</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16867</td>\n",
       "      <td>3423704519</td>\n",
       "      <td>0</td>\n",
       "      <td>Pu der BÃ?Â¤r. ( Ab 8 J.).</td>\n",
       "      <td>Alan Alexander Milne</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Dtv</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16867</td>\n",
       "      <td>3442442354</td>\n",
       "      <td>10</td>\n",
       "      <td>Der Strand</td>\n",
       "      <td>Alex Garland</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Wilhelm Goldmann Verlag, GmbH</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16867</td>\n",
       "      <td>3442446759</td>\n",
       "      <td>0</td>\n",
       "      <td>Lords und Ladies. Ein Roman von der bizarren S...</td>\n",
       "      <td>Terry Pratchett</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Goldmann</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16867</td>\n",
       "      <td>3442449898</td>\n",
       "      <td>7</td>\n",
       "      <td>Manila.</td>\n",
       "      <td>Alex Garland</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Goldmann</td>\n",
       "      <td>sw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16867</td>\n",
       "      <td>345319988X</td>\n",
       "      <td>10</td>\n",
       "      <td>Unten am Fluss. Watership Down.</td>\n",
       "      <td>Richard Adams</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Heyne</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16867</td>\n",
       "      <td>3492227694</td>\n",
       "      <td>10</td>\n",
       "      <td>Mariana.</td>\n",
       "      <td>Susanna Kearsley</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Piper</td>\n",
       "      <td>sw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16867</td>\n",
       "      <td>3492229905</td>\n",
       "      <td>9</td>\n",
       "      <td>Elisabeth: Kaiserin wider Willen (Serie Piper)</td>\n",
       "      <td>Brigitte Hamann</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Piper</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16867</td>\n",
       "      <td>3548255558</td>\n",
       "      <td>9</td>\n",
       "      <td>Das Blut des Teufels. Roman.</td>\n",
       "      <td>James Rollins</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Ullstein Tb</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16867</td>\n",
       "      <td>3596144868</td>\n",
       "      <td>8</td>\n",
       "      <td>Hannas Tochter</td>\n",
       "      <td>Mariann Fredriksson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fischer Taschenbuch Verlag</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User_ID        ISBN  Book_Rating  \\\n",
       "0     16867  0140622373            0   \n",
       "1     16867  0345337662            0   \n",
       "2     16867  0452283205            8   \n",
       "3     16867  2253142743            0   \n",
       "4     16867  3401020919            0   \n",
       "5     16867  3404202635            0   \n",
       "6     16867  3423242345            7   \n",
       "7     16867  3423704519            0   \n",
       "8     16867  3442442354           10   \n",
       "9     16867  3442446759            0   \n",
       "10    16867  3442449898            7   \n",
       "11    16867  345319988X           10   \n",
       "12    16867  3492227694           10   \n",
       "13    16867  3492229905            9   \n",
       "14    16867  3548255558            9   \n",
       "15    16867  3596144868            8   \n",
       "\n",
       "                                           Book_Title  \\\n",
       "0        A Little Princess (Penguin Popular Classics)   \n",
       "1                          Interview with the Vampire   \n",
       "2                                      Falling Angels   \n",
       "3                                            Simisola   \n",
       "4   Sara in Avonlea. Die Ankunft / Marillas Geheim...   \n",
       "5   Die Pandemia- Saga I. Der Weg nach Kinvale. Fa...   \n",
       "6                          Vor der ElfendÃ?Â¤mmerung.   \n",
       "7                          Pu der BÃ?Â¤r. ( Ab 8 J.).   \n",
       "8                                          Der Strand   \n",
       "9   Lords und Ladies. Ein Roman von der bizarren S...   \n",
       "10                                            Manila.   \n",
       "11                    Unten am Fluss. Watership Down.   \n",
       "12                                           Mariana.   \n",
       "13     Elisabeth: Kaiserin wider Willen (Serie Piper)   \n",
       "14                       Das Blut des Teufels. Roman.   \n",
       "15                                     Hannas Tochter   \n",
       "\n",
       "                Book_Author  Year_Of_Publication  \\\n",
       "0   Frances Hodgson Burnett               1996.0   \n",
       "1                 Anne Rice               1993.0   \n",
       "2           Tracy Chevalier               2002.0   \n",
       "3              Ruth Rendell               1997.0   \n",
       "4     Lucy Maud. Montgomery               1999.0   \n",
       "5               Dave Duncan               1995.0   \n",
       "6       Jean-Louis Fetjaine               2001.0   \n",
       "7      Alan Alexander Milne               1997.0   \n",
       "8              Alex Garland               2000.0   \n",
       "9           Terry Pratchett               2000.0   \n",
       "10             Alex Garland               2001.0   \n",
       "11            Richard Adams               2002.0   \n",
       "12         Susanna Kearsley               1999.0   \n",
       "13          Brigitte Hamann               1998.0   \n",
       "14            James Rollins               2003.0   \n",
       "15      Mariann Fredriksson                  NaN   \n",
       "\n",
       "                        Publisher language  \n",
       "0               Penguin Books Ltd       en  \n",
       "1                Ballantine Books       en  \n",
       "2                     Plume Books       tl  \n",
       "3                             LGF       fi  \n",
       "4                           Arena       de  \n",
       "5                        LÃ?Â¼bbe       de  \n",
       "6                             Dtv       de  \n",
       "7                             Dtv       de  \n",
       "8   Wilhelm Goldmann Verlag, GmbH       de  \n",
       "9                        Goldmann       de  \n",
       "10                       Goldmann       sw  \n",
       "11                          Heyne       en  \n",
       "12                          Piper       sw  \n",
       "13                          Piper       de  \n",
       "14                    Ullstein Tb       de  \n",
       "15     Fischer Taschenbuch Verlag       de  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings.merge(books, on='ISBN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "from surprise import PredictionImpossible\n",
    "from BookData import BookDataSet\n",
    "import math\n",
    "import numpy as np\n",
    "import heapq\n",
    "from scipy.sparse import lil_matrix, csr_matrix, save_npz, load_npz\n",
    "\n",
    "class ContentKNNAlgorithm(AlgoBase):\n",
    "\n",
    "    def __init__(self, k=40, sim_options={}):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.k = k\n",
    "        self.books = books\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        # Compute item similarity matrix based on content attributes\n",
    "\n",
    "        # Load up genre vectors for every movie\n",
    "        ml = BookDataSet(ratings, self.books, users)\n",
    "#         genres = ml.getGenres()\n",
    "        self.books = prepare_data(books, ratings)\n",
    "        count_matrix = compute_vect(self.books)\n",
    "        years = ml.get_year()\n",
    "        \n",
    "        print(\"Computing content-based similarity matrix...\")\n",
    "            \n",
    "        # Compute genre distance for every movie combination as a 2x2 matrix\n",
    "#         self.similarities = np.zeros((self.trainset.n_items, self.trainset.n_items))\n",
    "        self.similarities = lil_matrix((self.trainset.n_items, self.trainset.n_items))\n",
    "#         for i in range(self.trainset.n_items):\n",
    "#             print(i, self.trainset.to_raw_iid(i))\n",
    "        for thisRating in range(self.trainset.n_items):\n",
    "            if (thisRating % 100 == 0):\n",
    "                print(thisRating, \" of \", self.trainset.n_items)\n",
    "            for otherRating in range(thisRating+1, self.trainset.n_items):\n",
    "                try:\n",
    "                    thisMovieID = self.trainset.to_raw_iid(thisRating)\n",
    "                    otherMovieID = self.trainset.to_raw_iid(otherRating)\n",
    "#                     print(thisRating, otherRating, thisMovieID, otherMovieID)\n",
    "                    thisMovieID_index = self.books.loc[self.books.ISBN == str(thisMovieID)].index[0]\n",
    "                    otherMovieID_index = self.books.loc[self.books.ISBN == str(otherMovieID)].index[0]\n",
    "                    genreSimilarity = self.computeGenreSimilarity(thisMovieID_index, otherMovieID_index, count_matrix)\n",
    "                    yearSimilarity = self.computeYearSimilarity(thisMovieID, otherMovieID, years)\n",
    "                except:\n",
    "                    genreSimilarity, yearSimilarity = 0, 0\n",
    "#                 print('FFF',genreSimilarity, yearSimilarity)\n",
    "                #mesSimilarity = self.computeMiseEnSceneSimilarity(thisMovieID, otherMovieID, mes)\n",
    "                self.similarities[thisRating, otherRating] = genreSimilarity * yearSimilarity\n",
    "                self.similarities[otherRating, thisRating] = self.similarities[thisRating, otherRating]\n",
    "                \n",
    "        print(\"...done.\")\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def computeGenreSimilarity(self, movie1, movie2, count_matrix):\n",
    "        genres1 = np.array(count_matrix[movie1].todense())[0]\n",
    "        genres2 = np.array(count_matrix[movie2].todense())[0]\n",
    "#         print(genres1.shape, genres2.shape)\n",
    "        sumxx, sumxy, sumyy = 0, 0, 0\n",
    "        for i in range(len(genres1)):\n",
    "            x = genres1[i]\n",
    "            y = genres2[i]\n",
    "            sumxx += x * x\n",
    "            sumyy += y * y\n",
    "            sumxy += x * y\n",
    "        \n",
    "        return sumxy/math.sqrt(sumxx*sumyy)\n",
    "    \n",
    "#     def computeGenreSimilarity(self, movie1, movie2, genres):\n",
    "#         genres1 = genres[movie1]\n",
    "#         genres2 = genres[movie2]\n",
    "#         sumxx, sumxy, sumyy = 0, 0, 0\n",
    "#         for i in range(len(genres1)):\n",
    "#             x = genres1[i]\n",
    "#             y = genres2[i]\n",
    "#             sumxx += x * x\n",
    "#             sumyy += y * y\n",
    "#             sumxy += x * y\n",
    "        \n",
    "#         return sumxy/math.sqrt(sumxx*sumyy)\n",
    "    \n",
    "    def computeYearSimilarity(self, movie1, movie2, years):\n",
    "        diff = abs(years[movie1] - years[movie2])\n",
    "        sim = math.exp(-diff / 10.0)\n",
    "        return sim\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unkown.')\n",
    "        \n",
    "        # Build up similarity scores between this item and everything the user rated\n",
    "        neighbors = []\n",
    "        for rating in self.trainset.ur[u]:\n",
    "            genreSimilarity = self.similarities[i,rating[0]]\n",
    "            neighbors.append( (genreSimilarity, rating[1]) )\n",
    "        \n",
    "        # Extract the top-K most-similar ratings\n",
    "        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[0])\n",
    "        \n",
    "        # Compute average sim score of K neighbors weighted by user ratings\n",
    "        simTotal = weightedSum = 0\n",
    "        for (simScore, rating) in k_neighbors:\n",
    "            if (simScore > 0):\n",
    "                simTotal += simScore\n",
    "                weightedSum += simScore * rating\n",
    "            \n",
    "        if (simTotal == 0):\n",
    "            raise PredictionImpossible('No neighbors')\n",
    "\n",
    "        predictedRating = weightedSum / simTotal\n",
    "\n",
    "        return predictedRating\n",
    "    \n",
    "    \n",
    "    \n",
    "knn = ContentKNNAlgorithm()\n",
    "\n",
    "# prepared_books = prepare_data(books, sample_ratings)\n",
    "data = BookDataSet(ratings, books, users)\n",
    "rankings = data.get_popularity_ranks()\n",
    "evaluator = Evaluator(data, rankings)\n",
    "\n",
    "# SVD\n",
    "knn = ContentKNNAlgorithm()\n",
    "evaluator.add_model(knn, 'knn')\n",
    "\n",
    "# get RMSE and MAE scores \n",
    "evaluator.evaluate(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering -- User based k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24036, 3)\n"
     ]
    }
   ],
   "source": [
    "counts_users = ratings.User_ID.value_counts()\n",
    "counts_ratings = ratings.Book_Rating.value_counts()\n",
    "sample_ratings = ratings[ratings['User_ID'].isin(counts_users[counts_users >= 200].index)]\n",
    "sample_ratings = sample_ratings[ratings['Book_Rating'].isin(counts_ratings[counts_ratings >= 200].index)]\n",
    "isbn_group = sample_ratings.groupby('ISBN', as_index=False)['Book_Rating'].count()\n",
    "sample_ratings = sample_ratings[sample_ratings.ISBN.isin(list(isbn_group[isbn_group.Book_Rating > 1].ISBN.values))]\n",
    "print(sample_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ISBN</th>\n",
       "      <th>0001056107</th>\n",
       "      <th>0002570122</th>\n",
       "      <th>0006157629</th>\n",
       "      <th>000624565X</th>\n",
       "      <th>0006379702</th>\n",
       "      <th>0006472427</th>\n",
       "      <th>000648302X</th>\n",
       "      <th>0006496423</th>\n",
       "      <th>000649840X</th>\n",
       "      <th>0006498744</th>\n",
       "      <th>...</th>\n",
       "      <th>8485224752</th>\n",
       "      <th>8486433525</th>\n",
       "      <th>8489669635</th>\n",
       "      <th>849550121X</th>\n",
       "      <th>8496246620</th>\n",
       "      <th>9500700891</th>\n",
       "      <th>9500723549</th>\n",
       "      <th>9536000444</th>\n",
       "      <th>9706612084</th>\n",
       "      <th>O67174142X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8621 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ISBN     0001056107  0002570122  0006157629  000624565X  0006379702  \\\n",
       "User_ID                                                               \n",
       "2276            0.0         0.0         0.0         0.0         0.0   \n",
       "3757            0.0         0.0         0.0         0.0         0.0   \n",
       "4385            0.0         0.0         0.0         0.0         0.0   \n",
       "6242            0.0         0.0         0.0         0.0         0.0   \n",
       "6251            0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "ISBN     0006472427  000648302X  0006496423  000649840X  0006498744  \\\n",
       "User_ID                                                               \n",
       "2276            0.0         0.0         0.0         0.0         0.0   \n",
       "3757            0.0         0.0         0.0         0.0         0.0   \n",
       "4385            0.0         0.0         0.0         0.0         0.0   \n",
       "6242            0.0         0.0         0.0         0.0         0.0   \n",
       "6251            0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "ISBN        ...      8485224752  8486433525  8489669635  849550121X  \\\n",
       "User_ID     ...                                                       \n",
       "2276        ...             0.0         0.0         0.0         0.0   \n",
       "3757        ...             5.0         0.0         0.0         0.0   \n",
       "4385        ...             0.0         0.0         0.0         0.0   \n",
       "6242        ...             0.0         0.0         0.0         0.0   \n",
       "6251        ...             0.0         0.0         0.0         0.0   \n",
       "\n",
       "ISBN     8496246620  9500700891  9500723549  9536000444  9706612084  \\\n",
       "User_ID                                                               \n",
       "2276            0.0         0.0         0.0         0.0         0.0   \n",
       "3757            0.0         0.0         0.0         0.0         0.0   \n",
       "4385            0.0         0.0         0.0         0.0         0.0   \n",
       "6242            0.0         0.0         0.0         0.0         0.0   \n",
       "6251            0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "ISBN     O67174142X  \n",
       "User_ID              \n",
       "2276            0.0  \n",
       "3757            0.0  \n",
       "4385            0.0  \n",
       "6242            0.0  \n",
       "6251            0.0  \n",
       "\n",
       "[5 rows x 8621 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_ratings = sample_ratings.pivot_table(index='User_ID', columns='ISBN', values='Book_Rating').fillna(0)\n",
    "book_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='auto', leaf_size=30, metric='correlation',\n",
       "         metric_params=None, n_jobs=1, n_neighbors=10, p=2, radius=1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# pearson_sim = 1 - pairwise_distances(book_ratings.fillna(0), metric=\"correlation\")\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=10, metric='correlation')\n",
    "knn.fit(book_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx = book_ratings.index.get_loc(6242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies, distances = knn.kneighbors(book_ratings.iloc[user_idx].values[None, :], n_neighbors=10+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class UserCollaborativeFiltering:\n",
    "    def __init__(self, ratings):\n",
    "        self.ratings_interactions = ratings.pivot_table(index='User_ID', columns='ISBN', \n",
    "                                                        values='Book_Rating').fillna(0)\n",
    "        self.ratings_interactions_sparse = csr_matrix(self.ratings_interactions.values)\n",
    "        \n",
    "    def get_neighbors(self, user_id, metric='correlation', k=5, top_k=10):\n",
    "        # create model\n",
    "        knn = NearestNeighbors(n_neighbors=k, metric=metric)\n",
    "        knn.fit(self.ratings_interactions)    \n",
    "        # get user index\n",
    "        user_idx = self.ratings_interactions.index.get_loc(user_id)\n",
    "        # get k nearest neighbors \n",
    "        distances, indices = knn.kneighbors(self.ratings_interactions.iloc[user_idx].values[None, :], n_neighbors=top_k+1)\n",
    "        return indices.reshape(-1), distances\n",
    "    \n",
    "    def get_similarities(self, distances):\n",
    "        return 1 - distances.flatten()\n",
    "    \n",
    "    def show_top(self, user_id):\n",
    "        # can get TOP book from each if desired \n",
    "        distances, indices = indices, distances = self.get_neighbors(user_id)\n",
    "        user_idx = self.ratings_interactions.index.get_loc(user_id)\n",
    "        print(indices[0], user_idx)\n",
    "        print('Recommendations for {0}:\\n'.format(user_id))\n",
    "        for i in range(len(indices)):\n",
    "            if user_idx != indices[i]:\n",
    "                  print('{0}: {1}, with distance of {2}:'.format(i, book_ratings.index[indices[i]], distances.flatten()[i]))\n",
    "\n",
    "\n",
    "    def predict_one(self, user_id, book_id, metric='correlation', k=5, top_k=10):\n",
    "        pred = 0\n",
    "        user_idx = self.ratings_interactions.index.get_loc(user_id)\n",
    "        book_idx = self.ratings_interactions.columns.get_loc(book_id)\n",
    "        indices, distances = self.get_neighbors(user_id)\n",
    "        indices = indices.reshape(-1)\n",
    "        similarities = self.get_similarities(distances)\n",
    "        sum_wt = np.sum(similarities) - 1\n",
    "        # for deviation \n",
    "        mean_rating = self.ratings_interactions.iloc[user_idx].mean()\n",
    "        product = 1\n",
    "        wtd_sum = 0 \n",
    "\n",
    "        for i in range(len(indices)):\n",
    "            if user_idx != indices[i]:\n",
    "                deviation = self.ratings_interactions.iloc[indices[i], book_idx] - self.ratings_interactions.iloc[indices[i]].mean()\n",
    "                product = deviation * (similarities[i])\n",
    "                wtd_sum += product\n",
    "\n",
    "        pred = int(round(mean_rating + (wtd_sum / sum_wt)))\n",
    "        # clip ratings\n",
    "        pred = np.clip(pred, 1, 10)\n",
    "        return pred    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "Recommendations for 6242:\n",
      "\n",
      "1: 6575, with distance of 0.9243172259710751:\n",
      "2: 270713, with distance of 0.954836512919706:\n",
      "3: 89602, with distance of 0.9673411109625561:\n",
      "4: 110934, with distance of 0.973266978871884:\n",
      "5: 95359, with distance of 0.9736528332606148:\n",
      "6: 60244, with distance of 0.9741301592444811:\n",
      "7: 204864, with distance of 0.978974928147456:\n",
      "8: 46398, with distance of 0.9820821673162125:\n",
      "9: 69078, with distance of 0.983018174326214:\n",
      "10: 142524, with distance of 0.9836544091860651:\n"
     ]
    }
   ],
   "source": [
    "user_cf = UserCollaborativeFiltering(sample_ratings)\n",
    "# pred = user_cf.predict_one(user_id=6242, book_id='014028009X')\n",
    "# pred\n",
    "user_cf.show_top(user_id=6242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.92431723, 0.95483651, 0.96734111, 0.97326698,\n",
       "         0.97365283, 0.97413016, 0.97897493, 0.98208217, 0.98301817,\n",
       "         0.98365441]]),\n",
       " array([[  3,   5, 143,  52,  68,  57,  36, 112,  28,  40,  82]]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_interactions_sparse = csr_matrix(book_ratings.values)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=10, metric='correlation')\n",
    "knn.fit(book_ratings)\n",
    "\n",
    "user_idx = book_ratings.index.get_loc(6242)\n",
    "\n",
    "indicies, distances = knn.kneighbors(book_ratings.iloc[user_idx].values[None, :], n_neighbors=10+1)\n",
    "indicies, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-161259674d40>, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-161259674d40>\"\u001b[0;36m, line \u001b[0;32m81\u001b[0m\n\u001b[0;31m    printbook_loader.get_book_name(ratings[0]), ratings[1])\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class EvalMetrics:\n",
    "    \n",
    "    def RMSE(y_pred):\n",
    "        return accuracy.rmse(y_pred, verbose=True)\n",
    "    \n",
    "    def MAE(y_pred):\n",
    "        return accuracy.mae(y_pred, verbose=True)\n",
    "    \n",
    "    def hit_rate(topNPredicted, leftOutPredictions):\n",
    "        hits = 0\n",
    "        N = 0\n",
    "        \n",
    "        for leftOut in leftOutPredictions:\n",
    "            user_id = leftOut[0] \n",
    "            left_out_book_id = leftOut[1] \n",
    "            hit = False\n",
    "            for book_id, prediction in topNPredicted[int(user_id)]:\n",
    "                if left_out_book_id.strip() == book_id.strip():\n",
    "                    hit = True\n",
    "                    break\n",
    "            if hit: hits += 1\n",
    "            N += 1\n",
    "        \n",
    "        return hits / N \n",
    "    \n",
    "    \n",
    "    \n",
    "class Evaluator():\n",
    "    def __init__(self, dataset, rankings):\n",
    "        # create datasets\n",
    "        eval_data = CreateDataSets(dataset, rankings)\n",
    "        self.dataset = eval_data\n",
    "        # hold all models\n",
    "        self.models = []\n",
    "        \n",
    "    def add_model(self, model, name):\n",
    "        clf = EvaluatedModel(model, name)\n",
    "        self.models.append(clf)\n",
    "        \n",
    "    def evaluate(self, topN=False):\n",
    "        results = {}\n",
    "        for model in self.models:\n",
    "            print('running model: {} ....'.format(model.get_name()))\n",
    "            results[model.get_name()] = model.evaluate(self.dataset, topN)\n",
    "            \n",
    "        if (topN):\n",
    "            print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "                    \"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"cHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\"))\n",
    "            for (name, metrics) in results.items():\n",
    "                print(\"{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
    "                        name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"cHR\"], metrics[\"ARHR\"],\n",
    "                                      metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n",
    "        else:\n",
    "            print(\"{:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\"))\n",
    "            for (name, metrics) in results.items():\n",
    "                print(\"{:<10} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"]))\n",
    "        \n",
    "    \n",
    "    def sample_topN(self, book_loader, test_row, top_k=10):\n",
    "        for model in self.models:\n",
    "            print(\"\\nUsing recommender \", model.get_name())\n",
    "            \n",
    "            print(\"\\nBuilding recommendation model...\")\n",
    "            train = self.dataset.get_full_train()\n",
    "            model.get_algorithm().evaluate(train)\n",
    "            \n",
    "            print(\"Computing recommendations...\")\n",
    "            test_set = self.dataset.get_anti_test_for_user(test_row)\n",
    "        \n",
    "            preds = model.get_algorithm().test(test_set)\n",
    "            \n",
    "            recommendations = []\n",
    "            print (\"\\nWe recommend:\")\n",
    "            for userID, bookID, y_true, y_pred, _ in preds:\n",
    "                intbookID = int(bookID)\n",
    "                recommendations.append((intMovieID, y_pred))\n",
    "            \n",
    "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for ratings in recommendations[:top_k]:\n",
    "                printbook_loader.get_book_name(ratings[0]), ratings[1])\n",
    " \n",
    "\n",
    "\n",
    "class EvaluatedModel:\n",
    "    \n",
    "    def __init__(self, algorithm, name):\n",
    "        self.algorithm = algorithm\n",
    "        self.name = name\n",
    "        \n",
    "    def evaluate(self, evaluationData, doTopN, n=10, verbose=True):\n",
    "        metrics = {}\n",
    "        # Compute accuracy\n",
    "        if (verbose):\n",
    "            print(\"Evaluating accuracy...\")\n",
    "        self.model.fit(evaluationData.GetTrainSet())\n",
    "        predictions = self.model.test(evaluationData.get_())\n",
    "        metrics[\"RMSE\"] = EvalMetrics.RMSE(predictions)\n",
    "        metrics[\"MAE\"] = EvalMetrics.MAE(predictions)\n",
    "        \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  Item KNN ...\n",
      "Evaluating accuracy...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Analysis complete.\n",
      "Evaluating  User KNN pearson ...\n",
      "Evaluating accuracy...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Analysis complete.\n",
      "\n",
      "\n",
      "Algorithm  RMSE       MAE       \n",
      "Item KNN   1.5462     1.1423    \n",
      "User KNN pearson 1.9888     1.5526    \n",
      "\n",
      "Using recommender  Item KNN\n",
      "\n",
      "Building recommendation model...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing recommendations...\n",
      "\n",
      "We recommend:\n",
      "Come Along With Me: Part of a Novel, Sixteen Stories, and Three Lectures 10\n",
      "Happy Trails 10\n",
      "Here's to You, Rachel Robinson 10\n",
      "Girls on Film 9.5\n",
      "Election 9.5\n",
      "A Confederacy of Dunces 9.5\n",
      "Summer (Signet Classic) 9.5\n",
      "Love Poems 9.0\n",
      "Foreign Affairs 9.0\n",
      "Essential Rumi 9.0\n",
      "\n",
      "Using recommender  User KNN pearson\n",
      "\n",
      "Building recommendation model...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing recommendations...\n",
      "\n",
      "We recommend:\n",
      "Watership Down 10\n",
      "Dirk Gently's Holistic Detective Agency 10\n",
      "Heaven (Casteel) 10\n",
      "Robot Dreams (Masterworks of Science Fiction and Fantasy, No 5) 10\n",
      "The Wizard of Seattle 10\n",
      "HOW GREEN WAS MY VALLEY 10\n",
      "Sarah, Plain and Tall (Sarah, Plain and Tall) 10\n",
      "Each Peach Pear Plum (Picture Puffins) 10\n",
      "The Time Traveler's Wife (Harvest Book) 10\n",
      "You Are My I Love You 10\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from BookData import BookDataSet\n",
    "from Evaluator import Evaluator\n",
    "from surprise import KNNBasic\n",
    "from surprise import NormalPredictor\n",
    "\n",
    "def LoadBookData():\n",
    "    data = BookDataSet(sample_ratings, books, users)\n",
    "    rankings = data.get_popularity_ranks()\n",
    "    return (data, rankings)\n",
    "\n",
    "\n",
    "# Load up common data set for the recommender algorithms\n",
    "(evaluationData, rankings) = LoadBookData()\n",
    "\n",
    "# Construct an Evaluator to, you know, evaluate them\n",
    "evaluator = Evaluator(evaluationData, rankings)\n",
    "\n",
    "# User-based KNN\n",
    "# UserKNN = KNNBasic(sim_options = {'name': 'cosine', 'user_based': True})\n",
    "# evaluator.add_model(UserKNN, \"User KNN\")\n",
    "\n",
    "# Item-based KNN\n",
    "ItemKNN = KNNBasic(sim_options = {'name': 'cosine', 'user_based': False})\n",
    "evaluator.add_model(ItemKNN, \"Item KNN\")\n",
    "\n",
    "UserKNN_pearson = KNNBasic(sim_options = {'name': 'pearson', 'user_based': True})\n",
    "evaluator.add_model(UserKNN_pearson, \"User KNN pearson\")\n",
    "\n",
    "# Item-based KNN\n",
    "# ItemKNN_pearson = KNNBasic(sim_options = {'name': 'pearson', 'user_based': False})\n",
    "# evaluator.add_model(ItemKNN_pearson, \"Item KNN pearson\")\n",
    "\n",
    "# Fight!\n",
    "evaluator.evaluate(topN=False)\n",
    "\n",
    "# sample user \n",
    "evaluator.recommend_top_books(evaluationData, test_user_id=177458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalMetrics:\n",
    "    \n",
    "    def RMSE(y_pred):\n",
    "        return accuracy.rmse(y_pred, verbose=True)\n",
    "    \n",
    "    def MAE(y_pred):\n",
    "        return accuracy.mae(y_pred, verbose=True)\n",
    "    \n",
    "    def hit_rate(topNPredicted, leftOutPredictions):\n",
    "        hits = 0\n",
    "        N = 0\n",
    "        \n",
    "        for leftOut in leftOutPredictions:\n",
    "            user_id = leftOut[0] \n",
    "            left_out_book_id = leftOut[1] \n",
    "            hit = False\n",
    "            for book_id, prediction in topNPredicted[int(user_id)]:\n",
    "                if left_out_book_id.strip() == book_id.strip():\n",
    "                    hit = True\n",
    "                    break\n",
    "            if hit: hits += 1\n",
    "            N += 1\n",
    "        \n",
    "        return hits / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from sortedcontainers import SortedList\n",
    "from BookData import BookDataSet\n",
    "from EvaluationData import CreateDataSets\n",
    "from EvaluatedAlgorithm import EvaluatedAlgorithm\n",
    "\n",
    "\n",
    "class SimpleCollaborativeFiltering:\n",
    "    def __init__(self, ratings, books, users, k=10, max_rating=10.0):\n",
    "        self.data = BookDataSet(ratings, books, users)\n",
    "        self.k = k\n",
    "        self.max_rating = max_rating\n",
    "        \n",
    "    def get_neighbors(self, user_id, user_based_=True, metric='cosine', verbose=False):\n",
    "        # create model\n",
    "        self.train = self.data.build_full_trainset()\n",
    "\n",
    "        sim_options = {'name': metric,\n",
    "                       'user_based': user_based_\n",
    "        }\n",
    "\n",
    "        model = KNNBasic(sim_options=sim_options, verbose=verbose)\n",
    "        model.fit(self.train)\n",
    "        interations = model.compute_similarities()\n",
    "\n",
    "        self.surprise_user_id = self.train.to_inner_uid(user_id)\n",
    "        similarity_rows = interations[self.surprise_user_id]\n",
    "        return interations, similarity_rows\n",
    "\n",
    "    \n",
    "    def show_top_books(self, candidates, watched_list):\n",
    "        # print out top 10 books and score \n",
    "        N = 0\n",
    "        print('Top {} Book Recommendations:'.format(self.k))\n",
    "        print('\\n')\n",
    "        for item_id, ratings in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
    "            if item_id not in watched_list:\n",
    "                book_id = self.train.to_raw_iid(item_id)\n",
    "                book_name = self.data.get_book_name(book_id)\n",
    "                book_author = self.data.get_book_author(book_id)\n",
    "                book_year = self.data.get_book_year(book_id)\n",
    "                try:\n",
    "                    book_year = int(book_year) \n",
    "                except:\n",
    "                    book_year = 'Not Available'\n",
    "                print('{} - {} - ({}) - Score: {}'.format(book_name, book_author, book_year, round(ratings, 2)))\n",
    "                N += 1\n",
    "                if (N > self.k): break\n",
    "        \n",
    "    def user_based(self, user_id, threshold=False):\n",
    "        _, similarity_rows = self.get_neighbors(user_id)\n",
    "        \n",
    "        similar_users = SortedList(key=lambda x: -x[1])\n",
    "        for i, score in enumerate(similarity_rows):\n",
    "            if i != user_id:\n",
    "                similar_users.add((i, score))\n",
    "        \n",
    "        if threshold:\n",
    "            similar_users = [rating for rating in similar_users if rating[1] >= 0.95]\n",
    "\n",
    "        candidates = defaultdict(float)\n",
    "        for similar_user in similar_users[:self.k]:\n",
    "            surprise_sim_user_idx, score = similar_user\n",
    "            sim_user_rating = self.train.ur[surprise_sim_user_idx]\n",
    "            ### NEED TO GET MEANS FROM sim_user_rating\n",
    "            for info in sim_user_rating:\n",
    "                book_id, rating = info\n",
    "                # use += and increase weight for books that appear more than once \n",
    "                candidates[book_id] += (rating / self.max_rating) * score\n",
    "\n",
    "        # list of books that the user has seen\n",
    "        watched_list = [book_id for book_id, rating in self.train.ur[self.surprise_user_id]]\n",
    "\n",
    "        # Get top-rated items from similar users\n",
    "        self.show_top_books(candidates, watched_list)\n",
    "\n",
    "    def user_based_eval(self, threshold=False):\n",
    "        \n",
    "        rankings = self.data.get_popularity_ranks()\n",
    "        evalData = CreateDataSets(self.data, rankings)\n",
    "        \n",
    "        trainSet = evalData.get_LOOCV_train()\n",
    "#         trainSet = evalData.GetLOOCVTrainSet()\n",
    "        \n",
    "        sim_options = {'name': 'cosine',\n",
    "                       'user_based': True\n",
    "                       }\n",
    "\n",
    "        model = KNNBasic(sim_options=sim_options)\n",
    "        model.fit(trainSet)\n",
    "        simsMatrix = model.compute_similarities()\n",
    "\n",
    "        leftOutTestSet = evalData.get_LOOCV_test()\n",
    "#         leftOutTestSet = evalData.GetLOOCVTestSet()\n",
    "        \n",
    "        # Build up dict to lists of (int(movieID), predictedrating) pairs\n",
    "        topN = defaultdict(list)\n",
    "        k = 10\n",
    "        for uiid in range(trainSet.n_users):\n",
    "            # Get top N similar users to this one\n",
    "            similarityRow = simsMatrix[uiid]\n",
    "\n",
    "            similarUsers = []\n",
    "            for innerID, score in enumerate(similarityRow):\n",
    "                if (innerID != uiid):\n",
    "                    similarUsers.append( (innerID, score) )\n",
    "\n",
    "            kNeighbors = heapq.nlargest(k, similarUsers, key=lambda t: t[1])\n",
    "            \n",
    "            # Get the stuff they rated, and add up ratings for each item, weighted by user similarity\n",
    "            candidates = defaultdict(float)\n",
    "            for similarUser in kNeighbors:\n",
    "                innerID = similarUser[0]\n",
    "                userSimilarityScore = similarUser[1]\n",
    "                theirRatings = trainSet.ur[innerID]\n",
    "                for rating in theirRatings:\n",
    "                    candidates[rating[0]] += (rating[1] / 10.0) * userSimilarityScore\n",
    "\n",
    "            # Build a dictionary of stuff the user has already seen\n",
    "            watched = {}\n",
    "            for itemID, rating in trainSet.ur[uiid]:\n",
    "                watched[itemID] = 1\n",
    "\n",
    "            # Get top-rated items from similar users:\n",
    "            pos = 0\n",
    "            for itemID, ratingSum in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
    "                if not itemID in watched:\n",
    "                    movieID = trainSet.to_raw_iid(itemID)\n",
    "                    topN[int(trainSet.to_raw_uid(uiid))].append( (movieID, 0.0) )\n",
    "                    pos += 1\n",
    "                    if (pos > 40):\n",
    "                        break\n",
    "\n",
    "        # Measure\n",
    "        print(\"HR\", EvalMetrics.hit_rate(topN, leftOutTestSet)) \n",
    "#         print(\"HR\", RecommenderMetrics.HitRate(topN, leftOutTestSet))   \n",
    "\n",
    "        \n",
    "        \n",
    "    def item_based(self, user_id, threshold=False):\n",
    "        interations, _ = self.get_neighbors(user_id, user_based_=False)\n",
    "\n",
    "        # Get the top K items we rated\n",
    "        test_user_ratings = self.train.ur[self.surprise_user_id]\n",
    "\n",
    "        if threshold:\n",
    "            kNeighbors = [rating for rating in test_user_ratings if rating[1] >= 7.0]\n",
    "        else:\n",
    "            kNeighbors = heapq.nlargest(self.k, test_user_ratings, key=lambda t: t[1])\n",
    "        \n",
    "        # Get similar items to stuff we liked (weighted by rating)\n",
    "        candidates = defaultdict(float)\n",
    "        for item_id, rating in kNeighbors:\n",
    "            similarity_row = interations[item_id]\n",
    "            for inner_id, score in enumerate(similarity_row):\n",
    "                candidates[inner_id] += score * (rating / 10.0)\n",
    "\n",
    "        # list of books that the user has seen\n",
    "        watched_list = [book_id for book_id, rating in self.train.ur[self.surprise_user_id]]\n",
    "        \n",
    "        # Get top-rated items from similar items\n",
    "        self.show_top_books(candidates, watched_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 6242\n",
    "cf = SimpleCollaborativeFiltering(sample_ratings, books, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf.user_based_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Book Recommendations:\n",
      "\n",
      "\n",
      "All Around the Town - Mary Higgins Clark - (1993) - Score: 2.6\n",
      "Gerald's Game - Stephen King - (2001) - Score: 2.6\n",
      "The Scarlet Letter - NATHANIEL HAWTHORNE - (1965) - Score: 2.5\n",
      "Dolores Claiborne - Stephen King - (2004) - Score: 2.5\n",
      "The Dark Half - Stephen King - (1994) - Score: 2.4\n",
      "Loves Music, Loves to Dance - Mary Higgins Clark - (1992) - Score: 2.4\n",
      "One Door Away from Heaven - Dean R. Koontz - (2002) - Score: 2.1\n",
      "Harry Potter and the Prisoner of Azkaban (Book 3) - J. K. Rowling - (1999) - Score: 2.0\n",
      "Harry Potter and the Goblet of Fire (Book 4) - J. K. Rowling - (2000) - Score: 2.0\n",
      "Stuart Little - E.B. White - (Not Available) - Score: 2.0\n",
      "It Came From The Far Side - Gary Larson - (1986) - Score: 2.0\n"
     ]
    }
   ],
   "source": [
    "cf.user_based(user_id=user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Book Recommendations:\n",
      "\n",
      "\n",
      "Como Agua Para Chocolate/Like Water for Chocolate - Laura Esquivel - (2001) - Score: 8.0\n",
      "Naked Lunch - William S. Burroughs - (1992) - Score: 8.0\n",
      "Slaughterhouse Five or the Children's Crusade: A Duty Dance With Death - Kurt Vonnegut - (1991) - Score: 8.0\n",
      "One Hundred Years of Solitude - Gabriel Garcia Marquez - (1998) - Score: 7.99\n",
      "The Phantom Tollbooth - Norton Juster - (1993) - Score: 7.99\n",
      "Shiloh (Yearling Newbery) - Phyllis Reynolds Naylor - (1992) - Score: 7.99\n",
      "Lakota Woman - Dog Mary Crow - (1991) - Score: 7.98\n",
      "Girl in Hyacinth Blue - Susan Vreeland - (2000) - Score: 7.97\n",
      "The World According to Garp - John Irving - (1994) - Score: 7.92\n",
      "Anne Frank: The Diary of a Young Girl - ANNE FRANK - (1993) - Score: 7.91\n",
      "Blue Diary - Alice Hoffman - (2002) - Score: 7.89\n"
     ]
    }
   ],
   "source": [
    "cf.item_based(user_id=user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataSets\n",
      "Build a full training\n",
      "Build a full test\n",
      "Train / test split\n",
      "DONE with Train / test split\n",
      "Done\n",
      "Evaluating  SVD ...\n",
      "Evaluating accuracy...\n",
      "Analysis complete.\n",
      "Evaluating  SVD++ ...\n",
      "Evaluating accuracy...\n",
      "Analysis complete.\n",
      "Evaluating  Random ...\n",
      "Evaluating accuracy...\n",
      "Analysis complete.\n",
      "\n",
      "\n",
      "Algorithm  RMSE       MAE       \n",
      "SVD        1.6377     1.2648    \n",
      "SVD++      1.6493     1.2695    \n",
      "Random     2.5053     2.0013    \n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from surprise import SVD, SVDpp\n",
    "from surprise import KNNBasic\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from sortedcontainers import SortedList\n",
    "from BookData import BookDataSet\n",
    "from EvaluationData import CreateDataSets\n",
    "from EvaluatedAlgorithm import EvaluatedAlgorithm\n",
    "from Evaluator import Evaluator\n",
    "from surprise import NormalPredictor\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "evaluationData = BookDataSet(ratings, books, users)\n",
    "rankings = evaluationData.get_popularity_ranks()\n",
    "# Load up common data set for the recommender algorithms\n",
    "\n",
    "# Construct an Evaluator to, you know, evaluate them\n",
    "evaluator = Evaluator(evaluationData, rankings)\n",
    "\n",
    "# SVD\n",
    "SVD = SVD()\n",
    "evaluator.add_model(SVD, 'SVD')\n",
    "\n",
    "# SVD++\n",
    "SVDPlusPlus = SVDpp()\n",
    "evaluator.add_model(SVDPlusPlus, 'SVD++')\n",
    "\n",
    "\n",
    "evaluator.evaluate(False)\n",
    "\n",
    "# evaluator.SampleTopNRecs(evaluationData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  SVD ...\n",
      "Evaluating accuracy...\n",
      "Analysis complete.\n",
      "Evaluating  SVD++ ...\n",
      "Evaluating accuracy...\n",
      "Analysis complete.\n",
      "\n",
      "\n",
      "Algorithm  RMSE       MAE       \n",
      "SVD        1.6773     1.3131    \n",
      "SVD++      1.6835     1.3123    \n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from surprise import SVD, SVDpp\n",
    "from surprise import KNNBasic\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from sortedcontainers import SortedList\n",
    "from BookData import BookDataSet\n",
    "from EvaluationData import CreateDataSets\n",
    "from EvaluatedAlgorithm import EvaluatedAlgorithm\n",
    "from Evaluator import Evaluator\n",
    "from surprise import NormalPredictor\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "b_ds = BookDataSet(ratings, books, users)\n",
    "rankings = b_ds.get_popularity_ranks()\n",
    "evaluationData = b_ds.load_ratings_dataset(remove_outliers=True)\n",
    "# Load up common data set for the recommender algorithms\n",
    "\n",
    "# Construct an Evaluator to, you know, evaluate them\n",
    "evaluator = Evaluator(evaluationData, rankings)\n",
    "\n",
    "# SVD\n",
    "SVD = SVD()\n",
    "evaluator.add_model(SVD, 'SVD')\n",
    "\n",
    "# SVD++\n",
    "SVDPlusPlus = SVDpp()\n",
    "evaluator.add_model(SVDPlusPlus, 'SVD++')\n",
    "\n",
    "\n",
    "evaluator.evaluate(False)\n",
    "\n",
    "# evaluator.SampleTopNRecs(evaluationData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using recommender  SVD\n",
      "\n",
      "Building recommendation model...\n",
      "Computing recommendations...\n",
      "\n",
      "We recommend:\n",
      " 9.08\n",
      "Harry Potter and the Chamber of Secrets Postcard Book 9.05\n",
      "Harry Potter and the Prisoner of Azkaban (Book 3) 8.95\n",
      "Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback)) 8.92\n",
      "My Sister's Keeper : A Novel (Picoult, Jodi) 8.89\n",
      "The Grapes of Wrath 8.84\n",
      "Harry Potter and the Sorcerer's Stone (Book 1) 8.76\n",
      "Illusions 8.74\n",
      "To Kill a Mockingbird 8.72\n",
      "84 Charing Cross Road 8.71\n",
      "\n",
      "Using recommender  SVD++\n",
      "\n",
      "Building recommendation model...\n",
      "Computing recommendations...\n",
      "\n",
      "We recommend:\n",
      "Stupid White Men ...and Other Sorry Excuses for the State of the Nation! 9.4\n",
      "The Red Tent (Bestselling Backlist) 9.3\n",
      "Harry Potter and the Chamber of Secrets Postcard Book 9.22\n",
      "Harry Potter and the Prisoner of Azkaban (Book 3) 9.05\n",
      "My Sister's Keeper : A Novel (Picoult, Jodi) 9.03\n",
      "Dilbert: A Book of Postcards 8.97\n",
      "Lonesome Dove 8.97\n",
      "Weirdos From Another Planet! 8.97\n",
      "Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback)) 8.9\n",
      "The Two Towers (The Lord of the Rings, Part 2) 8.89\n"
     ]
    }
   ],
   "source": [
    "evaluator.recommend_top_books(b_ds, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras on all data\n",
    "from keras import Model, Input\n",
    "from keras.layers import Embedding, Dense, Dot, Add, Flatten, Concatenate, Dropout, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Rating</th>\n",
       "      <th>Book_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "      <td>225816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "      <td>246838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "      <td>246839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276736</td>\n",
       "      <td>3257224281</td>\n",
       "      <td>8</td>\n",
       "      <td>271361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276737</td>\n",
       "      <td>0600570967</td>\n",
       "      <td>6</td>\n",
       "      <td>271362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID        ISBN  Book_Rating  Book_ID\n",
       "0   276726  0155061224            5   225816\n",
       "1   276729  052165615X            3   246838\n",
       "2   276729  0521795028            6   246839\n",
       "3   276736  3257224281            8   271361\n",
       "4   276737  0600570967            6   271362"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/book_mapping.pkl', 'rb') as f:\n",
    "    book_mapping = dill.load(f)\n",
    "\n",
    "ratings['Book_ID'] = ratings.ISBN.map(book_mapping)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lengths of unique users and books \n",
    "N = ratings.User_ID.nunique() + 1\n",
    "M = ratings.Book_ID.nunique() + 1\n",
    "\n",
    "# create train / test\n",
    "train, test = train_test_split(ratings, test_size=.20, random_state=100)\n",
    "\n",
    "# parameters \n",
    "K = 100\n",
    "mu = train.Book_Rating.mean()\n",
    "epochs = 20\n",
    "reg = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_keras():\n",
    "    # input\n",
    "    user = Input(shape=(1,))\n",
    "    book = Input(shape=(1,))\n",
    "    # ratings table\n",
    "    user_embeddings = Embedding(N, K, embeddings_regularizer=l2(reg))(user)\n",
    "    book_embeddings = Embedding(M, K, embeddings_regularizer=l2(reg))(book)\n",
    "    X = Dot(axes=2)([user_embeddings, book_embeddings])\n",
    "    # bias terms\n",
    "    user_bias = Embedding(N, 1, embeddings_regularizer=l2(reg))(user)\n",
    "    book_bias = Embedding(M, 1, embeddings_regularizer=l2(reg))(book)\n",
    "    # add bias\n",
    "    X = Add()([X, user_bias, book_bias])\n",
    "    # flatten\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    # create model\n",
    "    model = Model([user, book], X)\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=0.08, momentum=0.9), metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mf_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eary_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# results = model.fit(\n",
    "#     x = [train.User_ID.values, train.Book_ID.values],\n",
    "#     y = train.Book_Rating - mu,\n",
    "#     epochs = 30,\n",
    "#     batch_size = 256,\n",
    "#     validation_data = ([test.User_ID.values, test.Book_ID.values], test.Book_Rating - mu),\n",
    "#     callbacks=[eary_stop]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_keras():\n",
    "    user = Input(shape=(1,))\n",
    "    book = Input(shape=(1,))\n",
    "    \n",
    "    user_embeddings = Embedding(N, K, embeddings_regularizer=l2(reg))(user)\n",
    "    book_embeddings = Embedding(M, K, embeddings_regularizer=l2(reg))(book)\n",
    "    user_embeddings = Flatten()(user_embeddings)\n",
    "    book_embeddings = Flatten()(book_embeddings)\n",
    "    X = Concatenate()([user_embeddings, book_embeddings])\n",
    "    X = Dense(400)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(100, activation='relu')(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(1)(X)\n",
    "    \n",
    "    model = Model([user, book], X)\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=0.08, momentum=0.9), metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eary_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# results = model.fit(\n",
    "#     x = [train.User_ID.values, train.Book_ID.values],\n",
    "#     y = train.Book_Rating - mu,\n",
    "#     epochs = 30,\n",
    "#     batch_size = 256,\n",
    "#     validation_data = ([test.User_ID.values, test.Book_ID.values], test.Book_Rating - mu),\n",
    "#     callbacks=[eary_stop]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_keras():\n",
    "    user = Input(shape=(1,))\n",
    "    book = Input(shape=(1,))\n",
    "\n",
    "    # MF\n",
    "    user_embeddings = Embedding(N, K, embeddings_regularizer=l2(reg))(user)\n",
    "    book_embeddings = Embedding(M, K, embeddings_regularizer=l2(reg))(book)\n",
    "    X = Dot(axes=2)([user_embeddings, book_embeddings])\n",
    "    user_bias = Embedding(N, 1, embeddings_regularizer=l2(reg))(user)\n",
    "    book_bias = Embedding(M, 1, embeddings_regularizer=l2(reg))(book)\n",
    "    X = Add()([X, user_bias, book_bias])\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    # Side \n",
    "    user_embeddings = Flatten()(user_embeddings)\n",
    "    book_embeddings = Flatten()(book_embeddings)\n",
    "    X2 = Concatenate()([user_embeddings, book_embeddings])\n",
    "    X2 = Dense(400)(X2)\n",
    "    X2 = BatchNormalization()(X2)\n",
    "    X2 = Activation('relu')(X2)\n",
    "    X2 = Dropout(0.5)(X2)\n",
    "    X2 = Dense(100, activation='relu')(X2)\n",
    "    X2 = Dropout(0.5)(X2)\n",
    "    X2 = Dense(1)(X2)\n",
    "    \n",
    "    # Add two together\n",
    "    X = Add()([X, X2])\n",
    "    \n",
    "    model = Model([user, book], X)\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=0.08, momentum=0.9), metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = residual_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eary_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# results = model.fit(\n",
    "#     x = [train.User_ID.values, train.Book_ID.values],\n",
    "#     y = train.Book_Rating - mu,\n",
    "#     epochs = 30,\n",
    "#     batch_size = 256,\n",
    "#     validation_data = ([test.User_ID.values, test.Book_ID.values], test.Book_Rating - mu),\n",
    "#     callbacks=[eary_stop]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import *\n",
    "from fastai.column_data import *\n",
    "\n",
    "\n",
    "val_idxs = get_cv_idxs(len(ratings))\n",
    "wd=2e-4\n",
    "n_factors = 100\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = CollabFilterDataset.from_csv(path, 'ratings_explicit_clean.csv', 'User_ID', 'ISBN', 'Book_Rating')\n",
    "learn = cf.get_learner(n_factors, val_idxs, 64, opt_fn=optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa9149e2fba437cab1292a3a02195f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      3.530962   3.570459  \n",
      "    1      3.473049   3.449649                                 \n",
      "    2      3.236338   3.453688                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([3.45369])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1e-2, 2, wds=wd, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8584100731539313"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(3.453688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ratings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-231f5eba4aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu_uniq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUser_ID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muser2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_uniq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'New_User_ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUser_ID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mm_uniq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mISBN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ratings' is not defined"
     ]
    }
   ],
   "source": [
    "u_uniq = ratings.User_ID.unique()\n",
    "user2idx = {o:i for i,o in enumerate(u_uniq)}\n",
    "ratings['New_User_ID'] = ratings.User_ID.apply(lambda x: user2idx[x])\n",
    "\n",
    "m_uniq = ratings.ISBN.unique()\n",
    "book2idx = {o:i for i,o in enumerate(m_uniq)}\n",
    "ratings['New_Book_ID'] = ratings.ISBN.apply(lambda x: book2idx[x])\n",
    "\n",
    "n_users = int(ratings.New_User_ID.nunique())\n",
    "n_books = int(ratings.New_Book_ID.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TorchDataSet(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.X = ratings[['New_User_ID', 'New_Book_ID']].values\n",
    "        self.y = ratings['Book_Rating'].values\n",
    "        self.N = len(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_torch_data_set = TorchDataSet(train)\n",
    "test_torch_data_set = TorchDataSet(train)\n",
    "train_data_loader = DataLoader(train_torch_data_set, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_torch_data_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "n_factors = 50\n",
    "max_rating = float(train.Book_Rating.max())\n",
    "min_rating = float(train.Book_Rating.min())\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def rmse(x, y): \n",
    "    return np.sqrt(mse(x, y))\n",
    "\n",
    "def mae(x, y): \n",
    "    return np.abs((x-y)).mean()\n",
    "\n",
    "\n",
    "metrics=[mse, rmse, mae]\n",
    "\n",
    "\n",
    "class MiniNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books, hidden=100):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.linear1 = nn.Linear(n_factors * 2, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, 1)\n",
    "        self.drop1 = nn.Dropout(0.75)\n",
    "        self.drop2 = nn.Dropout(0.75)\n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_embedding.weight.data.uniform_(-0.01,0.01)       \n",
    "    \n",
    "    def forward(self, users, books):\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "        X = self.drop1(torch.cat([u, b], dim=1))\n",
    "        X = self.drop2(F.relu(self.linear1(X)))\n",
    "        return F.sigmoid(self.linear2(X)) * (max_rating - min_rating+1) + min_rating-0.5\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "class EmbeddingDotBias(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.book_bias = nn.Embedding(n_books, 1)\n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_embedding.weight.data.uniform_(-0.01,0.01)       \n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.book_bias.weight.data.uniform_(-0.01,0.01)\n",
    "    \n",
    "    def forward(self, users, books):\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "#         print(self.user_bias(users).size())\n",
    "        u_b = self.user_bias(users).squeeze()\n",
    "#         print(u.size(), u_b.size())\n",
    "        b_b = self.book_bias(books).squeeze()\n",
    "        X = ( (u * b).sum(1) ) + u_b + b_b\n",
    "        X = F.sigmoid(X) * (max_rating - min_rating) + max_rating\n",
    "        return X.view(-1, 1)\n",
    "    \n",
    "    \n",
    "class EmbeddingDot(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.user_embedding.weight.data.uniform_(0, 0.05)\n",
    "        self.book_embedding.weight.data.uniform_(0, 0.05)\n",
    "    \n",
    "    def forward(self, users, books):\n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "        return (u * b).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingDotBias(n_users, n_books).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.learner import fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(model, train_data_loader, n_epochs=10, opt=torch.optim.Adam(model.parameters()), crit=nn.MSELoss(), metrics=[mse, rmse, mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from training import fit\n",
    "# fit(model, train_data_loader, loss=nn.MSELoss(), epochs=5, save=True, val_loader=test_data_loader, \n",
    "#                                                     metrics=[mse, rmse, mae], cycle_len=2, print_period=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24036, 3)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from BookData import BookDataSet\n",
    "from EvaluationData import CreateDataSets\n",
    "from EvaluatedAlgorithm import EvaluatedAlgorithm\n",
    "from Evaluator import Evaluator\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def load_feather(filepath, **kwargs):\n",
    "    '''\n",
    "    input: (path to feather file)\n",
    "    read feather file to pandas dataframe\n",
    "    output: (pandas dataframe)\n",
    "    '''\n",
    "    return pd.read_feather(filepath, **kwargs)\n",
    "\n",
    "ratings = load_feather(DATA_PATH/'ratings_explicit_clean.feather')\n",
    "ratings.drop(['index'], axis=1, inplace=True)\n",
    "users = load_feather(DATA_PATH/'users_clean.feather')\n",
    "books = load_feather(DATA_PATH/'books_clean.feather')\n",
    "\n",
    "\n",
    "counts_users = ratings.User_ID.value_counts()\n",
    "counts_ratings = ratings.Book_Rating.value_counts()\n",
    "sample_ratings = ratings[ratings['User_ID'].isin(counts_users[counts_users >= 200].index)]\n",
    "sample_ratings = sample_ratings[ratings['Book_Rating'].isin(counts_ratings[counts_ratings >= 200].index)]\n",
    "isbn_group = sample_ratings.groupby('ISBN', as_index=False)['Book_Rating'].count()\n",
    "sample_ratings = sample_ratings[sample_ratings.ISBN.isin(list(isbn_group[isbn_group.Book_Rating > 1].ISBN.values))]\n",
    "print(sample_ratings.shape)\n",
    "\n",
    "import dill\n",
    "with open('data/book_mapping.pkl', 'rb') as f:\n",
    "    book_mapping = dill.load(f)\n",
    "\n",
    "ratings['Book_ID'] = ratings.ISBN.map(book_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Rating</th>\n",
       "      <th>Book_ID</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "      <td>225816</td>\n",
       "      <td>Rites of Passage</td>\n",
       "      <td>Judith Rae</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Heinle</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "      <td>246838</td>\n",
       "      <td>Help!: Level 1</td>\n",
       "      <td>Philip Prowse</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "      <td>246839</td>\n",
       "      <td>The Amsterdam Connection : Level 4 (Cambridge ...</td>\n",
       "      <td>Sue Leather</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276736</td>\n",
       "      <td>3257224281</td>\n",
       "      <td>8</td>\n",
       "      <td>271361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276737</td>\n",
       "      <td>0600570967</td>\n",
       "      <td>6</td>\n",
       "      <td>271362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID        ISBN  Book_Rating  Book_ID  \\\n",
       "0   276726  0155061224            5   225816   \n",
       "1   276729  052165615X            3   246838   \n",
       "2   276729  0521795028            6   246839   \n",
       "3   276736  3257224281            8   271361   \n",
       "4   276737  0600570967            6   271362   \n",
       "\n",
       "                                          Book_Title    Book_Author  \\\n",
       "0                                   Rites of Passage     Judith Rae   \n",
       "1                                     Help!: Level 1  Philip Prowse   \n",
       "2  The Amsterdam Connection : Level 4 (Cambridge ...    Sue Leather   \n",
       "3                                                NaN            NaN   \n",
       "4                                                NaN            NaN   \n",
       "\n",
       "   Year_Of_Publication                   Publisher language  \n",
       "0               2001.0                      Heinle       en  \n",
       "1               1999.0  Cambridge University Press       nl  \n",
       "2               2001.0  Cambridge University Press       en  \n",
       "3                  0.0                         NaN      NaN  \n",
       "4                  0.0                         NaN      NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ratings = ratings.merge(books, on='ISBN', how='left')\n",
    "full_ratings.Year_Of_Publication = full_ratings.Year_Of_Publication.fillna(0)\n",
    "full_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Rating</th>\n",
       "      <th>Book_ID</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>ace</th>\n",
       "      <th>...</th>\n",
       "      <th>university</th>\n",
       "      <th>usa</th>\n",
       "      <th>verlag</th>\n",
       "      <th>viking</th>\n",
       "      <th>vintage</th>\n",
       "      <th>vision</th>\n",
       "      <th>warner</th>\n",
       "      <th>washington</th>\n",
       "      <th>william</th>\n",
       "      <th>yearling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "      <td>225816</td>\n",
       "      <td>Rites of Passage</td>\n",
       "      <td>Judith Rae</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Heinle</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "      <td>246838</td>\n",
       "      <td>Help!: Level 1</td>\n",
       "      <td>Philip Prowse</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>nl</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "      <td>246839</td>\n",
       "      <td>The Amsterdam Connection : Level 4 (Cambridge ...</td>\n",
       "      <td>Sue Leather</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276736</td>\n",
       "      <td>3257224281</td>\n",
       "      <td>8</td>\n",
       "      <td>271361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276737</td>\n",
       "      <td>0600570967</td>\n",
       "      <td>6</td>\n",
       "      <td>271362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID        ISBN  Book_Rating  Book_ID  \\\n",
       "0   276726  0155061224            5   225816   \n",
       "1   276729  052165615X            3   246838   \n",
       "2   276729  0521795028            6   246839   \n",
       "3   276736  3257224281            8   271361   \n",
       "4   276737  0600570967            6   271362   \n",
       "\n",
       "                                          Book_Title    Book_Author  \\\n",
       "0                                   Rites of Passage     Judith Rae   \n",
       "1                                     Help!: Level 1  Philip Prowse   \n",
       "2  The Amsterdam Connection : Level 4 (Cambridge ...    Sue Leather   \n",
       "3                                                NaN            NaN   \n",
       "4                                                NaN            NaN   \n",
       "\n",
       "   Year_Of_Publication                   Publisher language  ace    ...     \\\n",
       "0               2001.0                      Heinle       en    0    ...      \n",
       "1               1999.0  Cambridge University Press       nl    0    ...      \n",
       "2               2001.0  Cambridge University Press       en    0    ...      \n",
       "3                  0.0                         NaN      NaN    0    ...      \n",
       "4                  0.0                         NaN      NaN    0    ...      \n",
       "\n",
       "   university  usa  verlag  viking  vintage  vision  warner  washington  \\\n",
       "0           0    0       0       0        0       0       0           0   \n",
       "1           1    0       0       0        0       0       0           0   \n",
       "2           1    0       0       0        0       0       0           0   \n",
       "3           0    0       0       0        0       0       0           0   \n",
       "4           0    0       0       0        0       0       0           0   \n",
       "\n",
       "   william  yearling  \n",
       "0        0         0  \n",
       "1        0         0  \n",
       "2        0         0  \n",
       "3        0         0  \n",
       "4        0         0  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_features=100, binary=True)\n",
    "publisher_counts = vect.fit_transform(full_ratings.Publisher.fillna('Null').values)\n",
    "full_ratings = full_ratings.join(pd.DataFrame(publisher_counts.todense(), columns=vect.get_feature_names()))\n",
    "full_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       None\n",
       "1                       None\n",
       "2                       None\n",
       "3                       None\n",
       "4                       None\n",
       "5               john grisham\n",
       "6                       None\n",
       "7              rebecca wells\n",
       "8             terry mcmillan\n",
       "9                       None\n",
       "10                      None\n",
       "11                      None\n",
       "12                      None\n",
       "13                      None\n",
       "14              stephen king\n",
       "15               ken follett\n",
       "16                      None\n",
       "17              joy fielding\n",
       "18               dean koontz\n",
       "19                      None\n",
       "20                      None\n",
       "21                      None\n",
       "22            rita mae brown\n",
       "23                      None\n",
       "24                      None\n",
       "25                      None\n",
       "26                      None\n",
       "27                      None\n",
       "28                      None\n",
       "29               leo tolstoy\n",
       "                 ...        \n",
       "433641                  None\n",
       "433642          carl hiaasen\n",
       "433643                  None\n",
       "433644                  None\n",
       "433645                  None\n",
       "433646    jonathan kellerman\n",
       "433647      elizabeth george\n",
       "433648      elizabeth george\n",
       "433649      elizabeth george\n",
       "433650      elizabeth george\n",
       "433651                  None\n",
       "433652                  None\n",
       "433653        sharyn mccrumb\n",
       "433654                  None\n",
       "433655        faye kellerman\n",
       "433656             ed mcbain\n",
       "433657                  None\n",
       "433658           gary larson\n",
       "433659           scott adams\n",
       "433660                  None\n",
       "433661                  None\n",
       "433662                  None\n",
       "433663        dean r. koontz\n",
       "433664                  None\n",
       "433665          stephen king\n",
       "433666                  None\n",
       "433667                  None\n",
       "433668     catherine coulter\n",
       "433669        jerry spinelli\n",
       "433670                  None\n",
       "Name: Book_Author_Clean, Length: 433671, dtype: category\n",
       "Categories (527, object): [None < a. manette ansay < adriana trigiani < agatha christie ... william shakespeare < winston groom < yann martel < zora neale hurston]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Book_Author\n",
    "\n",
    "full_ratings['Book_Author_Clean'] = full_ratings.Book_Author.fillna('').str.lower()\n",
    "author_counts = full_ratings.Book_Author_Clean.value_counts()\n",
    "kept_authors = author_counts[author_counts > 100].index.tolist()[1:]\n",
    "full_ratings.loc[~full_ratings.Book_Author_Clean.isin(kept_authors), 'Book_Author_Clean'] = 'None'\n",
    "# full_ratings = full_ratings.join(pd.get_dummies(full_ratings.Book_Author_Clean))\n",
    "full_ratings.Book_Author_Clean = full_ratings.Book_Author_Clean.astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_ratings.Year_Of_Publication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ratings.Year_Of_Publication = scaler.fit_transform(full_ratings.Year_Of_Publication.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "full_ratings = pd.read_csv('data/full_ratings.csv')\n",
    "\n",
    "# Year_Of_Publication\n",
    "full_ratings.Year_Of_Publication = full_ratings.Year_Of_Publication.fillna(0)\n",
    "\n",
    "# # Book_Author\n",
    "\n",
    "\n",
    "# full_ratings['Book_Author_Clean'] = full_ratings.Book_Author.fillna('').str.lower()\n",
    "# author_counts = full_ratings.Book_Author_Clean.value_counts()\n",
    "# kept_authors = author_counts[author_counts > 100].index.tolist()[1:]\n",
    "# full_ratings.loc[~full_ratings.Book_Author_Clean.isin(kept_authors), 'Book_Author_Clean'] = 'None'\n",
    "# full_ratings.Book_Author_Clean = full_ratings.Book_Author_Clean.astype('category').cat.as_ordered()\n",
    "\n",
    "\n",
    "# # Use top 100 book authors, all other will be 0 \n",
    "\n",
    "full_ratings['Book_Author_Clean'] = full_ratings.Book_Author.fillna('').str.lower()\n",
    "author_counts = full_ratings.Book_Author_Clean.value_counts()\n",
    "kept_authors = author_counts[author_counts > 300].index.tolist()[1:]\n",
    "full_ratings.loc[~full_ratings.Book_Author_Clean.isin(kept_authors), 'Book_Author_Clean'] = 'None'\n",
    "full_ratings = full_ratings.join(pd.get_dummies(full_ratings.Book_Author_Clean))\n",
    "\n",
    "# # Book_Author\n",
    "# full_ratings['Book_Author_Clean'] = full_ratings.Book_Author.fillna('').str.lower()\n",
    "\n",
    "# # Publisher \n",
    "# vect = CountVectorizer(max_features=100, binary=True)\n",
    "# publisher_counts = vect.fit_transform(full_ratings.Publisher.fillna('Null').values)\n",
    "# full_ratings = full_ratings.join(pd.DataFrame(publisher_counts.todense(), columns=vect.get_feature_names()))\n",
    "\n",
    "# drop extra info\n",
    "# full_ratings.drop(\n",
    "#     ['User_ID', 'Book_ID', 'ISBN', 'Book_Title', 'Book_Author', 'Year_Of_Publication', 'Publisher', \n",
    "#      'language', 'Book_Author_Clean'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset, testset = train_test_split(full_ratings, test_size=0.2, random_state=100)\n",
    "\n",
    "# Year_Of_Publication\n",
    "# scale year \n",
    "scaler = StandardScaler()\n",
    "trainset.Year_Of_Publication = scaler.fit_transform(trainset.Year_Of_Publication.values.reshape(-1, 1))\n",
    "testset.Year_Of_Publication = scaler.transform(testset.Year_Of_Publication.values.reshape(-1, 1))\n",
    "\n",
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "# Publisher \n",
    "# 100 total features, fill NaN with Null\n",
    "# vect = CountVectorizer(max_features=100, binary=True)\n",
    "# publisher_counts_trainset = vect.fit_transform(trainset.Publisher.fillna('Null').values)\n",
    "# publisher_counts_testset = vect.transform(testset.Publisher.fillna('Null').values)\n",
    "\n",
    "# trainset = trainset.join(pd.DataFrame(publisher_counts_trainset.todense(), columns=vect.get_feature_names()))\n",
    "# testset = testset.join(pd.DataFrame(publisher_counts_testset.todense(), columns=vect.get_feature_names()))\n",
    "\n",
    "# # drop extra info\n",
    "for df in (trainset, testset):\n",
    "    df.drop(\n",
    "        ['User_ID', 'Book_ID', 'ISBN', 'Book_Title', 'Book_Author', 'Book_Author_Clean', 'Publisher', 'language'], axis=1, inplace=True)\n",
    "#     df = df[['New_User_ID', 'New_Book_ID', 'Year_Of_Publication', 'Book_Rating']]\n",
    "\n",
    "n_factors = 50\n",
    "max_rating = float(trainset.Book_Rating.max())\n",
    "min_rating = float(trainset.Book_Rating.min())\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def rmse(x, y): \n",
    "    return np.sqrt(mse(x, y))\n",
    "\n",
    "def mae(x, y): \n",
    "    return np.abs((x-y)).mean()\n",
    "\n",
    "\n",
    "metrics=[mse, rmse, mae]\n",
    "\n",
    "class TorchDataSetMeta(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "#         self.X = ratings[['New_User_ID', 'New_Book_ID', 'Year_Of_Publication']].values\n",
    "        self.X = ratings.drop('Book_Rating', axis=1).values\n",
    "        self.y = ratings['Book_Rating'].values\n",
    "        self.N = len(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_torch_data_set_meta = TorchDataSetMeta(trainset)\n",
    "test_torch_data_set_meta = TorchDataSetMeta(testset)\n",
    "train_data_loader_meta = DataLoader(train_torch_data_set_meta, batch_size=16, shuffle=True)\n",
    "test_data_loader_meta = DataLoader(test_torch_data_set_meta, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# def emb_init(x):\n",
    "#     x = x.weight.data\n",
    "#     sc = 2 / (x.size(1) + 1)\n",
    "#     x.uniform_(-sc, sc)\n",
    "    \n",
    "class MiniNetPlusMeta(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_books, hidden=100, n_cont=len(trainset.columns[3:])):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embedding = nn.Embedding(n_books, n_factors)\n",
    "        self.linear1 = nn.Linear((n_factors * 2) + n_cont, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, 1)\n",
    "        self.drop1 = nn.Dropout(0.75)\n",
    "        self.drop2 = nn.Dropout(0.75)\n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.book_embedding.weight.data.uniform_(-0.01, 0.01)   \n",
    "        \n",
    "        nn.init.kaiming_normal(self.linear1.weight.data)\n",
    "        nn.init.kaiming_normal(self.linear2.weight.data)\n",
    "        \n",
    "        self.n_cont = n_cont\n",
    "    \n",
    "    def forward(self, users, books, conts):\n",
    "        \n",
    "        u = self.user_embedding(users)\n",
    "        b = self.book_embedding(books)\n",
    "        X = torch.cat([u, b], dim=1)\n",
    "#         print(conts.size())\n",
    "        if self.n_cont != 0:\n",
    "#             X = torch.cat([X, conts.view(-1, 1)], dim=1)\n",
    "            X = torch.cat([X, conts], dim=1)\n",
    "#             print(X.size())\n",
    "#         print(X.size(), year.size(), year.view(-1, 1).size(), year.view(1, -1).size(), year.view(-1, 1).squeeze().size(), year.view(1, -1).squeeze().size())\n",
    "#         X = torch.cat([X, year.view(-1, 1)], dim=1)\n",
    "#         print(X.size(), 'HERE')\n",
    "        X = self.drop1(F.relu(self.linear1(X)))\n",
    "        return F.sigmoid(self.linear2(X)) * (max_rating - min_rating+1) + min_rating-0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = full_ratings.New_User_ID.nunique()\n",
    "n_books = full_ratings.New_Book_ID.nunique()\n",
    "\n",
    "model = MiniNetPlusMeta(n_users, n_books).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniNetPlusMeta(\n",
       "  (user_embedding): Embedding(77805, 50)\n",
       "  (book_embedding): Embedding(185973, 50)\n",
       "  (linear1): Linear(in_features=258, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (drop1): Dropout(p=0.75)\n",
       "  (drop2): Dropout(p=0.75)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6886d6374bb4689b8051e8d9eb2c01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500/21683 (7%)]\n",
      "[21600/21683 (100%)]\n",
      " [1, 2.8900738667956314, 2.726768428098232, [1.9653071667435424, 1.3973155341904981, 1.5916718317573801]]\n",
      "[21600/21683 (100%)]\n",
      " [2, 2.261802062627954, 2.77124555050447, [2.0519552871309963, 1.4282277913111163, 1.6554916325530442]]\n",
      "[21600/21683 (100%)]\n",
      " [3, 2.259878089443837, 2.750249239626406, [2.0643428563191883, 1.4324711535689576, 1.6629999927928967]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from training import fit\n",
    "fit(model, train_data_loader_meta, loss=nn.MSELoss(), epochs=3, save=True, val_loader=test_data_loader_meta, \n",
    "                                                    metrics=[mse, rmse, mae], cycle_len=2, print_period=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32]) torch.Size([32, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0215  0.6042 -1.6678  ...   0.5722 -0.9601  1.0491\n",
       " 0.2472  1.2187  0.2371  ...  -0.1111  0.4594 -2.0057\n",
       "-1.1620 -1.4457 -1.3800  ...  -1.2330  0.0560  2.0366\n",
       "          ...             ⋱             ...          \n",
       "-1.3632 -0.2363 -0.0339  ...   0.8770  0.1878  2.0995\n",
       "-0.7188 -1.0034 -0.5026  ...   0.2437 -1.2797  1.3134\n",
       " 0.6264  0.0505 -0.5076  ...  -0.2832 -0.4162  0.6637\n",
       "[torch.FloatTensor of size 32x51]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.Tensor(torch.randn([32, 50]))\n",
    "print(z.size())\n",
    "w = torch.Tensor(torch.randn([32]))\n",
    "print(w.size(), w.view(-1, 1).size())\n",
    "\n",
    "torch.cat([z, w.view(-1, 1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# loss = torch.nn.MSELoss()\n",
    "\n",
    "# train_score = []\n",
    "# for epoch in range(5):\n",
    "#     total_train_mse = 0\n",
    "#     model.train()\n",
    "#     for i, (X, y) in enumerate(train_data_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         user_ids, book_ids = X[:, 0], X[:, 1]\n",
    "#         user_ids = Variable(user_ids.long()).cuda()\n",
    "#         book_ids = Variable(book_ids.long()).cuda()\n",
    "#         y = Variable(y.float()).cuda()\n",
    "#         y_hat = model(user_ids, book_ids)\n",
    "#         train_score.append(y_hat.data.cpu().numpy()[0])\n",
    "#         l = loss(y_hat, y)\n",
    "#         l.backward()\n",
    "#         optimizer.step()\n",
    "#         if i % 2500 == 0:\n",
    "#             print('train MSE', l.data[0])\n",
    "#     N = 0\n",
    "#     total_mse = 0\n",
    "#     model.eval()\n",
    "#     for i, (X_test, y_test) in enumerate(test_data_loader):\n",
    "#         user_ids_test, book_ids_test = X_test[:, 0], X_test[:, 1]\n",
    "#         user_ids_test = Variable(user_ids_test.long()).cuda()\n",
    "#         book_ids_test = Variable(book_ids_test.long()).cuda()\n",
    "#         y_test = Variable(y_test.float()).cuda()\n",
    "#         y_hat_test = model(user_ids_test, book_ids_test)\n",
    "#         total_mse += y_hat_test.data.cpu().numpy()[0]\n",
    "#         N += 1\n",
    "#     final_mse = total_mse / len(test_data_loader)\n",
    "#     print('Test MSE', final_mse)\n",
    "#     print('Test RMSE', np.sqrt(final_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm_notebook, tnrange, tqdm\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import kaiming_normal\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import RMSprop\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class _LRScheduler(object):\n",
    "    def __init__(self, optimizer, last_epoch=-1):\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        self.step(last_epoch + 1)\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "    def get_lr(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "class CosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.optimizer = optimizer\n",
    "        super(CosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + np.cos(np.pi * self.last_epoch / self.T_max)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "    \n",
    "    def _reset(self, epoch, T_max):\n",
    "        \"\"\"\n",
    "        Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        return CosineAnnealingLR(self.optimizer, self.T_max, self.eta_min, last_epoch=epoch)\n",
    "\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def rmse(x, y): \n",
    "    return np.sqrt(mse(x, y))\n",
    "\n",
    "def mae(x, y): \n",
    "    return np.abs((x-y)).mean()\n",
    "\n",
    "\n",
    "metrics=[mse, rmse, mae]\n",
    "\n",
    "def fit(model, train_loader, loss, opt_fn=None, learning_rate=1e-3, batch_size=64, epochs=1, cycle_len=1, val_loader=None, metrics=None, \n",
    "                save=False, save_path='tmp/checkpoint.pth.tar', pre_saved=False, print_period=1000):\n",
    "        \n",
    "    if opt_fn:\n",
    "        optimizer = opt_fn(model.parameters(), lr=learning_rate)\n",
    "    else:  \n",
    "        optimizer = RMSprop(model.parameters(), lr=learning_rate)\n",
    "    # for stepper \n",
    "    n_batches = int(len(train_loader.dataset) // train_loader.batch_size)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=n_batches*cycle_len)\n",
    "    global all_lr\n",
    "    all_lr = []\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    if pre_saved:\n",
    "        checkpoint = torch.load(save_path)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print('...restoring model...')\n",
    "    begin = True\n",
    "    \n",
    "    for epoch_ in tnrange(1, epochs+1, desc='Epoch'):\n",
    "        \n",
    "        if pre_saved:      \n",
    "            if begin:\n",
    "                epoch = start_epoch\n",
    "                begin = False\n",
    "        else:\n",
    "            epoch = epoch_\n",
    "        \n",
    "        # training\n",
    "        train_loss = train(model, train_loader, optimizer, scheduler, loss, print_period)\n",
    "        \n",
    "        print_output = [epoch, train_loss]\n",
    "        \n",
    "        # validation\n",
    "        if val_loader:\n",
    "            val_loss = validate(model, val_loader, optimizer, loss, metrics)\n",
    "            if val_loss[0] < best_val_loss:\n",
    "                best_val_loss = val_loss[0]\n",
    "                \n",
    "                # save model     \n",
    "                if save:\n",
    "                    if save_path:\n",
    "                        ensure_dir(save_path)\n",
    "                        state = {\n",
    "                            'epoch': epoch,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_val_loss': best_val_loss,\n",
    "                            'optimizer': optimizer.state_dict()\n",
    "                        }\n",
    "                        save_checkpoint(state, save_path=save_path)\n",
    "                        \n",
    "            for i in val_loss: print_output.append(i)\n",
    "\n",
    "        # epoch, train loss, val loss, metrics (optional)\n",
    "        print('\\n', print_output)\n",
    "        # sys.stdout.write('\\r' + str(print_output))\n",
    "\n",
    "        # reset scheduler\n",
    "        if epoch_ % cycle_len == 0:\n",
    "            scheduler = scheduler._reset(epoch, T_max=n_batches*cycle_len)\n",
    "        \n",
    "        epoch += 1\n",
    "    \n",
    "    \n",
    "def get_data(X, y):\n",
    "    # first two need to be user_id and book_id, meta after \n",
    "    user_ids, book_ids, conts = X[:, 0], X[:, 1], X[:, 2:]\n",
    "    user_ids = Variable(user_ids.long()).cuda()\n",
    "    book_ids = Variable(book_ids.long()).cuda()\n",
    "    conts = Variable(conts.float()).cuda()\n",
    "    y = Variable(y.float()).cuda()\n",
    "    return user_ids, book_ids, conts, y\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, loss, print_period=1000):\n",
    "\n",
    "    # change this to show expontially weighted moving average\n",
    "    # avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\n",
    "    epoch_loss = 0.\n",
    "    n_batches = int(train_loader.dataset.N / train_loader.batch_size)\n",
    "    model.train()\n",
    "    \n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        user_ids, book_ids, conts, y = get_data(X, y)\n",
    "\n",
    "        y_hat = model(user_ids, book_ids, conts)\n",
    "        l = loss(y_hat, y)\n",
    "        epoch_loss += l.data[0]\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler\n",
    "        scheduler.step()\n",
    "        all_lr.append(scheduler.get_lr())\n",
    "\n",
    "        if i != 0 and i % print_period == 0:\n",
    "            # sys.stdout.write('\\r' + 'iteration: {} of n_batches: {}'.format(i, n_batches))\n",
    "            # sys.stdout.flush()\n",
    "            # print('iteration: {} of n_batches: {}'.format(i, n_batches))\n",
    "            statement = '[{}/{} ({:.0f}%)]'.format(i, n_batches, (i / n_batches)*100.)\n",
    "            sys.stdout.write('\\r' + statement)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    train_loss = epoch_loss / n_batches\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validate(model, val_loader, optimizer, loss, metrics=None):\n",
    "    model.eval()\n",
    "    n_batches = int(val_loader.dataset.N / val_loader.batch_size)\n",
    "    total_loss = 0.\n",
    "    metric_scores = {}\n",
    "    if metrics:\n",
    "        for metric in metrics:\n",
    "            metric_scores[str(metric)] = []\n",
    "            \n",
    "    for i, (X_test, y_test) in enumerate(val_loader):\n",
    "        user_ids, book_ids, conts, y_test = get_data(X_test, y_test)\n",
    "        y_hat = model(user_ids, book_ids, conts)\n",
    "\n",
    "        l = loss(y_hat, y_test)\n",
    "        total_loss += l.data[0]\n",
    "\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                metric_scores[str(metric)].append(metric(y_test.data.cpu().numpy(), y_hat.data.cpu().numpy()))\n",
    "    if metrics:\n",
    "        final_metrics = []\n",
    "        for metric in metrics:\n",
    "            final_metrics.append(np.sum(metric_scores[str(metric)]) / n_batches)\n",
    "        return total_loss / n_batches, final_metrics\n",
    "    else:\n",
    "        return total_loss / n_batches\n",
    "\n",
    "\n",
    "def save_checkpoint(state, save_path='tmp/checkpoint.pth.tar'):\n",
    "    torch.save(state, save_path)\n",
    "\n",
    "# def predict(model, df, cat_flds, cont_flds):\n",
    "#     model.eval()\n",
    "\n",
    "#     cats = np.asarray(df[cat_flds], dtype=np.int64)\n",
    "#     conts = np.asarray(df[cont_flds], dtype=np.float32)\n",
    "#     x_cat = Variable(torch.from_numpy(cats))\n",
    "#     x_cont = Variable(torch.from_numpy(conts))\n",
    "#     pred = model(x_cat, x_cont)\n",
    "#     return pred.data.numpy().flatten()\n",
    "\n",
    "def load_model(model, save_path='tmp/checkpoint.pth.tar'):\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model\n",
    "\n",
    "def save_model(model, save_path='tmp/checkpoint.pth.tar'):\n",
    "    model.save_state_dict(save_path)\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [1 2 4 4]\n",
      " [1 3 4 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 4],\n",
       "       [4, 4],\n",
       "       [4, 4]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 2, 3, 4],\n",
    "     [1, 2, 4, 4],\n",
    "     [1, 3, 4, 4]])\n",
    "print(X)\n",
    "X[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 77805 M: 185973\n",
      "N // batch_size: 607\n",
      "mu: 7.59891449719833\n",
      "Epoch 1/1\n",
      "608/608 [==============================] - 389s 639ms/step - loss: 3.6176 - custom_loss: 3.3581 - custom_loss2: 1.8269 - val_loss: 3.5404 - val_custom_loss: 3.2969 - val_custom_loss2: 1.8066\n",
      "dict_keys(['val_loss', 'val_custom_loss', 'val_custom_loss2', 'loss', 'custom_loss', 'custom_loss2'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# config\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "reg = 0.0001\n",
    "# reg = 0\n",
    "\n",
    "A = load_npz(\"data/Atrain.npz\")\n",
    "A_test = load_npz(\"data/Atest.npz\")\n",
    "mask = (A > 0) * 1.0\n",
    "mask_test = (A_test > 0) * 1.0\n",
    "\n",
    "# make copies since we will shuffle\n",
    "A_copy = A.copy()\n",
    "mask_copy = mask.copy()\n",
    "A_test_copy = A_test.copy()\n",
    "mask_test_copy = mask_test.copy()\n",
    "\n",
    "N, M = A.shape\n",
    "print(\"N:\", N, \"M:\", M)\n",
    "print(\"N // batch_size:\", N // batch_size)\n",
    "\n",
    "# center the data\n",
    "mu = A.sum() / mask.sum()\n",
    "print(\"mu:\", mu)\n",
    "\n",
    "\n",
    "\n",
    "# build the model - just a 1 hidden layer autoencoder\n",
    "i = Input(shape=(M,))\n",
    "# bigger hidden layer size seems to help!\n",
    "x = Dropout(0.7)(i)\n",
    "x = Dense(700, activation='tanh', kernel_regularizer=l2(reg))(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "x = Dense(M, kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "  mask = K.cast(K.not_equal(y_true, 0), dtype='float32')\n",
    "  diff = y_pred - y_true\n",
    "  sqdiff = diff * diff * mask\n",
    "  sse = K.sum(K.sum(sqdiff))\n",
    "  n = K.sum(K.sum(mask))\n",
    "  return sse / n\n",
    "\n",
    "def custom_loss2(y_true, y_pred):\n",
    "  mask = K.cast(K.not_equal(y_true, 0), dtype='float32')\n",
    "  diff = y_pred - y_true\n",
    "  sqdiff = diff * diff * mask\n",
    "  sse = K.sum(K.sum(sqdiff))\n",
    "  n = K.sum(K.sum(mask))\n",
    "  mse = sse / n\n",
    "  return K.sqrt(mse)\n",
    "\n",
    "\n",
    "def generator(A, M):\n",
    "  while True:\n",
    "    A, M = shuffle(A, M)\n",
    "    for i in range(A.shape[0] // batch_size + 1):\n",
    "      upper = min((i+1)*batch_size, A.shape[0])\n",
    "      a = A[i*batch_size:upper].toarray()\n",
    "      m = M[i*batch_size:upper].toarray()\n",
    "      a = a - mu * m # must keep zeros at zero!\n",
    "      # m2 = (np.random.random(a.shape) > 0.5)\n",
    "      # noisy = a * m2\n",
    "      noisy = a # no noise\n",
    "      yield noisy, a\n",
    "\n",
    "\n",
    "def test_generator(A, M, A_test, M_test):\n",
    "  # assumes A and A_test are in corresponding order\n",
    "  # both of size N x M\n",
    "  while True:\n",
    "    for i in range(A.shape[0] // batch_size + 1):\n",
    "      upper = min((i+1)*batch_size, A.shape[0])\n",
    "      a = A[i*batch_size:upper].toarray()\n",
    "      m = M[i*batch_size:upper].toarray()\n",
    "      at = A_test[i*batch_size:upper].toarray()\n",
    "      mt = M_test[i*batch_size:upper].toarray()\n",
    "      a = a - mu * m\n",
    "      at = at - mu * mt\n",
    "      yield a, at\n",
    "\n",
    "\n",
    "\n",
    "model = Model(i, x)\n",
    "model.compile(\n",
    "  loss=custom_loss,\n",
    "  optimizer=SGD(lr=0.08, momentum=0.9),\n",
    "  # optimizer='adam',\n",
    "  metrics=[custom_loss, custom_loss2],\n",
    ")\n",
    "\n",
    "\n",
    "r = model.fit_generator(\n",
    "  generator(A, mask),\n",
    "  validation_data=test_generator(A_copy, mask_copy, A_test_copy, mask_test_copy),\n",
    "  epochs=epochs,\n",
    "  steps_per_epoch=A.shape[0] // batch_size + 1,\n",
    "  validation_steps=A_test.shape[0] // batch_size + 1,\n",
    ")\n",
    "print(r.history.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1782, 8.68672),\n",
       " (280, 8.623345),\n",
       " (1783, 8.562433),\n",
       " (2541, 8.557728),\n",
       " (1146, 8.542907),\n",
       " (2542, 8.51985),\n",
       " (1567, 8.503412),\n",
       " (33, 8.496413),\n",
       " (2551, 8.486841),\n",
       " (2025, 8.486477)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user = A[1000, :].todense()\n",
    "test_user_mask = test_user == 0\n",
    "print(test_user[test_user_mask].shape[1])\n",
    "test_user_mask = np.array(test_user_mask)[0]\n",
    "preds = np.array(model.predict(test_user)[0] + mu)\n",
    "recs = []\n",
    "for i, (pred, mask) in enumerate(zip(preds, test_user_mask)):\n",
    "    if test_user_mask[i]:\n",
    "        recs.append((i, pred))\n",
    "# len(preds)\n",
    "recs = sorted(recs, key=lambda x: x[1], reverse=True)[:10]\n",
    "recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>score</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>Year_Of_Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0316089745</td>\n",
       "      <td>8.686720</td>\n",
       "      <td>The Wooden Nickel: A Novel</td>\n",
       "      <td>William Carpenter</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Back Bay Books</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439510112</td>\n",
       "      <td>8.623345</td>\n",
       "      <td>Born Confused</td>\n",
       "      <td>Tanuja Desai Hidier</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Push</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>037542217X</td>\n",
       "      <td>8.562433</td>\n",
       "      <td>The Kalahari Typing School for Men : More from...</td>\n",
       "      <td>Alexander McCall Smith</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Pantheon</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0020083513</td>\n",
       "      <td>8.557728</td>\n",
       "      <td>BATTLE OF THE TITANS</td>\n",
       "      <td>Carolyn Keene</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Scribner Paper Fiction</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>068930935X</td>\n",
       "      <td>8.542907</td>\n",
       "      <td>A STRING OF CHANCES (String of Chances Nrf)</td>\n",
       "      <td>Phyllis Reynolds Naylor</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>Atheneum</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0140376410</td>\n",
       "      <td>8.519850</td>\n",
       "      <td>The Ear, the Eye, and the Arm</td>\n",
       "      <td>Nancy Farmer</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Puffin Books</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0440217520</td>\n",
       "      <td>8.503412</td>\n",
       "      <td>Tunnel Vision (V.I. Warshawski Novels (Paperba...</td>\n",
       "      <td>Sara Paretsky</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Dell</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3442353866</td>\n",
       "      <td>8.496413</td>\n",
       "      <td>Der Fluch der Kaiserin. Ein Richter- Di- Roman.</td>\n",
       "      <td>Eleanor Cooney</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Goldmann</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0151003394</td>\n",
       "      <td>8.486841</td>\n",
       "      <td>Africans in America: AMERICAN (AMERI)ca's Jour...</td>\n",
       "      <td>Charles Johnson</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Harcourt</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>038080350X</td>\n",
       "      <td>8.486477</td>\n",
       "      <td>Crescent City Rhapsody</td>\n",
       "      <td>Kathleen Ann Goonan</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Eos</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN     score                                         Book_Title  \\\n",
       "0  0316089745  8.686720                         The Wooden Nickel: A Novel   \n",
       "1  0439510112  8.623345                                      Born Confused   \n",
       "2  037542217X  8.562433  The Kalahari Typing School for Men : More from...   \n",
       "3  0020083513  8.557728                               BATTLE OF THE TITANS   \n",
       "4  068930935X  8.542907        A STRING OF CHANCES (String of Chances Nrf)   \n",
       "5  0140376410  8.519850                      The Ear, the Eye, and the Arm   \n",
       "6  0440217520  8.503412  Tunnel Vision (V.I. Warshawski Novels (Paperba...   \n",
       "7  3442353866  8.496413    Der Fluch der Kaiserin. Ein Richter- Di- Roman.   \n",
       "8  0151003394  8.486841  Africans in America: AMERICAN (AMERI)ca's Jour...   \n",
       "9  038080350X  8.486477                             Crescent City Rhapsody   \n",
       "\n",
       "               Book_Author  Year_Of_Publication               Publisher  \\\n",
       "0        William Carpenter               2003.0          Back Bay Books   \n",
       "1      Tanuja Desai Hidier               2003.0                    Push   \n",
       "2   Alexander McCall Smith               2003.0                Pantheon   \n",
       "3            Carolyn Keene               1991.0  Scribner Paper Fiction   \n",
       "4  Phyllis Reynolds Naylor               1982.0                Atheneum   \n",
       "5             Nancy Farmer               1995.0            Puffin Books   \n",
       "6            Sara Paretsky               1995.0                    Dell   \n",
       "7           Eleanor Cooney               2001.0                Goldmann   \n",
       "8          Charles Johnson               1998.0                Harcourt   \n",
       "9      Kathleen Ann Goonan               2001.0                     Eos   \n",
       "\n",
       "  language  \n",
       "0       en  \n",
       "1       en  \n",
       "2       en  \n",
       "3       vi  \n",
       "4       en  \n",
       "5       en  \n",
       "6       en  \n",
       "7       de  \n",
       "8       en  \n",
       "9       en  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dill\n",
    "\n",
    "with open('data/book_mapping.pkl', 'rb') as f:\n",
    "    book_mapping = dill.load(f)\n",
    "    \n",
    "inverse_book_mapping = {i:j for i, j in enumerate(book_mapping)}\n",
    "    \n",
    "orig = []\n",
    "\n",
    "for i, score in recs:\n",
    "    orig.append((inverse_book_mapping[i], score))\n",
    "    \n",
    "    \n",
    "pd.DataFrame(orig, columns=['ISBN', 'score']).merge(books, on='ISBN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# # plot losses\n",
    "# plt.plot(r.history['loss'], label=\"train loss\")\n",
    "# plt.plot(r.history['val_loss'], label=\"test loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # plot mse\n",
    "# plt.plot(r.history['custom_loss'], label=\"train mse\")\n",
    "# plt.plot(r.history['val_custom_loss'], label=\"test mse\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 185973)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0, :].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5989145  0.         0.        ...  0.         0.         0.       ]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "N = 0\n",
    "for i in range(A.shape[0] // batch_size + 1)[:2]:\n",
    "    upper = min((i+1)*batch_size, A.shape[0])\n",
    "    a = A[i*batch_size:upper].toarray()\n",
    "    m = mask[i*batch_size:upper].toarray()\n",
    "    a = a - mu * m # must keep zeros at\n",
    "    print(a[0])\n",
    "    N += 1\n",
    "    if N == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.541049 , 7.5565567, 7.588127 , ..., 7.56397  , 7.5742903,\n",
       "       7.599548 ], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(A[0, :].todense())[0] + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24036, 3)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from BookData import BookDataSet\n",
    "from EvaluationData import CreateDataSets\n",
    "from EvaluatedAlgorithm import EvaluatedAlgorithm\n",
    "from Evaluator import Evaluator\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def load_feather(filepath, **kwargs):\n",
    "    '''\n",
    "    input: (path to feather file)\n",
    "    read feather file to pandas dataframe\n",
    "    output: (pandas dataframe)\n",
    "    '''\n",
    "    return pd.read_feather(filepath, **kwargs)\n",
    "\n",
    "ratings = load_feather(DATA_PATH/'ratings_explicit_clean.feather')\n",
    "ratings.drop(['index'], axis=1, inplace=True)\n",
    "users = load_feather(DATA_PATH/'users_clean.feather')\n",
    "books = load_feather(DATA_PATH/'books_clean.feather')\n",
    "\n",
    "\n",
    "counts_users = ratings.User_ID.value_counts()\n",
    "counts_ratings = ratings.Book_Rating.value_counts()\n",
    "sample_ratings = ratings[ratings['User_ID'].isin(counts_users[counts_users >= 200].index)]\n",
    "sample_ratings = sample_ratings[ratings['Book_Rating'].isin(counts_ratings[counts_ratings >= 200].index)]\n",
    "isbn_group = sample_ratings.groupby('ISBN', as_index=False)['Book_Rating'].count()\n",
    "sample_ratings = sample_ratings[sample_ratings.ISBN.isin(list(isbn_group[isbn_group.Book_Rating > 1].ISBN.values))]\n",
    "print(sample_ratings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "from surprise import PredictionImpossible\n",
    "\n",
    "\n",
    "from scipy.sparse import lil_matrix, csr_matrix, save_npz, load_npz\n",
    "    \n",
    "def create_sparse(trainset, N, M, load=True, path='data/Atrain_2.npz'):\n",
    "    \n",
    "    if load:\n",
    "        A = load_npz(path)\n",
    "        print('sparse dataset loaded')\n",
    "        return A\n",
    "    else:     \n",
    "        A = lil_matrix((N, M))\n",
    "        for (uid, iid, rating) in trainset.all_ratings():\n",
    "            i = int(uid)\n",
    "            j = int(iid)\n",
    "            A[i, j] = rating\n",
    "        A = A.tocsr()\n",
    "        save_npz(path, A)\n",
    "        print('sparse dataset created and loaded')\n",
    "        return A\n",
    "\n",
    "class AutoRecModelCoolBeans(AlgoBase):\n",
    "    def __init__(self, epochs=10, hidden=100, lr=0.01, batch_size=100, params={}):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.epochs = epochs\n",
    "        self.hidden = hidden\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        \n",
    "        N = trainset.n_users\n",
    "        M = trainset.n_items\n",
    "        \n",
    "        trainingMatrix = create_sparse(trainset, N, M, load=False)\n",
    "        print(trainingMatrix.shape)\n",
    "            \n",
    "        autoRec = AutoReccommendation(N, M, hidden=self.hidden, lr=self.lr, \n",
    "                          batch_size=self.batch_size, epochs=self.epochs)\n",
    "        autoRec.train(trainingMatrix)\n",
    "\n",
    "        self.predictedRatings = autoRec.model.predict(trainingMatrix)    \n",
    "    \n",
    "    def estimate(self, u, i):\n",
    "\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unkown.')\n",
    "        \n",
    "        rating = self.predictedRatings[u, i]\n",
    "        \n",
    "        if (rating < 0.001):\n",
    "            raise PredictionImpossible('No valid prediction exists.')\n",
    "            \n",
    "        return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.utils import shuffle\n",
    "# from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "# import keras.backend as K\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input, Dropout, Dense\n",
    "# from keras.regularizers import l2\n",
    "# from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# class AutoReccommendation(object):\n",
    "#     def __init__(self, N, M, epochs=5, hidden=500, lr=0.1, batch_size=128, reg=0.01):\n",
    "#         self.N = N\n",
    "#         self.M = M\n",
    "#         self.epochs = epochs\n",
    "#         self.hidden = hidden\n",
    "#         self.lr = lr\n",
    "#         self.reg = reg\n",
    "#         self.batch_size = batch_size\n",
    "            \n",
    "#     def train(self, X_train):\n",
    "# #         X_test = load_npz(\"data/Atest.npz\")\n",
    "#         mask = (X_train > 0) * 1.0\n",
    "# #         mask_test = (X_test > 0) * 1.0\n",
    "\n",
    "#         # make copies since we will shuffle\n",
    "#         X_train_copy = X_train.copy()\n",
    "#         mask_copy = mask.copy()\n",
    "# #         X_test_copy = X_test.copy()\n",
    "# #         mask_test_copy = mask_test.copy()\n",
    "        \n",
    "#         self.mu = X_train.sum() / mask.sum()\n",
    "        \n",
    "#         self.build()\n",
    "        \n",
    "#         self.output = self.model.fit_generator(\n",
    "#               self.generator(X_train, mask),\n",
    "# #               validation_data=self.test_generator(X_train_copy, mask_copy, X_test_copy, mask_test_copy),\n",
    "#               epochs = self.epochs,\n",
    "#               steps_per_epoch = X_train.shape[0] // self.batch_size + 1,\n",
    "# #               validation_steps = X_test.shape[0] // self.batch_size + 1,\n",
    "#         )\n",
    "        \n",
    "        \n",
    "    \n",
    "#     def generator(self, X, mask):\n",
    "#         while True:\n",
    "#             X, mask = shuffle(X, mask)\n",
    "#             for i in range(X.shape[0] // self.batch_size + 1):\n",
    "#                 upper = min((i + 1) * self.batch_size, X.shape[0])\n",
    "#                 a = X[i * self.batch_size:upper].toarray()\n",
    "#                 m = mask[i * self.batch_size:upper].toarray()\n",
    "#                 a = a - self.mu * m # must keep zeros at zero!\n",
    "#                 # m2 = (np.random.random(a.shape) > 0.5)\n",
    "#                 # noisy = a * m2\n",
    "#                 noisy = a # no noise\n",
    "#                 yield noisy, a\n",
    "\n",
    "\n",
    "#     def test_generator(self, X, mask, X_test, mask_test):\n",
    "#       # assumes A and A_test are in corresponding order\n",
    "#       # both of size N x M\n",
    "#         while True:\n",
    "#             for i in range(X.shape[0] // self.batch_size + 1):\n",
    "#                 upper = min((i + 1) * self.batch_size, X.shape[0])\n",
    "#                 a = X[i * self.batch_size:upper]#.toarray()\n",
    "#                 m = mask[i * self.batch_size:upper]#.toarray()\n",
    "#                 at = X_test[i * self.batch_size:upper]#.toarray()\n",
    "#                 mt = mask_test[i * self.batch_size:upper]#.toarray()\n",
    "#                 a = a - self.mu * m\n",
    "#                 at = at - self.mu * mt\n",
    "#                 yield a, at\n",
    "                \n",
    "#     def mse_mask_loss(self, y_true, y_pred):\n",
    "#         mask = K.cast(K.not_equal(y_true, 0), dtype='float32')\n",
    "#         diff = y_pred - y_true\n",
    "#         sqdiff = K.square(diff) * mask\n",
    "#         sse = K.sum(K.sum(sqdiff))\n",
    "#         n = K.sum(K.sum(mask))\n",
    "#         return sse / n\n",
    "\n",
    "#     def rmse_mask_loss(self, y_true, y_pred):\n",
    "#         mask = K.cast(K.not_equal(y_true, 0), dtype='float32')\n",
    "#         diff = y_pred - y_true\n",
    "#         sqdiff = K.square(diff) * mask\n",
    "#         sse = K.sum(K.sum(sqdiff))\n",
    "#         n = K.sum(K.sum(mask))\n",
    "#         mse = sse / n\n",
    "#         return K.sqrt(mse)\n",
    "    \n",
    "#     def build(self):       \n",
    "#         # build the model - just a 1 hidden layer autoencoder\n",
    "#         input_ = Input(shape=(self.M,))\n",
    "#         X = Dropout(0.2)(input_)\n",
    "#         X = Dense(self.hidden, activation='tanh', kernel_regularizer=l2(self.reg))(X)\n",
    "#         X = Dense(self.M, kernel_regularizer=l2(self.reg))(X)\n",
    "        \n",
    "        \n",
    "#         self.model = Model(input_, X)\n",
    "#         self.model.compile(loss=self.mse_mask_loss,\n",
    "#                       optimizer=SGD(lr=0.08, momentum=0.9),\n",
    "#                       metrics=[self.rmse_mask_loss],\n",
    "#         )\n",
    "\n",
    "#     def get_recommendations(self, inputUser):\n",
    "#         pred = self.model.predict([inputUser]) + self.mu\n",
    "#         return pred \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample_ratings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ca9220600af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load up common data set for the recommender algorithms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mevaluationData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadBookData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Construct an Evaluator to, you know, evaluate them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ca9220600af8>\u001b[0m in \u001b[0;36mLoadBookData\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# tf.reset_default_graph()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoadBookData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBookDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mrankings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_popularity_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_ratings' is not defined"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from BookData import BookDataSet\n",
    "from Evaluator import Evaluator\n",
    "from surprise import KNNBasic\n",
    "from surprise import NormalPredictor\n",
    "from auto_rec_model import AutoRecModel\n",
    "\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "def LoadBookData():\n",
    "    data = BookDataSet(sample_ratings, books, users)\n",
    "    rankings = data.get_popularity_ranks()\n",
    "    return (data, rankings)\n",
    "\n",
    "\n",
    "# Load up common data set for the recommender algorithms\n",
    "(evaluationData, rankings) = LoadBookData()\n",
    "\n",
    "# Construct an Evaluator to, you know, evaluate them\n",
    "evaluator = Evaluator(evaluationData, rankings)\n",
    "\n",
    "\n",
    "# Random = KNNBasic()\n",
    "# evaluator.add_model(Random, \"Random\")\n",
    "\n",
    "#Autoencoder\n",
    "AutoRec = AutoRecModel()\n",
    "evaluator.add_model(AutoRec, \"AutoRec\")\n",
    "\n",
    "# Just make random recommendations\n",
    "\n",
    "\n",
    "# Fight!\n",
    "evaluator.evaluate(topN=False)\n",
    "\n",
    "# sample user \n",
    "evaluator.recommend_top_books(evaluationData, test_user_id=177458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 8218)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = load_npz('data/Atrain_2.npz')\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "from surprise import PredictionImpossible\n",
    "\n",
    "class TestModel(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "        \n",
    "    def estimate(self, user, item):\n",
    "        return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import AlgoBase\n",
    "# from surprise import PredictionImpossible\n",
    "# import numpy as np\n",
    "# from AutoRec import AutoRec\n",
    "\n",
    "\n",
    "# class AutoRecAlgorithm(AlgoBase):\n",
    "\n",
    "#     def __init__(self, epochs=100, hiddenDim=100, learningRate=0.01, batchSize=100, sim_options={}):\n",
    "#         AlgoBase.__init__(self)\n",
    "#         self.epochs = epochs\n",
    "#         self.hiddenDim = hiddenDim\n",
    "#         self.learningRate = learningRate\n",
    "#         self.batchSize = batchSize\n",
    "\n",
    "#     def fit(self, trainset):\n",
    "#         AlgoBase.fit(self, trainset)\n",
    "\n",
    "#         numUsers = trainset.n_users\n",
    "#         numItems = trainset.n_items\n",
    "        \n",
    "#         trainingMatrix = np.zeros([numUsers, numItems], dtype=np.float32)\n",
    "        \n",
    "#         for (uid, iid, rating) in trainset.all_ratings():\n",
    "#             trainingMatrix[int(uid), int(iid)] = rating / 5.0\n",
    "        \n",
    "#         # Create an RBM with (num items * rating values) visible nodes\n",
    "#         autoRec = AutoRec(trainingMatrix.shape[1], hiddenDimensions=self.hiddenDim, learningRate=self.learningRate, batchSize=self.batchSize, epochs=self.epochs)\n",
    "#         autoRec.Train(trainingMatrix)\n",
    "\n",
    "#         self.predictedRatings = np.zeros([numUsers, numItems], dtype=np.float32)\n",
    "        \n",
    "#         for uiid in range(trainset.n_users):\n",
    "#             if (uiid % 50 == 0):\n",
    "#                 print(\"Processing user \", uiid)\n",
    "#             recs = autoRec.GetRecommendations([trainingMatrix[uiid]])\n",
    "            \n",
    "#             for itemID, rec in enumerate(recs):\n",
    "#                 self.predictedRatings[uiid, itemID] = rec * 5.0\n",
    "        \n",
    "#         return self\n",
    "\n",
    "\n",
    "#     def estimate(self, u, i):\n",
    "\n",
    "#         if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "#             raise PredictionImpossible('User and/or item is unkown.')\n",
    "        \n",
    "#         rating = self.predictedRatings[u, i]\n",
    "        \n",
    "#         if (rating < 0.001):\n",
    "#             raise PredictionImpossible('No valid prediction exists.')\n",
    "            \n",
    "#         return rating\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0244886617399844\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdae267c5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sample_book = average_ratings.loc['014028009X']\n",
    "sample_book\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as st\n",
    "\n",
    "data = norm.rvs(loc=sample_book['mean'], scale=sample_book['std'], size=int(sample_book['count']))\n",
    "\n",
    "# Fit a normal distribution to the data:\n",
    "mu, std = norm.fit(data)\n",
    "# Plot the histogram.\n",
    "plt.hist(data, bins=25, density=True, alpha=0.6, color='g')\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "interval = st.t.interval(0.95, int(sample_book['count']), loc=sample_book['mean'], scale=sample_book['std'])\n",
    "# print(interval[0])\n",
    "print(norm.cdf(interval[0], loc=sample_book['mean'], scale=sample_book['std']))\n",
    "plt.axvline(interval[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0248930301985469"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "# is the probability that X will take a value less than or equal to x.\n",
    "\n",
    "norm.cdf(interval[0], loc=sample_book['mean'], scale=sample_book['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity level: 0.9971933672929102\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ISBN</th>\n",
       "      <th>0330299891</th>\n",
       "      <th>0375404120</th>\n",
       "      <th>0586045007</th>\n",
       "      <th>9022906116</th>\n",
       "      <th>9032803328</th>\n",
       "      <th>9044922564</th>\n",
       "      <th>9044922572</th>\n",
       "      <th>9044922718</th>\n",
       "      <th>9044923161</th>\n",
       "      <th>904492401X</th>\n",
       "      <th>...</th>\n",
       "      <th>UNGRANDHOMMED</th>\n",
       "      <th>X000000000</th>\n",
       "      <th>YOUTELLEM,AND</th>\n",
       "      <th>ZR903CX0003</th>\n",
       "      <th>\\0432534220\\\"\"</th>\n",
       "      <th>\\2842053052\\\"\"</th>\n",
       "      <th>b00005wz75</th>\n",
       "      <th>cn108465</th>\n",
       "      <th>cn113107</th>\n",
       "      <th>Ô½crosoft</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207699 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ISBN      0330299891   0375404120   0586045007   9022906116   9032803328  \\\n",
       "User_ID                                                                    \n",
       "254              NaN          NaN          NaN          NaN          NaN   \n",
       "2276             NaN          NaN          NaN          NaN          NaN   \n",
       "2766             NaN          NaN          NaN          NaN          NaN   \n",
       "2977             NaN          NaN          NaN          NaN          NaN   \n",
       "3363             NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "ISBN      9044922564   9044922572   9044922718   9044923161   904492401X  \\\n",
       "User_ID                                                                    \n",
       "254              NaN          NaN          NaN          NaN          NaN   \n",
       "2276             NaN          NaN          NaN          NaN          NaN   \n",
       "2766             NaN          NaN          NaN          NaN          NaN   \n",
       "2977             NaN          NaN          NaN          NaN          NaN   \n",
       "3363             NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "ISBN       ...      UNGRANDHOMMED  X000000000  YOUTELLEM,AND  ZR903CX0003  \\\n",
       "User_ID    ...                                                              \n",
       "254        ...                NaN         NaN            NaN          NaN   \n",
       "2276       ...                NaN         NaN            NaN          NaN   \n",
       "2766       ...                NaN         NaN            NaN          NaN   \n",
       "2977       ...                NaN         NaN            NaN          NaN   \n",
       "3363       ...                NaN         NaN            NaN          NaN   \n",
       "\n",
       "ISBN     \\0432534220\\\"\"  \\2842053052\\\"\"  b00005wz75  cn108465  cn113107  \\\n",
       "User_ID                                                                   \n",
       "254                 NaN             NaN         NaN       NaN       NaN   \n",
       "2276                NaN             NaN         NaN       NaN       NaN   \n",
       "2766                NaN             NaN         NaN       NaN       NaN   \n",
       "2977                NaN             NaN         NaN       NaN       NaN   \n",
       "3363                NaN             NaN         NaN       NaN       NaN   \n",
       "\n",
       "ISBN     Ô½crosoft  \n",
       "User_ID             \n",
       "254            NaN  \n",
       "2276           NaN  \n",
       "2766           NaN  \n",
       "2977           NaN  \n",
       "3363           NaN  \n",
       "\n",
       "[5 rows x 207699 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The simplest model of all. Most highly rated and most frequent books in the dataset\n",
    "\n",
    "def subset_ratings(ratings, user_id_limit=200, book_rating_limit=100):\n",
    "    user_counts = ratings['User_ID'].value_counts()\n",
    "    rating_counts = ratings['Book_Rating'].value_counts()\n",
    "    simple_ratings = ratings[ratings['User_ID'].isin(list(user_counts[user_counts >= user_id_limit].index))]\n",
    "    simple_ratings = simple_ratings[ratings['Book_Rating'].isin(list(rating_counts[rating_counts >= book_rating_limit].index))]\n",
    "    # create interaction dataset \n",
    "    simple_interactions = simple_ratings.pivot(index='User_ID', columns='ISBN')['Book_Rating']\n",
    "    print('Sparsity level: {}'.format(simple_interactions.isnull().sum().sum() / np.multiply(*simple_interactions.shape)))\n",
    "    return simple_ratings, simple_interactions\n",
    "\n",
    "simple_ratings, simple_interactions = subset_ratings(ratings, user_id_limit=200, book_rating_limit=100)\n",
    "simple_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3175: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n",
      "/home/matt/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3109: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  c *= 1. / np.float64(fact)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearsonR</th>\n",
       "      <th>ratingCount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISBN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0316666343</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0312291639</th>\n",
       "      <td>0.471872</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0316601950</th>\n",
       "      <td>0.434248</td>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0446610038</th>\n",
       "      <td>0.429712</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0446672211</th>\n",
       "      <td>0.421478</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0385265700</th>\n",
       "      <td>0.351635</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0345342968</th>\n",
       "      <td>0.316922</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0060930535</th>\n",
       "      <td>0.309860</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0375707972</th>\n",
       "      <td>0.308145</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0684872153</th>\n",
       "      <td>0.272480</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pearsonR  ratingCount\n",
       "ISBN                             \n",
       "0316666343  1.000000         1295\n",
       "0312291639  0.471872          354\n",
       "0316601950  0.434248          568\n",
       "0446610038  0.429712          391\n",
       "0446672211  0.421478          585\n",
       "0385265700  0.351635          319\n",
       "0345342968  0.316922          321\n",
       "0060930535  0.309860          494\n",
       "0375707972  0.308145          354\n",
       "0684872153  0.272480          326"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bones_ratings = simple_interactions['0316666343']\n",
    "similar_to_bones = simple_interactions.corrwith(bones_ratings)\n",
    "corr_bones = pd.DataFrame(similar_to_bones, columns=['pearsonR'])\n",
    "corr_bones.dropna(inplace=True)\n",
    "corr_summary = corr_bones.join(average_rating['ratingCount'])\n",
    "corr_summary[corr_summary['ratingCount'] >= 300].sort_values('pearsonR', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan,  0.],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_interactions[['0316666343', '0971880107']].values.reshape(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-8f97d4c17301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_interactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_interactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0316666343'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0971880107'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(x, y, rowvar, bias, ddof)\u001b[0m\n\u001b[1;32m   3173\u001b[0m         warnings.warn('bias and ddof have no effect and are deprecated',\n\u001b[1;32m   3174\u001b[0m                       DeprecationWarning, stacklevel=2)\n\u001b[0;32m-> 3175\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3177\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights)\u001b[0m\n\u001b[1;32m   3106\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3107\u001b[0m         \u001b[0mX_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3108\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3109\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.corrcoef(simple_interactions.T, simple_interactions[['0316666343', '0971880107']].values.reshape(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '0316666343'\n",
    "corr_df = pd.DataFrame()\n",
    "\n",
    "for value in ['0316666343', '0971880107']:\n",
    "    new_per = simple_interactions.corrwith(simple_interactions[value])\n",
    "    column = ['pearsonR_' + str(value)]\n",
    "    corr_df[column] = new_per\n",
    "\n",
    "corr_df.dropna(inplace=True)\n",
    "corr_df = corr_df.join(average_ratings['count'])\n",
    "corr_df[corr_df['count'] >= 300].sort_values('pearsonR', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3175: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n",
      "/home/matt/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3109: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  c *= 1. / np.float64(fact)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearsonR</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISBN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0316666343</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0312291639</th>\n",
       "      <td>0.471872</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0316601950</th>\n",
       "      <td>0.434248</td>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0446610038</th>\n",
       "      <td>0.429712</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0446672211</th>\n",
       "      <td>0.421478</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0385265700</th>\n",
       "      <td>0.351635</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0345342968</th>\n",
       "      <td>0.316922</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0060930535</th>\n",
       "      <td>0.309860</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0375707972</th>\n",
       "      <td>0.308145</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0684872153</th>\n",
       "      <td>0.272480</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pearsonR  count\n",
       "ISBN                       \n",
       "0316666343  1.000000   1295\n",
       "0312291639  0.471872    354\n",
       "0316601950  0.434248    568\n",
       "0446610038  0.429712    391\n",
       "0446672211  0.421478    585\n",
       "0385265700  0.351635    319\n",
       "0345342968  0.316922    321\n",
       "0060930535  0.309860    494\n",
       "0375707972  0.308145    354\n",
       "0684872153  0.272480    326"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '0316666343'\n",
    "corr_df = pd.DataFrame(simple_interactions.corrwith(simple_interactions['0316666343']), columns=['pearsonR'])\n",
    "corr_df.dropna(inplace=True)\n",
    "corr_df.head()\n",
    "corr_df = corr_df.join(average_ratings['count'])\n",
    "corr_df[corr_df['count'] >= 300].sort_values('pearsonR', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISBN\n",
       "3379015180    1.0\n",
       "3522128001    1.0\n",
       "3596224284    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3175: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n",
      "/home/matt/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3109: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  c *= 1. / np.float64(fact)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearsonR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISBN</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pearsonR]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings = ratings[ratings.User_ID == sample_user]\n",
    "top_book = user_ratings[user_ratings.Book_Rating == user_ratings.Book_Rating.max()]\n",
    "top_book_isbn = top_book.ISBN.values[0]\n",
    "\n",
    "top_book_name = books.Book_Title[books.ISBN == top_book_isbn].values[0]\n",
    "top_book_author = books.Book_Author[books.ISBN == top_book_isbn].values[0]\n",
    "\n",
    "col = simple_interactions[top_book_isbn]\n",
    "similar_books = simple_interactions.corrwith(col)\n",
    "corr_df = pd.DataFrame(similar_books, columns=['pearsonR'])\n",
    "corr_df.dropna(inplace=True)\n",
    "corr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df.dropna(inplace=True)\n",
    "corr_summary = corr_df.join(average_rating['ratingCount'])\n",
    "corr_summary[corr_summary['ratingCount'] >= 300].sort_values('pearsonR', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore the cold start problem for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')\n",
    "\n",
    "\n",
    "#Making necesarry imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import correlation\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "metric='cosine'\n",
    "k=10\n",
    "#This function finds k similar users given the user_id and ratings matrix \n",
    "#These similarities are same as obtained via using pairwise_distances\n",
    "def findksimilarusers(user_id, ratings, metric = metric, k=k):\n",
    "    similarities=[]\n",
    "    indices=[]\n",
    "    model_knn = NearestNeighbors(metric = metric, algorithm = 'brute') \n",
    "    model_knn.fit(ratings)\n",
    "    loc = ratings.index.get_loc(user_id)\n",
    "    distances, indices = model_knn.kneighbors(ratings.iloc[loc, :].values.reshape(1, -1), n_neighbors = k+1)\n",
    "    similarities = 1-distances.flatten()\n",
    "            \n",
    "    return similarities,indices\n",
    "\n",
    "#This function predicts rating for specified user-item combination based on user-based approach\n",
    "def predict_userbased(user_id, item_id, ratings, metric = metric, k=k):\n",
    "    prediction=0\n",
    "    user_loc = ratings.index.get_loc(user_id)\n",
    "    item_loc = ratings.columns.get_loc(item_id)\n",
    "    similarities, indices=findksimilarusers(user_id, ratings,metric, k) #similar users based on cosine similarity\n",
    "    mean_rating = ratings.iloc[user_loc,:].mean() #to adjust for zero based indexing\n",
    "    sum_wt = np.sum(similarities)-1\n",
    "    product=1\n",
    "    wtd_sum = 0 \n",
    "    \n",
    "    for i in range(0, len(indices.flatten())):\n",
    "        if indices.flatten()[i] == user_loc:\n",
    "            continue;\n",
    "        else: \n",
    "            ratings_diff = ratings.iloc[indices.flatten()[i],item_loc]-np.mean(ratings.iloc[indices.flatten()[i],:])\n",
    "            product = ratings_diff * (similarities[i])\n",
    "            wtd_sum = wtd_sum + product\n",
    "    \n",
    "    #in case of very sparse datasets, using correlation metric for collaborative based approach may give negative ratings\n",
    "    #which are handled here as below\n",
    "    if prediction <= 0:\n",
    "        prediction = 1   \n",
    "    elif prediction >10:\n",
    "        prediction = 10\n",
    "    \n",
    "    prediction = int(round(mean_rating + (wtd_sum/sum_wt)))\n",
    "    print('\\nPredicted rating for user {0} -> item {1}: {2}'.format(user_id,item_id,prediction))\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# predict_userbased(11676,'0001056107',ratings_matrix);\n",
    "# Predicted rating for user 11676 -> item 0001056107: 2\n",
    "\n",
    "    \n",
    "# Item-based Recommendation Systems\n",
    "\n",
    "\n",
    "#This function finds k similar items given the item_id and ratings matrix\n",
    "\n",
    "def findksimilaritems(item_id, ratings, metric=metric, k=k):\n",
    "    similarities=[]\n",
    "    indices=[]\n",
    "    ratings=ratings.T\n",
    "    loc = ratings.index.get_loc(item_id)\n",
    "    model_knn = NearestNeighbors(metric = metric, algorithm = 'brute')\n",
    "    model_knn.fit(ratings)\n",
    "    \n",
    "    distances, indices = model_knn.kneighbors(ratings.iloc[loc, :].values.reshape(1, -1), n_neighbors = k+1)\n",
    "    similarities = 1-distances.flatten()\n",
    "\n",
    "    return similarities,indices\n",
    "\n",
    "# similarities,indices=findksimilaritems('0001056107',ratings_matrix)\n",
    "\n",
    "#This function predicts the rating for specified user-item combination based on item-based approach\n",
    "def predict_itembased(user_id, item_id, ratings, metric = metric, k=k):\n",
    "    prediction= wtd_sum =0\n",
    "    user_loc = ratings.index.get_loc(user_id)\n",
    "    item_loc = ratings.columns.get_loc(item_id)\n",
    "    similarities, indices=findksimilaritems(item_id, ratings) #similar users based on correlation coefficients\n",
    "    sum_wt = np.sum(similarities)-1\n",
    "    product=1\n",
    "    for i in range(0, len(indices.flatten())):\n",
    "        if indices.flatten()[i] == item_loc:\n",
    "            continue;\n",
    "        else:\n",
    "            product = ratings.iloc[user_loc,indices.flatten()[i]] * (similarities[i])\n",
    "            wtd_sum = wtd_sum + product                              \n",
    "    prediction = int(round(wtd_sum/sum_wt))\n",
    "    \n",
    "    #in case of very sparse datasets, using correlation metric for collaborative based approach may give negative ratings\n",
    "    #which are handled here as below //code has been validated without the code snippet below, below snippet is to avoid negative\n",
    "    #predictions which might arise in case of very sparse datasets when using correlation metric\n",
    "    if prediction <= 0:\n",
    "        prediction = 1   \n",
    "    elif prediction >10:\n",
    "        prediction = 10\n",
    "\n",
    "    print('\\nPredicted rating for user {0} -> item {1}: {2}'.format(user_id,item_id,prediction)      )\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# prediction = predict_itembased(11676,'0001056107',ratings_matrix)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "#This function utilizes above functions to recommend items for item/user based approach and cosine/correlation. \n",
    "#Recommendations are made if the predicted rating for an item is >= to 6,and the items have not been rated already\n",
    "def recommendItem(user_id, ratings, metric=metric):  \n",
    "    print('FFF')\n",
    "    if (user_id not in ratings.index.values) or type(user_id) is not int:\n",
    "        print(\"User id should be a valid integer from this list :\\n\\n {} \".format(re.sub('[\\[\\]]', '', np.array_str(book_ratings.index.values))))\n",
    "    else:    \n",
    "        ids = ['Item-based (correlation)','Item-based (cosine)','User-based (correlation)','User-based (cosine)']\n",
    "        select = widgets.Dropdown(options=ids, value=ids[0],description='Select approach', width='1000px')\n",
    "        def on_change(change):\n",
    "            clear_output(wait=True)\n",
    "            prediction = []            \n",
    "            if change['type'] == 'change' and change['name'] == 'value':            \n",
    "                if (select.value == 'Item-based (correlation)') | (select.value == 'User-based (correlation)') :\n",
    "                    metric = 'correlation'\n",
    "                else:                       \n",
    "                    metric = 'cosine'   \n",
    "                with suppress_stdout():\n",
    "                    if (select.value == 'Item-based (correlation)') | (select.value == 'Item-based (cosine)'):\n",
    "                        for i in range(ratings.shape[1]):\n",
    "                            if (ratings[str(ratings.columns[i])][user_id] !=0): #not rated already\n",
    "                                prediction.append(predict_itembased(user_id, str(ratings.columns[i]) ,ratings, metric))\n",
    "                            else:                    \n",
    "                                prediction.append(-1) #for already rated items\n",
    "                    else:\n",
    "                        for i in range(ratings.shape[1]):\n",
    "                            if (ratings[str(ratings.columns[i])][user_id] !=0): #not rated already\n",
    "                                prediction.append(predict_userbased(user_id, str(ratings.columns[i]) ,ratings, metric))\n",
    "                            else:                    \n",
    "                                prediction.append(-1) #for already rated items\n",
    "                prediction = pd.Series(prediction)\n",
    "                prediction = prediction.sort_values(ascending=False)\n",
    "                recommended = prediction[:10]\n",
    "                print(\"As per {0} approach....Following books are recommended...\".format(select.value))\n",
    "                for i in range(len(recommended)):\n",
    "                     print(\"{0}. {1}\".format(i+1,books.Book_Title[recommended.index[i]].encode('utf-8')))                        \n",
    "        select.observe(on_change)\n",
    "        display(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendItem(6242, book_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
